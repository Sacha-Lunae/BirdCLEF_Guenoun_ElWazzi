{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv(\"./birdclef-2024/train_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>39.2297</td>\n",
       "      <td>118.1987</td>\n",
       "      <td>Muscicapa dauurica</td>\n",
       "      <td>Asian Brown Flycatcher</td>\n",
       "      <td>Matt Slaymaker</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.xeno-canto.org/134896</td>\n",
       "      <td>asbfly/XC134896.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['song']</td>\n",
       "      <td>51.4030</td>\n",
       "      <td>104.6401</td>\n",
       "      <td>Muscicapa dauurica</td>\n",
       "      <td>Asian Brown Flycatcher</td>\n",
       "      <td>Magnus Hellström</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>https://www.xeno-canto.org/164848</td>\n",
       "      <td>asbfly/XC164848.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['song']</td>\n",
       "      <td>36.3319</td>\n",
       "      <td>127.3555</td>\n",
       "      <td>Muscicapa dauurica</td>\n",
       "      <td>Asian Brown Flycatcher</td>\n",
       "      <td>Stuart Fisher</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>https://www.xeno-canto.org/175797</td>\n",
       "      <td>asbfly/XC175797.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>21.1697</td>\n",
       "      <td>70.6005</td>\n",
       "      <td>Muscicapa dauurica</td>\n",
       "      <td>Asian Brown Flycatcher</td>\n",
       "      <td>vir joshi</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.xeno-canto.org/207738</td>\n",
       "      <td>asbfly/XC207738.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>15.5442</td>\n",
       "      <td>73.7733</td>\n",
       "      <td>Muscicapa dauurica</td>\n",
       "      <td>Asian Brown Flycatcher</td>\n",
       "      <td>Albert Lastukhin &amp; Sergei Karpeev</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.xeno-canto.org/209218</td>\n",
       "      <td>asbfly/XC209218.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24454</th>\n",
       "      <td>zitcis1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>43.5925</td>\n",
       "      <td>4.5434</td>\n",
       "      <td>Cisticola juncidis</td>\n",
       "      <td>Zitting Cisticola</td>\n",
       "      <td>Chèvremont Fabian</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://xeno-canto.org/845747</td>\n",
       "      <td>zitcis1/XC845747.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24455</th>\n",
       "      <td>zitcis1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>43.5925</td>\n",
       "      <td>4.5434</td>\n",
       "      <td>Cisticola juncidis</td>\n",
       "      <td>Zitting Cisticola</td>\n",
       "      <td>Chèvremont Fabian</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://xeno-canto.org/845817</td>\n",
       "      <td>zitcis1/XC845817.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24456</th>\n",
       "      <td>zitcis1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>51.1207</td>\n",
       "      <td>4.5607</td>\n",
       "      <td>Cisticola juncidis</td>\n",
       "      <td>Zitting Cisticola</td>\n",
       "      <td>Wim Jacobs</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://xeno-canto.org/856176</td>\n",
       "      <td>zitcis1/XC856176.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24457</th>\n",
       "      <td>zitcis1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>41.5607</td>\n",
       "      <td>-8.4236</td>\n",
       "      <td>Cisticola juncidis</td>\n",
       "      <td>Zitting Cisticola</td>\n",
       "      <td>Jorge Leitão</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>https://xeno-canto.org/856723</td>\n",
       "      <td>zitcis1/XC856723.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24458</th>\n",
       "      <td>zitcis1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['']</td>\n",
       "      <td>13.7747</td>\n",
       "      <td>100.8919</td>\n",
       "      <td>Cisticola juncidis</td>\n",
       "      <td>Zitting Cisticola</td>\n",
       "      <td>Sam Hambly</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://xeno-canto.org/858550</td>\n",
       "      <td>zitcis1/XC858550.ogg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24459 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      primary_label secondary_labels      type  latitude  longitude  \\\n",
       "0            asbfly               []  ['call']   39.2297   118.1987   \n",
       "1            asbfly               []  ['song']   51.4030   104.6401   \n",
       "2            asbfly               []  ['song']   36.3319   127.3555   \n",
       "3            asbfly               []  ['call']   21.1697    70.6005   \n",
       "4            asbfly               []  ['call']   15.5442    73.7733   \n",
       "...             ...              ...       ...       ...        ...   \n",
       "24454       zitcis1               []      ['']   43.5925     4.5434   \n",
       "24455       zitcis1               []      ['']   43.5925     4.5434   \n",
       "24456       zitcis1               []      ['']   51.1207     4.5607   \n",
       "24457       zitcis1               []      ['']   41.5607    -8.4236   \n",
       "24458       zitcis1               []      ['']   13.7747   100.8919   \n",
       "\n",
       "          scientific_name             common_name  \\\n",
       "0      Muscicapa dauurica  Asian Brown Flycatcher   \n",
       "1      Muscicapa dauurica  Asian Brown Flycatcher   \n",
       "2      Muscicapa dauurica  Asian Brown Flycatcher   \n",
       "3      Muscicapa dauurica  Asian Brown Flycatcher   \n",
       "4      Muscicapa dauurica  Asian Brown Flycatcher   \n",
       "...                   ...                     ...   \n",
       "24454  Cisticola juncidis       Zitting Cisticola   \n",
       "24455  Cisticola juncidis       Zitting Cisticola   \n",
       "24456  Cisticola juncidis       Zitting Cisticola   \n",
       "24457  Cisticola juncidis       Zitting Cisticola   \n",
       "24458  Cisticola juncidis       Zitting Cisticola   \n",
       "\n",
       "                                  author  \\\n",
       "0                         Matt Slaymaker   \n",
       "1                       Magnus Hellström   \n",
       "2                          Stuart Fisher   \n",
       "3                              vir joshi   \n",
       "4      Albert Lastukhin & Sergei Karpeev   \n",
       "...                                  ...   \n",
       "24454                  Chèvremont Fabian   \n",
       "24455                  Chèvremont Fabian   \n",
       "24456                         Wim Jacobs   \n",
       "24457                       Jorge Leitão   \n",
       "24458                         Sam Hambly   \n",
       "\n",
       "                                                 license  rating  \\\n",
       "0      Creative Commons Attribution-NonCommercial-Sha...     5.0   \n",
       "1      Creative Commons Attribution-NonCommercial-Sha...     2.5   \n",
       "2      Creative Commons Attribution-NonCommercial-Sha...     2.5   \n",
       "3      Creative Commons Attribution-NonCommercial-Sha...     4.0   \n",
       "4      Creative Commons Attribution-NonCommercial-Sha...     4.0   \n",
       "...                                                  ...     ...   \n",
       "24454  Creative Commons Attribution-NonCommercial-Sha...     5.0   \n",
       "24455  Creative Commons Attribution-NonCommercial-Sha...     4.0   \n",
       "24456  Creative Commons Attribution-NonCommercial-Sha...     4.0   \n",
       "24457  Creative Commons Attribution-NonCommercial-Sha...     4.5   \n",
       "24458  Creative Commons Attribution-NonCommercial-Sha...     5.0   \n",
       "\n",
       "                                     url              filename  \n",
       "0      https://www.xeno-canto.org/134896   asbfly/XC134896.ogg  \n",
       "1      https://www.xeno-canto.org/164848   asbfly/XC164848.ogg  \n",
       "2      https://www.xeno-canto.org/175797   asbfly/XC175797.ogg  \n",
       "3      https://www.xeno-canto.org/207738   asbfly/XC207738.ogg  \n",
       "4      https://www.xeno-canto.org/209218   asbfly/XC209218.ogg  \n",
       "...                                  ...                   ...  \n",
       "24454      https://xeno-canto.org/845747  zitcis1/XC845747.ogg  \n",
       "24455      https://xeno-canto.org/845817  zitcis1/XC845817.ogg  \n",
       "24456      https://xeno-canto.org/856176  zitcis1/XC856176.ogg  \n",
       "24457      https://xeno-canto.org/856723  zitcis1/XC856723.ogg  \n",
       "24458      https://xeno-canto.org/858550  zitcis1/XC858550.ogg  \n",
       "\n",
       "[24459 rows x 12 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_meta[[\"primary_label\", \"filename\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features_with_path(args):\n",
    "    ogg_file_path, max_length = args\n",
    "    return extract_audio_features(ogg_file_path, max_length)\n",
    "\n",
    "def extract_audio_features(ogg_file_path, max_length=22050*5):\n",
    "    y, sr = librosa.load(ogg_file_path, sr=None)\n",
    "    \n",
    "    # Ensure the audio is of fixed length\n",
    "    if len(y) < max_length:\n",
    "        y = np.pad(y, (0, max_length - len(y)), 'constant')\n",
    "    else:\n",
    "        y = y[:max_length]\n",
    "    \n",
    "    # Extract features\n",
    "    features = {}\n",
    "\n",
    "    # Mel spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    features['mel_spectrogram'] = S_dB\n",
    "\n",
    "    # MFCC\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    features['mfcc'] = mfcc\n",
    "\n",
    "    # Chroma feature\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    features['chroma'] = chroma\n",
    "\n",
    "    # Spectral contrast\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    features['spectral_contrast'] = spectral_contrast\n",
    "\n",
    "    # Tonnetz\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "    features['tonnetz'] = tonnetz\n",
    "\n",
    "    # Spectral centroid\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    features['spectral_centroid'] = spectral_centroid\n",
    "\n",
    "    # Spectral bandwidth\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    features['spectral_bandwidth'] = spectral_bandwidth\n",
    "\n",
    "    # Spectral rolloff\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    features['spectral_rolloff'] = spectral_rolloff\n",
    "\n",
    "    # Zero crossing rate\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n",
    "    features['zero_crossing_rate'] = zero_crossing_rate\n",
    "\n",
    "    # RMS\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    features['rms'] = rms\n",
    "\n",
    "    return features\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def load_audio(filepath, sample_rate=22050, max_length=22050*5):\n",
    "    audio, sr = librosa.load(filepath, sr=sample_rate)\n",
    "    if len(audio) < max_length:\n",
    "        audio = np.pad(audio, (0, max_length - len(audio)), 'constant')\n",
    "    else:\n",
    "        audio = audio[:max_length]\n",
    "    return audio, sr\n",
    "\n",
    "def spectral_gate(S, noise_mean, threshold):\n",
    "    mask = S > (noise_mean[:, np.newaxis] * threshold)\n",
    "    S_denoised = S * mask\n",
    "    return S_denoised\n",
    "\n",
    "def extract_audio_features_denoised(ogg_file_path, max_length=22050*5):\n",
    "    y, sr = load_audio(ogg_file_path, max_length=max_length)\n",
    "\n",
    "    # Noise reduction\n",
    "    noise_profile = y[:int(0.5 * sr)]\n",
    "    S_noise = librosa.feature.melspectrogram(y=noise_profile, sr=sr, n_mels=128, fmax=8000)\n",
    "    noise_mean = np.mean(S_noise, axis=1)\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "    S_denoised = spectral_gate(S, noise_mean, 2.2)\n",
    "    y_denoised = librosa.feature.inverse.mel_to_audio(S_denoised, sr=sr)\n",
    "\n",
    "    # Extract features from denoised audio\n",
    "    features = {}\n",
    "\n",
    "    # Mel spectrogram\n",
    "    S_dB = librosa.power_to_db(S_denoised, ref=np.max)\n",
    "    features['mel_spectrogram'] = S_dB\n",
    "\n",
    "    # MFCC\n",
    "    mfcc = librosa.feature.mfcc(y=y_denoised, sr=sr, n_mfcc=13)\n",
    "    features['mfcc'] = mfcc\n",
    "\n",
    "    # Chroma feature\n",
    "    chroma = librosa.feature.chroma_stft(y=y_denoised, sr=sr)\n",
    "    features['chroma'] = chroma\n",
    "\n",
    "    # Spectral contrast\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y_denoised, sr=sr)\n",
    "    features['spectral_contrast'] = spectral_contrast\n",
    "\n",
    "    # Tonnetz\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y_denoised), sr=sr)\n",
    "    features['tonnetz'] = tonnetz\n",
    "\n",
    "    # Spectral centroid\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y_denoised, sr=sr)\n",
    "    features['spectral_centroid'] = spectral_centroid\n",
    "\n",
    "    # Spectral bandwidth\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y_denoised, sr=sr)\n",
    "    features['spectral_bandwidth'] = spectral_bandwidth\n",
    "\n",
    "    # Spectral rolloff\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y_denoised, sr=sr)\n",
    "    features['spectral_rolloff'] = spectral_rolloff\n",
    "\n",
    "    # Zero crossing rate\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y_denoised)\n",
    "    features['zero_crossing_rate'] = zero_crossing_rate\n",
    "\n",
    "    # RMS\n",
    "    rms = librosa.feature.rms(y=y_denoised)\n",
    "    features['rms'] = rms\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature: mel_spectrogram\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-71.131088</td>\n",
       "      <td>-64.629059</td>\n",
       "      <td>-64.798882</td>\n",
       "      <td>-65.517334</td>\n",
       "      <td>-64.943108</td>\n",
       "      <td>-66.595901</td>\n",
       "      <td>...</td>\n",
       "      <td>-68.542099</td>\n",
       "      <td>-62.939053</td>\n",
       "      <td>-61.753540</td>\n",
       "      <td>-64.483032</td>\n",
       "      <td>-63.943794</td>\n",
       "      <td>-68.828384</td>\n",
       "      <td>-75.564575</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-69.745300</td>\n",
       "      <td>-58.308228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-78.152901</td>\n",
       "      <td>-77.862320</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-66.897995</td>\n",
       "      <td>-64.690155</td>\n",
       "      <td>-63.297081</td>\n",
       "      <td>-61.861908</td>\n",
       "      <td>-62.233330</td>\n",
       "      <td>-65.241440</td>\n",
       "      <td>-66.423111</td>\n",
       "      <td>-74.839699</td>\n",
       "      <td>-68.645683</td>\n",
       "      <td>-57.374542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-61.738266</td>\n",
       "      <td>-63.308044</td>\n",
       "      <td>-63.542503</td>\n",
       "      <td>-68.342384</td>\n",
       "      <td>-66.267365</td>\n",
       "      <td>-65.165871</td>\n",
       "      <td>-68.677277</td>\n",
       "      <td>-73.797104</td>\n",
       "      <td>-69.288116</td>\n",
       "      <td>-58.136024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-78.352684</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-79.861069</td>\n",
       "      <td>...</td>\n",
       "      <td>-64.366867</td>\n",
       "      <td>-71.845642</td>\n",
       "      <td>-70.448776</td>\n",
       "      <td>-73.537903</td>\n",
       "      <td>-71.473465</td>\n",
       "      <td>-71.168381</td>\n",
       "      <td>-72.104736</td>\n",
       "      <td>-69.305649</td>\n",
       "      <td>-67.361259</td>\n",
       "      <td>-57.354889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-73.917900</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-76.070633</td>\n",
       "      <td>-72.207756</td>\n",
       "      <td>-73.334656</td>\n",
       "      <td>-75.337105</td>\n",
       "      <td>-80.000000</td>\n",
       "      <td>-70.599350</td>\n",
       "      <td>-68.497894</td>\n",
       "      <td>-58.325111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3          4          5          6          7    \\\n",
       "0 -80.0 -80.0 -80.0 -80.0 -71.131088 -64.629059 -64.798882 -65.517334   \n",
       "1 -80.0 -80.0 -80.0 -80.0 -78.152901 -77.862320 -80.000000 -80.000000   \n",
       "2 -80.0 -80.0 -80.0 -80.0 -80.000000 -80.000000 -80.000000 -80.000000   \n",
       "3 -80.0 -80.0 -80.0 -80.0 -78.352684 -80.000000 -80.000000 -80.000000   \n",
       "4 -80.0 -80.0 -80.0 -80.0 -80.000000 -80.000000 -80.000000 -80.000000   \n",
       "\n",
       "         8          9    ...        206        207        208        209  \\\n",
       "0 -64.943108 -66.595901  ... -68.542099 -62.939053 -61.753540 -64.483032   \n",
       "1 -80.000000 -80.000000  ... -66.897995 -64.690155 -63.297081 -61.861908   \n",
       "2 -80.000000 -80.000000  ... -61.738266 -63.308044 -63.542503 -68.342384   \n",
       "3 -80.000000 -79.861069  ... -64.366867 -71.845642 -70.448776 -73.537903   \n",
       "4 -80.000000 -80.000000  ... -73.917900 -80.000000 -76.070633 -72.207756   \n",
       "\n",
       "         210        211        212        213        214        215  \n",
       "0 -63.943794 -68.828384 -75.564575 -80.000000 -69.745300 -58.308228  \n",
       "1 -62.233330 -65.241440 -66.423111 -74.839699 -68.645683 -57.374542  \n",
       "2 -66.267365 -65.165871 -68.677277 -73.797104 -69.288116 -58.136024  \n",
       "3 -71.473465 -71.168381 -72.104736 -69.305649 -67.361259 -57.354889  \n",
       "4 -73.334656 -75.337105 -80.000000 -70.599350 -68.497894 -58.325111  \n",
       "\n",
       "[5 rows x 216 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature: mfcc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-695.373413</td>\n",
       "      <td>-695.373413</td>\n",
       "      <td>-695.354370</td>\n",
       "      <td>-680.230347</td>\n",
       "      <td>-643.505615</td>\n",
       "      <td>-619.225769</td>\n",
       "      <td>-591.797607</td>\n",
       "      <td>-570.970276</td>\n",
       "      <td>-563.205383</td>\n",
       "      <td>-558.302429</td>\n",
       "      <td>...</td>\n",
       "      <td>-306.195435</td>\n",
       "      <td>-323.671509</td>\n",
       "      <td>-334.408813</td>\n",
       "      <td>-339.167542</td>\n",
       "      <td>-338.049377</td>\n",
       "      <td>-331.991302</td>\n",
       "      <td>-328.520233</td>\n",
       "      <td>-321.373627</td>\n",
       "      <td>-314.663361</td>\n",
       "      <td>-300.228394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>-1.203629</td>\n",
       "      <td>-5.267553</td>\n",
       "      <td>-6.745506</td>\n",
       "      <td>-6.516578</td>\n",
       "      <td>-5.824792</td>\n",
       "      <td>-6.088688</td>\n",
       "      <td>-6.661309</td>\n",
       "      <td>...</td>\n",
       "      <td>-41.372093</td>\n",
       "      <td>-18.021332</td>\n",
       "      <td>-13.957712</td>\n",
       "      <td>-14.097721</td>\n",
       "      <td>-14.342066</td>\n",
       "      <td>-13.824280</td>\n",
       "      <td>-19.953529</td>\n",
       "      <td>-27.281433</td>\n",
       "      <td>-24.620096</td>\n",
       "      <td>-22.640423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.026895</td>\n",
       "      <td>-17.487480</td>\n",
       "      <td>-48.594494</td>\n",
       "      <td>-65.481293</td>\n",
       "      <td>-84.124863</td>\n",
       "      <td>-95.908485</td>\n",
       "      <td>-98.228455</td>\n",
       "      <td>-101.852127</td>\n",
       "      <td>...</td>\n",
       "      <td>-174.743835</td>\n",
       "      <td>-175.672638</td>\n",
       "      <td>-179.184937</td>\n",
       "      <td>-179.671829</td>\n",
       "      <td>-181.474518</td>\n",
       "      <td>-182.583771</td>\n",
       "      <td>-186.912994</td>\n",
       "      <td>-188.352554</td>\n",
       "      <td>-176.718735</td>\n",
       "      <td>-133.417740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002972</td>\n",
       "      <td>2.805940</td>\n",
       "      <td>12.572548</td>\n",
       "      <td>14.241904</td>\n",
       "      <td>11.571680</td>\n",
       "      <td>11.357303</td>\n",
       "      <td>12.281723</td>\n",
       "      <td>13.987467</td>\n",
       "      <td>...</td>\n",
       "      <td>65.480469</td>\n",
       "      <td>54.011528</td>\n",
       "      <td>45.415066</td>\n",
       "      <td>47.022861</td>\n",
       "      <td>52.940224</td>\n",
       "      <td>50.364700</td>\n",
       "      <td>52.326881</td>\n",
       "      <td>65.002045</td>\n",
       "      <td>67.655022</td>\n",
       "      <td>71.527222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026674</td>\n",
       "      <td>8.966082</td>\n",
       "      <td>11.528910</td>\n",
       "      <td>8.192678</td>\n",
       "      <td>3.498780</td>\n",
       "      <td>-2.270911</td>\n",
       "      <td>-9.443663</td>\n",
       "      <td>-8.712378</td>\n",
       "      <td>...</td>\n",
       "      <td>-91.412827</td>\n",
       "      <td>-79.573364</td>\n",
       "      <td>-68.684639</td>\n",
       "      <td>-62.239941</td>\n",
       "      <td>-48.788322</td>\n",
       "      <td>-53.536591</td>\n",
       "      <td>-59.001015</td>\n",
       "      <td>-60.851425</td>\n",
       "      <td>-54.280022</td>\n",
       "      <td>-25.470638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0           1           2           3           4           5    \\\n",
       "0 -695.373413 -695.373413 -695.354370 -680.230347 -643.505615 -619.225769   \n",
       "1    0.000000    0.000000    0.000993   -1.203629   -5.267553   -6.745506   \n",
       "2    0.000000    0.000000   -0.026895  -17.487480  -48.594494  -65.481293   \n",
       "3    0.000000    0.000000   -0.002972    2.805940   12.572548   14.241904   \n",
       "4    0.000000    0.000000    0.026674    8.966082   11.528910    8.192678   \n",
       "\n",
       "          6           7           8           9    ...         206  \\\n",
       "0 -591.797607 -570.970276 -563.205383 -558.302429  ... -306.195435   \n",
       "1   -6.516578   -5.824792   -6.088688   -6.661309  ...  -41.372093   \n",
       "2  -84.124863  -95.908485  -98.228455 -101.852127  ... -174.743835   \n",
       "3   11.571680   11.357303   12.281723   13.987467  ...   65.480469   \n",
       "4    3.498780   -2.270911   -9.443663   -8.712378  ...  -91.412827   \n",
       "\n",
       "          207         208         209         210         211         212  \\\n",
       "0 -323.671509 -334.408813 -339.167542 -338.049377 -331.991302 -328.520233   \n",
       "1  -18.021332  -13.957712  -14.097721  -14.342066  -13.824280  -19.953529   \n",
       "2 -175.672638 -179.184937 -179.671829 -181.474518 -182.583771 -186.912994   \n",
       "3   54.011528   45.415066   47.022861   52.940224   50.364700   52.326881   \n",
       "4  -79.573364  -68.684639  -62.239941  -48.788322  -53.536591  -59.001015   \n",
       "\n",
       "          213         214         215  \n",
       "0 -321.373627 -314.663361 -300.228394  \n",
       "1  -27.281433  -24.620096  -22.640423  \n",
       "2 -188.352554 -176.718735 -133.417740  \n",
       "3   65.002045   67.655022   71.527222  \n",
       "4  -60.851425  -54.280022  -25.470638  \n",
       "\n",
       "[5 rows x 216 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature: chroma\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708295</td>\n",
       "      <td>0.640073</td>\n",
       "      <td>0.707260</td>\n",
       "      <td>0.544579</td>\n",
       "      <td>0.313167</td>\n",
       "      <td>0.284966</td>\n",
       "      <td>0.254466</td>\n",
       "      <td>0.247745</td>\n",
       "      <td>0.324839</td>\n",
       "      <td>0.466124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114438</td>\n",
       "      <td>0.319022</td>\n",
       "      <td>0.370808</td>\n",
       "      <td>0.189577</td>\n",
       "      <td>0.042638</td>\n",
       "      <td>0.022667</td>\n",
       "      <td>0.024571</td>\n",
       "      <td>0.025156</td>\n",
       "      <td>0.057726</td>\n",
       "      <td>0.065114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.595520</td>\n",
       "      <td>0.761191</td>\n",
       "      <td>0.617285</td>\n",
       "      <td>0.474223</td>\n",
       "      <td>0.340369</td>\n",
       "      <td>0.308571</td>\n",
       "      <td>0.460019</td>\n",
       "      <td>0.379475</td>\n",
       "      <td>0.303072</td>\n",
       "      <td>0.324227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102003</td>\n",
       "      <td>0.304017</td>\n",
       "      <td>0.354480</td>\n",
       "      <td>0.219087</td>\n",
       "      <td>0.042779</td>\n",
       "      <td>0.023521</td>\n",
       "      <td>0.017317</td>\n",
       "      <td>0.014781</td>\n",
       "      <td>0.018537</td>\n",
       "      <td>0.015970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.703772</td>\n",
       "      <td>0.855086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.511553</td>\n",
       "      <td>0.417635</td>\n",
       "      <td>0.389911</td>\n",
       "      <td>0.582626</td>\n",
       "      <td>0.537955</td>\n",
       "      <td>0.582327</td>\n",
       "      <td>0.628322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233476</td>\n",
       "      <td>0.346289</td>\n",
       "      <td>0.495336</td>\n",
       "      <td>0.291200</td>\n",
       "      <td>0.052698</td>\n",
       "      <td>0.023576</td>\n",
       "      <td>0.022504</td>\n",
       "      <td>0.014497</td>\n",
       "      <td>0.009743</td>\n",
       "      <td>0.012317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.729621</td>\n",
       "      <td>0.740412</td>\n",
       "      <td>0.929025</td>\n",
       "      <td>0.534931</td>\n",
       "      <td>0.516650</td>\n",
       "      <td>0.398323</td>\n",
       "      <td>0.351357</td>\n",
       "      <td>0.400521</td>\n",
       "      <td>0.466655</td>\n",
       "      <td>0.598876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588484</td>\n",
       "      <td>0.842825</td>\n",
       "      <td>0.873660</td>\n",
       "      <td>0.598802</td>\n",
       "      <td>0.114322</td>\n",
       "      <td>0.032365</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.024819</td>\n",
       "      <td>0.015485</td>\n",
       "      <td>0.013818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.966631</td>\n",
       "      <td>0.828872</td>\n",
       "      <td>0.708723</td>\n",
       "      <td>0.531631</td>\n",
       "      <td>0.500283</td>\n",
       "      <td>0.516987</td>\n",
       "      <td>0.690588</td>\n",
       "      <td>0.737944</td>\n",
       "      <td>0.609317</td>\n",
       "      <td>0.782331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807133</td>\n",
       "      <td>0.989460</td>\n",
       "      <td>0.916617</td>\n",
       "      <td>0.499404</td>\n",
       "      <td>0.099249</td>\n",
       "      <td>0.059439</td>\n",
       "      <td>0.052222</td>\n",
       "      <td>0.020962</td>\n",
       "      <td>0.019204</td>\n",
       "      <td>0.018052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.708295  0.640073  0.707260  0.544579  0.313167  0.284966  0.254466   \n",
       "1  0.595520  0.761191  0.617285  0.474223  0.340369  0.308571  0.460019   \n",
       "2  0.703772  0.855086  1.000000  0.511553  0.417635  0.389911  0.582626   \n",
       "3  0.729621  0.740412  0.929025  0.534931  0.516650  0.398323  0.351357   \n",
       "4  0.966631  0.828872  0.708723  0.531631  0.500283  0.516987  0.690588   \n",
       "\n",
       "        7         8         9    ...       206       207       208       209  \\\n",
       "0  0.247745  0.324839  0.466124  ...  0.114438  0.319022  0.370808  0.189577   \n",
       "1  0.379475  0.303072  0.324227  ...  0.102003  0.304017  0.354480  0.219087   \n",
       "2  0.537955  0.582327  0.628322  ...  0.233476  0.346289  0.495336  0.291200   \n",
       "3  0.400521  0.466655  0.598876  ...  0.588484  0.842825  0.873660  0.598802   \n",
       "4  0.737944  0.609317  0.782331  ...  0.807133  0.989460  0.916617  0.499404   \n",
       "\n",
       "        210       211       212       213       214       215  \n",
       "0  0.042638  0.022667  0.024571  0.025156  0.057726  0.065114  \n",
       "1  0.042779  0.023521  0.017317  0.014781  0.018537  0.015970  \n",
       "2  0.052698  0.023576  0.022504  0.014497  0.009743  0.012317  \n",
       "3  0.114322  0.032365  0.044700  0.024819  0.015485  0.013818  \n",
       "4  0.099249  0.059439  0.052222  0.020962  0.019204  0.018052  \n",
       "\n",
       "[5 rows x 216 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature: spectral_contrast\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.354793</td>\n",
       "      <td>11.055970</td>\n",
       "      <td>18.828681</td>\n",
       "      <td>13.490245</td>\n",
       "      <td>10.935963</td>\n",
       "      <td>17.737940</td>\n",
       "      <td>25.388246</td>\n",
       "      <td>23.359664</td>\n",
       "      <td>17.671976</td>\n",
       "      <td>16.888773</td>\n",
       "      <td>...</td>\n",
       "      <td>11.537984</td>\n",
       "      <td>17.709943</td>\n",
       "      <td>20.112632</td>\n",
       "      <td>13.965468</td>\n",
       "      <td>9.895126</td>\n",
       "      <td>7.356802</td>\n",
       "      <td>12.945463</td>\n",
       "      <td>12.272881</td>\n",
       "      <td>1.991081</td>\n",
       "      <td>0.512265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.655448</td>\n",
       "      <td>9.460604</td>\n",
       "      <td>13.589380</td>\n",
       "      <td>8.253302</td>\n",
       "      <td>6.355038</td>\n",
       "      <td>13.855677</td>\n",
       "      <td>5.488939</td>\n",
       "      <td>9.860909</td>\n",
       "      <td>11.352250</td>\n",
       "      <td>20.051979</td>\n",
       "      <td>...</td>\n",
       "      <td>10.357248</td>\n",
       "      <td>7.098002</td>\n",
       "      <td>13.164112</td>\n",
       "      <td>9.920544</td>\n",
       "      <td>10.515016</td>\n",
       "      <td>9.414235</td>\n",
       "      <td>11.883525</td>\n",
       "      <td>5.564786</td>\n",
       "      <td>6.395963</td>\n",
       "      <td>1.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.175265</td>\n",
       "      <td>17.008937</td>\n",
       "      <td>10.132582</td>\n",
       "      <td>9.868057</td>\n",
       "      <td>10.725284</td>\n",
       "      <td>9.491105</td>\n",
       "      <td>13.383158</td>\n",
       "      <td>10.775962</td>\n",
       "      <td>15.808067</td>\n",
       "      <td>12.061553</td>\n",
       "      <td>...</td>\n",
       "      <td>10.227426</td>\n",
       "      <td>11.273117</td>\n",
       "      <td>17.308825</td>\n",
       "      <td>12.270414</td>\n",
       "      <td>16.327901</td>\n",
       "      <td>13.583436</td>\n",
       "      <td>14.425934</td>\n",
       "      <td>16.306118</td>\n",
       "      <td>14.946777</td>\n",
       "      <td>10.478192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.181861</td>\n",
       "      <td>18.487100</td>\n",
       "      <td>7.492485</td>\n",
       "      <td>13.558920</td>\n",
       "      <td>11.697548</td>\n",
       "      <td>16.721800</td>\n",
       "      <td>12.770150</td>\n",
       "      <td>18.760126</td>\n",
       "      <td>14.912207</td>\n",
       "      <td>12.216167</td>\n",
       "      <td>...</td>\n",
       "      <td>10.045255</td>\n",
       "      <td>14.306967</td>\n",
       "      <td>15.011778</td>\n",
       "      <td>14.692581</td>\n",
       "      <td>13.983972</td>\n",
       "      <td>16.311010</td>\n",
       "      <td>12.784020</td>\n",
       "      <td>19.127118</td>\n",
       "      <td>17.829591</td>\n",
       "      <td>11.865277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.863674</td>\n",
       "      <td>11.875513</td>\n",
       "      <td>12.481405</td>\n",
       "      <td>14.705043</td>\n",
       "      <td>15.351461</td>\n",
       "      <td>15.150303</td>\n",
       "      <td>13.763269</td>\n",
       "      <td>11.776819</td>\n",
       "      <td>11.068494</td>\n",
       "      <td>17.253260</td>\n",
       "      <td>...</td>\n",
       "      <td>18.722942</td>\n",
       "      <td>19.851819</td>\n",
       "      <td>14.893240</td>\n",
       "      <td>17.568424</td>\n",
       "      <td>17.895550</td>\n",
       "      <td>21.664876</td>\n",
       "      <td>21.846788</td>\n",
       "      <td>22.297705</td>\n",
       "      <td>16.560607</td>\n",
       "      <td>15.399926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3          4          5    \\\n",
       "0   8.354793  11.055970  18.828681  13.490245  10.935963  17.737940   \n",
       "1   5.655448   9.460604  13.589380   8.253302   6.355038  13.855677   \n",
       "2   7.175265  17.008937  10.132582   9.868057  10.725284   9.491105   \n",
       "3  13.181861  18.487100   7.492485  13.558920  11.697548  16.721800   \n",
       "4  11.863674  11.875513  12.481405  14.705043  15.351461  15.150303   \n",
       "\n",
       "         6          7          8          9    ...        206        207  \\\n",
       "0  25.388246  23.359664  17.671976  16.888773  ...  11.537984  17.709943   \n",
       "1   5.488939   9.860909  11.352250  20.051979  ...  10.357248   7.098002   \n",
       "2  13.383158  10.775962  15.808067  12.061553  ...  10.227426  11.273117   \n",
       "3  12.770150  18.760126  14.912207  12.216167  ...  10.045255  14.306967   \n",
       "4  13.763269  11.776819  11.068494  17.253260  ...  18.722942  19.851819   \n",
       "\n",
       "         208        209        210        211        212        213  \\\n",
       "0  20.112632  13.965468   9.895126   7.356802  12.945463  12.272881   \n",
       "1  13.164112   9.920544  10.515016   9.414235  11.883525   5.564786   \n",
       "2  17.308825  12.270414  16.327901  13.583436  14.425934  16.306118   \n",
       "3  15.011778  14.692581  13.983972  16.311010  12.784020  19.127118   \n",
       "4  14.893240  17.568424  17.895550  21.664876  21.846788  22.297705   \n",
       "\n",
       "         214        215  \n",
       "0   1.991081   0.512265  \n",
       "1   6.395963   1.331900  \n",
       "2  14.946777  10.478192  \n",
       "3  17.829591  11.865277  \n",
       "4  16.560607  15.399926  \n",
       "\n",
       "[5 rows x 216 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature: tonnetz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.053550</td>\n",
       "      <td>0.046576</td>\n",
       "      <td>0.069057</td>\n",
       "      <td>-0.033937</td>\n",
       "      <td>0.008213</td>\n",
       "      <td>0.029023</td>\n",
       "      <td>0.030646</td>\n",
       "      <td>-0.002728</td>\n",
       "      <td>0.043914</td>\n",
       "      <td>0.068861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>-0.044043</td>\n",
       "      <td>-0.096815</td>\n",
       "      <td>-0.092799</td>\n",
       "      <td>-0.093778</td>\n",
       "      <td>-0.024391</td>\n",
       "      <td>0.022639</td>\n",
       "      <td>0.041814</td>\n",
       "      <td>-0.040458</td>\n",
       "      <td>0.034045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006453</td>\n",
       "      <td>-0.015780</td>\n",
       "      <td>0.014258</td>\n",
       "      <td>0.041921</td>\n",
       "      <td>0.020128</td>\n",
       "      <td>0.015061</td>\n",
       "      <td>0.021392</td>\n",
       "      <td>-0.037868</td>\n",
       "      <td>-0.028053</td>\n",
       "      <td>0.050517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011720</td>\n",
       "      <td>-0.085404</td>\n",
       "      <td>0.003190</td>\n",
       "      <td>-0.085780</td>\n",
       "      <td>0.012057</td>\n",
       "      <td>-0.011803</td>\n",
       "      <td>-0.030609</td>\n",
       "      <td>0.036463</td>\n",
       "      <td>-0.000963</td>\n",
       "      <td>-0.023262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.059750</td>\n",
       "      <td>0.059457</td>\n",
       "      <td>0.074420</td>\n",
       "      <td>0.068720</td>\n",
       "      <td>0.029005</td>\n",
       "      <td>-0.084037</td>\n",
       "      <td>0.049583</td>\n",
       "      <td>-0.014404</td>\n",
       "      <td>0.035721</td>\n",
       "      <td>0.032192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.045641</td>\n",
       "      <td>0.028443</td>\n",
       "      <td>-0.021292</td>\n",
       "      <td>-0.084611</td>\n",
       "      <td>0.078510</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>-0.084613</td>\n",
       "      <td>0.032052</td>\n",
       "      <td>-0.095086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063891</td>\n",
       "      <td>0.044799</td>\n",
       "      <td>0.006855</td>\n",
       "      <td>0.047675</td>\n",
       "      <td>0.045305</td>\n",
       "      <td>0.079124</td>\n",
       "      <td>0.070752</td>\n",
       "      <td>0.129191</td>\n",
       "      <td>0.091020</td>\n",
       "      <td>0.047130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062414</td>\n",
       "      <td>-0.093077</td>\n",
       "      <td>0.102964</td>\n",
       "      <td>0.102695</td>\n",
       "      <td>-0.054047</td>\n",
       "      <td>0.059749</td>\n",
       "      <td>-0.066663</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>-0.094087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003680</td>\n",
       "      <td>-0.011251</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>-0.045971</td>\n",
       "      <td>0.009795</td>\n",
       "      <td>-0.042154</td>\n",
       "      <td>0.018350</td>\n",
       "      <td>0.020736</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>-0.031041</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034227</td>\n",
       "      <td>-0.024232</td>\n",
       "      <td>-0.041018</td>\n",
       "      <td>-0.120868</td>\n",
       "      <td>-0.063487</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>-0.023469</td>\n",
       "      <td>-0.047408</td>\n",
       "      <td>-0.042899</td>\n",
       "      <td>-0.044499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.053550  0.046576  0.069057 -0.033937  0.008213  0.029023  0.030646   \n",
       "1  0.006453 -0.015780  0.014258  0.041921  0.020128  0.015061  0.021392   \n",
       "2  0.059750  0.059457  0.074420  0.068720  0.029005 -0.084037  0.049583   \n",
       "3  0.063891  0.044799  0.006855  0.047675  0.045305  0.079124  0.070752   \n",
       "4 -0.003680 -0.011251  0.000642 -0.045971  0.009795 -0.042154  0.018350   \n",
       "\n",
       "        7         8         9    ...       206       207       208       209  \\\n",
       "0 -0.002728  0.043914  0.068861  ...  0.000652 -0.044043 -0.096815 -0.092799   \n",
       "1 -0.037868 -0.028053  0.050517  ... -0.011720 -0.085404  0.003190 -0.085780   \n",
       "2 -0.014404  0.035721  0.032192  ...  0.006173  0.045641  0.028443 -0.021292   \n",
       "3  0.129191  0.091020  0.047130  ... -0.062414 -0.093077  0.102964  0.102695   \n",
       "4  0.020736  0.005233 -0.031041  ... -0.034227 -0.024232 -0.041018 -0.120868   \n",
       "\n",
       "        210       211       212       213       214       215  \n",
       "0 -0.093778 -0.024391  0.022639  0.041814 -0.040458  0.034045  \n",
       "1  0.012057 -0.011803 -0.030609  0.036463 -0.000963 -0.023262  \n",
       "2 -0.084611  0.078510  0.001111 -0.084613  0.032052 -0.095086  \n",
       "3 -0.054047  0.059749 -0.066663  0.001458  0.000603 -0.094087  \n",
       "4 -0.063487  0.012695 -0.023469 -0.047408 -0.042899 -0.044499  \n",
       "\n",
       "[5 rows x 216 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature: spectral_centroid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7677.594522</td>\n",
       "      <td>6883.725464</td>\n",
       "      <td>5391.344823</td>\n",
       "      <td>4644.819335</td>\n",
       "      <td>4256.43529</td>\n",
       "      <td>4117.689942</td>\n",
       "      <td>4004.929086</td>\n",
       "      <td>3984.257218</td>\n",
       "      <td>4061.659922</td>\n",
       "      <td>4006.887847</td>\n",
       "      <td>...</td>\n",
       "      <td>5067.518515</td>\n",
       "      <td>4680.967893</td>\n",
       "      <td>4378.585674</td>\n",
       "      <td>4230.923206</td>\n",
       "      <td>3971.862278</td>\n",
       "      <td>3890.484845</td>\n",
       "      <td>3928.581672</td>\n",
       "      <td>4022.806</td>\n",
       "      <td>4128.249837</td>\n",
       "      <td>4278.453318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0            1            2            3           4    \\\n",
       "0  7677.594522  6883.725464  5391.344823  4644.819335  4256.43529   \n",
       "\n",
       "           5            6            7            8            9    ...  \\\n",
       "0  4117.689942  4004.929086  3984.257218  4061.659922  4006.887847  ...   \n",
       "\n",
       "           206          207          208          209          210  \\\n",
       "0  5067.518515  4680.967893  4378.585674  4230.923206  3971.862278   \n",
       "\n",
       "           211          212       213          214          215  \n",
       "0  3890.484845  3928.581672  4022.806  4128.249837  4278.453318  \n",
       "\n",
       "[1 rows x 216 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature: spectral_bandwidth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4317.705768</td>\n",
       "      <td>4241.620262</td>\n",
       "      <td>3764.47843</td>\n",
       "      <td>3102.94098</td>\n",
       "      <td>2587.063436</td>\n",
       "      <td>2375.034378</td>\n",
       "      <td>2266.970457</td>\n",
       "      <td>2138.180219</td>\n",
       "      <td>2141.049613</td>\n",
       "      <td>2112.659167</td>\n",
       "      <td>...</td>\n",
       "      <td>1282.843863</td>\n",
       "      <td>1538.289848</td>\n",
       "      <td>1647.334525</td>\n",
       "      <td>1606.466814</td>\n",
       "      <td>1534.741298</td>\n",
       "      <td>1504.182266</td>\n",
       "      <td>1461.898212</td>\n",
       "      <td>1388.453856</td>\n",
       "      <td>1453.93369</td>\n",
       "      <td>1711.389557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0            1           2           3            4            5    \\\n",
       "0  4317.705768  4241.620262  3764.47843  3102.94098  2587.063436  2375.034378   \n",
       "\n",
       "           6            7            8            9    ...          206  \\\n",
       "0  2266.970457  2138.180219  2141.049613  2112.659167  ...  1282.843863   \n",
       "\n",
       "           207          208          209          210          211  \\\n",
       "0  1538.289848  1647.334525  1606.466814  1534.741298  1504.182266   \n",
       "\n",
       "           212          213         214          215  \n",
       "0  1461.898212  1388.453856  1453.93369  1711.389557  \n",
       "\n",
       "[1 rows x 216 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature: spectral_rolloff\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13000.0</td>\n",
       "      <td>12156.25</td>\n",
       "      <td>10078.125</td>\n",
       "      <td>7750.0</td>\n",
       "      <td>6421.875</td>\n",
       "      <td>6343.75</td>\n",
       "      <td>6312.5</td>\n",
       "      <td>6078.125</td>\n",
       "      <td>6203.125</td>\n",
       "      <td>6015.625</td>\n",
       "      <td>...</td>\n",
       "      <td>6109.375</td>\n",
       "      <td>6093.75</td>\n",
       "      <td>5968.75</td>\n",
       "      <td>5796.875</td>\n",
       "      <td>5703.125</td>\n",
       "      <td>5515.625</td>\n",
       "      <td>5375.0</td>\n",
       "      <td>5375.0</td>\n",
       "      <td>5609.375</td>\n",
       "      <td>6031.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1          2       3         4        5       6         7    \\\n",
       "0  13000.0  12156.25  10078.125  7750.0  6421.875  6343.75  6312.5  6078.125   \n",
       "\n",
       "        8         9    ...       206      207      208       209       210  \\\n",
       "0  6203.125  6015.625  ...  6109.375  6093.75  5968.75  5796.875  5703.125   \n",
       "\n",
       "        211     212     213       214      215  \n",
       "0  5515.625  5375.0  5375.0  5609.375  6031.25  \n",
       "\n",
       "[1 rows x 216 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature: zero_crossing_rate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.223633</td>\n",
       "      <td>0.297852</td>\n",
       "      <td>0.361328</td>\n",
       "      <td>0.295898</td>\n",
       "      <td>0.250488</td>\n",
       "      <td>0.234863</td>\n",
       "      <td>0.235352</td>\n",
       "      <td>0.245117</td>\n",
       "      <td>0.246582</td>\n",
       "      <td>0.242676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323242</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.288086</td>\n",
       "      <td>0.258301</td>\n",
       "      <td>0.239258</td>\n",
       "      <td>0.218262</td>\n",
       "      <td>0.212891</td>\n",
       "      <td>0.219727</td>\n",
       "      <td>0.186035</td>\n",
       "      <td>0.133789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.223633  0.297852  0.361328  0.295898  0.250488  0.234863  0.235352   \n",
       "\n",
       "        7         8         9    ...       206     207       208       209  \\\n",
       "0  0.245117  0.246582  0.242676  ...  0.323242  0.3125  0.288086  0.258301   \n",
       "\n",
       "        210       211       212       213       214       215  \n",
       "0  0.239258  0.218262  0.212891  0.219727  0.186035  0.133789  \n",
       "\n",
       "[1 rows x 216 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature: rms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.00027</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090907</td>\n",
       "      <td>0.066837</td>\n",
       "      <td>0.030106</td>\n",
       "      <td>0.029994</td>\n",
       "      <td>0.035177</td>\n",
       "      <td>0.04654</td>\n",
       "      <td>0.061427</td>\n",
       "      <td>0.067807</td>\n",
       "      <td>0.066481</td>\n",
       "      <td>0.057839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3        4         5         6    \\\n",
       "0  0.000013  0.000031  0.000073  0.000177  0.00027  0.000456  0.000616   \n",
       "\n",
       "        7         8         9    ...       206       207       208       209  \\\n",
       "0  0.000727  0.000869  0.000953  ...  0.090907  0.066837  0.030106  0.029994   \n",
       "\n",
       "        210      211       212       213       214       215  \n",
       "0  0.035177  0.04654  0.061427  0.067807  0.066481  0.057839  \n",
       "\n",
       "[1 rows x 216 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "audio_features = extract_audio_features('./birdclef-2024/train_audio/asbfly/XC49755.ogg')\n",
    "\n",
    "# Convert features to a dictionary of DataFrames for better visualization\n",
    "features_df = {key: pd.DataFrame(value) for key, value in audio_features.items()}\n",
    "\n",
    "# Display the extracted features\n",
    "for feature_name, df in features_df.items():\n",
    "    print(f\"\\nFeature: {feature_name}\")\n",
    "    display(df.head())  # Using display() from IPython.display for better visualization in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_features(features):\n",
    "    aggregated_features = {}\n",
    "    for key, value in features.items():\n",
    "        aggregated_features[key] = {\n",
    "            'mean': np.mean(value, axis=1),\n",
    "            'std': np.std(value, axis=1),\n",
    "            'min': np.min(value, axis=1),\n",
    "            'max': np.max(value, axis=1)\n",
    "        }\n",
    "    return aggregated_features\n",
    "\n",
    "def format_features(aggregated_features):\n",
    "    formatted_features = []\n",
    "    for key in aggregated_features:\n",
    "        for stat in aggregated_features[key]:\n",
    "            formatted_features.extend(aggregated_features[key][stat])\n",
    "    return np.array(formatted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(684,)\n",
      "[-6.78951569e+01 -7.15780640e+01 -7.08037949e+01 -7.23653183e+01\n",
      " -7.57353821e+01 -7.60923462e+01 -7.72794800e+01 -7.69549332e+01\n",
      " -7.69535065e+01 -7.48166962e+01 -7.59738770e+01 -7.50231094e+01\n",
      " -7.48816452e+01 -7.21270599e+01 -7.18847809e+01 -6.99784622e+01\n",
      " -6.92440033e+01 -6.64677048e+01 -6.55641098e+01 -6.29078407e+01\n",
      " -6.35348587e+01 -6.19041100e+01 -6.14968414e+01 -6.00429764e+01\n",
      " -5.98880310e+01 -5.87049522e+01 -5.84746971e+01 -5.72715187e+01\n",
      " -5.75240135e+01 -5.58471870e+01 -5.55498352e+01 -5.40977554e+01\n",
      " -5.38592224e+01 -5.26902885e+01 -5.27443390e+01 -5.14852333e+01\n",
      " -5.19623528e+01 -5.10422897e+01 -5.14217949e+01 -5.06474075e+01\n",
      " -5.04771423e+01 -4.96308174e+01 -5.02442932e+01 -4.85827980e+01\n",
      " -4.86090584e+01 -4.86892738e+01 -4.79537125e+01 -4.74637413e+01\n",
      " -4.74799500e+01 -4.70840187e+01 -4.66258774e+01 -4.66038132e+01\n",
      " -4.59247398e+01 -4.57369804e+01 -4.46376686e+01 -4.42834740e+01\n",
      " -4.41352196e+01 -4.43094559e+01 -4.40298653e+01 -4.37026901e+01\n",
      " -4.32755165e+01 -4.15394707e+01 -3.99824142e+01 -3.92749367e+01\n",
      " -3.92779465e+01 -3.87621880e+01 -3.89020996e+01 -3.95993652e+01\n",
      " -3.94569855e+01 -3.88339195e+01 -3.77464409e+01 -3.76755600e+01\n",
      " -3.86658936e+01 -3.90819778e+01 -4.00564690e+01 -3.94911041e+01\n",
      " -3.94883385e+01 -3.96045380e+01 -3.94877930e+01 -3.88125076e+01\n",
      " -3.85692368e+01 -3.84432793e+01 -3.79438934e+01 -3.67786636e+01\n",
      " -3.60531273e+01 -3.50859528e+01 -3.31388969e+01 -3.15234241e+01\n",
      " -3.29588699e+01 -3.34340057e+01 -3.32153320e+01 -3.30964012e+01\n",
      " -3.21221428e+01 -3.20766640e+01 -3.23976784e+01 -3.35128937e+01\n",
      " -3.53165016e+01 -3.57124748e+01 -3.53675613e+01 -3.51138268e+01\n",
      " -3.53335991e+01 -3.51394196e+01 -3.54274139e+01 -3.58785133e+01\n",
      " -3.50933533e+01 -3.49577370e+01 -3.44668770e+01 -3.33479576e+01\n",
      " -3.35960121e+01 -3.35661087e+01 -3.43455620e+01 -3.52197037e+01\n",
      " -3.56412086e+01 -3.67417564e+01 -3.71535378e+01 -3.82551422e+01\n",
      " -3.89039726e+01 -3.93453941e+01 -4.11509705e+01 -4.36231461e+01\n",
      " -4.62478561e+01 -4.63357735e+01 -4.67927780e+01 -4.65924492e+01\n",
      " -4.74542236e+01 -4.95269623e+01 -5.05189056e+01 -5.26859665e+01\n",
      "  6.56299448e+00  6.18674850e+00  6.55345249e+00  5.51750898e+00\n",
      "  4.18041134e+00  4.23090696e+00  3.52300167e+00  3.76272321e+00\n",
      "  3.88103509e+00  4.66800356e+00  4.14625502e+00  4.58000612e+00\n",
      "  4.80571890e+00  5.48072720e+00  5.57625580e+00  6.21433544e+00\n",
      "  6.24613905e+00  6.18533564e+00  6.36747169e+00  6.70571947e+00\n",
      "  7.04883575e+00  6.95166397e+00  7.01741982e+00  7.15030336e+00\n",
      "  6.66945076e+00  6.77303505e+00  6.60638714e+00  6.78836250e+00\n",
      "  7.33285570e+00  7.65487671e+00  7.20324278e+00  7.25675488e+00\n",
      "  7.44987154e+00  7.43501425e+00  7.61926603e+00  7.73002291e+00\n",
      "  8.07750893e+00  8.22564697e+00  8.29813480e+00  8.07737827e+00\n",
      "  7.72395992e+00  7.92968988e+00  7.91483164e+00  7.62226868e+00\n",
      "  8.00218964e+00  7.69910574e+00  7.59039450e+00  7.76662970e+00\n",
      "  8.30974960e+00  8.12859058e+00  8.16521549e+00  8.16108894e+00\n",
      "  8.06208038e+00  8.64978027e+00  7.88691998e+00  7.88651323e+00\n",
      "  8.45182991e+00  8.15819359e+00  8.03371143e+00  7.80946159e+00\n",
      "  8.20490742e+00  8.17265511e+00  8.35328579e+00  8.57586479e+00\n",
      "  8.34617424e+00  8.30582714e+00  8.58960915e+00  8.80618191e+00\n",
      "  8.86267662e+00  1.05642033e+01  1.14022331e+01  1.11419153e+01\n",
      "  1.06528606e+01  9.42436409e+00  9.12918854e+00  9.68060970e+00\n",
      "  9.20140266e+00  9.09740639e+00  9.50028229e+00  9.99795818e+00\n",
      "  9.65746117e+00  9.29731369e+00  9.82576370e+00  1.01280441e+01\n",
      "  1.06159306e+01  1.11368818e+01  1.23328352e+01  1.34491043e+01\n",
      "  1.18223820e+01  1.10503883e+01  1.09108925e+01  1.06719265e+01\n",
      "  1.09011621e+01  1.12801561e+01  1.20857611e+01  1.12677145e+01\n",
      "  1.08428488e+01  1.07253704e+01  1.10011864e+01  1.18069811e+01\n",
      "  1.18015528e+01  1.22768888e+01  1.24530239e+01  1.22428036e+01\n",
      "  1.28905382e+01  1.36546593e+01  1.39125195e+01  1.42805138e+01\n",
      "  1.41864090e+01  1.46822739e+01  1.43236561e+01  1.40023136e+01\n",
      "  1.32522650e+01  1.20269718e+01  1.24992142e+01  1.25431204e+01\n",
      "  1.28116131e+01  1.27642851e+01  1.19424372e+01  1.12910728e+01\n",
      "  1.11514626e+01  1.02449818e+01  1.11749325e+01  1.22489347e+01\n",
      "  1.25241871e+01  1.15843544e+01  1.16579790e+01  1.18155575e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -8.00000000e+01 -8.00000000e+01 -8.00000000e+01 -8.00000000e+01\n",
      " -5.58989983e+01 -5.53959618e+01 -5.41812973e+01 -5.73548889e+01\n",
      " -5.83251114e+01 -5.71344910e+01 -5.75622215e+01 -5.79604683e+01\n",
      " -5.83976135e+01 -5.69162674e+01 -5.75878296e+01 -5.60618629e+01\n",
      " -5.50282974e+01 -5.40371857e+01 -5.43599167e+01 -5.18604851e+01\n",
      " -5.38039742e+01 -5.16452866e+01 -5.41225090e+01 -5.16966972e+01\n",
      " -5.20693436e+01 -4.94093895e+01 -4.70459824e+01 -4.82545662e+01\n",
      " -4.81084404e+01 -4.82258759e+01 -4.76104126e+01 -4.33761940e+01\n",
      " -4.47535896e+01 -4.39473267e+01 -4.41831665e+01 -4.31752701e+01\n",
      " -4.27493286e+01 -4.07317505e+01 -4.05458221e+01 -3.97431412e+01\n",
      " -4.04049149e+01 -3.68087997e+01 -3.79459915e+01 -3.91301994e+01\n",
      " -4.03443451e+01 -3.89789505e+01 -3.88422165e+01 -3.70711975e+01\n",
      " -3.79855576e+01 -3.90382538e+01 -3.56820030e+01 -3.58552933e+01\n",
      " -3.61914864e+01 -3.70150146e+01 -3.58783340e+01 -3.54745102e+01\n",
      " -3.41478653e+01 -3.54403305e+01 -3.49919510e+01 -3.37266884e+01\n",
      " -3.25184631e+01 -3.41697502e+01 -3.31934738e+01 -3.42239304e+01\n",
      " -2.90324287e+01 -2.81320724e+01 -2.88013763e+01 -2.80316296e+01\n",
      " -2.82570419e+01 -2.85703602e+01 -2.78563728e+01 -2.27793846e+01\n",
      " -2.02830105e+01 -9.31635189e+00 -6.44812393e+00 -1.29913139e+01\n",
      " -1.47169743e+01 -2.24343472e+01 -2.91979141e+01 -2.57494621e+01\n",
      " -2.54320374e+01 -2.24489536e+01 -2.46336040e+01 -2.46523056e+01\n",
      " -2.11715031e+01 -2.32916107e+01 -2.47804298e+01 -2.35608349e+01\n",
      " -2.10312634e+01 -1.83106289e+01 -1.32193012e+01 -1.04374714e+01\n",
      " -1.37160959e+01 -1.26200771e+01 -1.11487732e+01 -1.67393608e+01\n",
      " -9.03981781e+00 -7.31603050e+00 -8.54920578e+00 -9.53508854e+00\n",
      " -7.58930969e+00 -1.09238672e+01 -1.24443722e+01 -1.01276379e+01\n",
      " -9.14618683e+00 -9.87414932e+00 -6.04185486e+00 -9.62997246e+00\n",
      " -7.19623661e+00 -1.26617241e+00 -3.28722000e+00 -3.13824272e+00\n",
      " -2.87443161e-01 -1.68604469e+00  0.00000000e+00 -6.46774292e-01\n",
      " -3.31578827e+00 -8.79345417e+00 -5.26008987e+00 -3.25497818e+00\n",
      " -1.62715340e+00 -2.09276581e+00 -6.58005238e+00 -1.85611172e+01\n",
      " -2.38423748e+01 -2.52900162e+01 -2.22083569e+01 -1.10508595e+01\n",
      " -8.56939220e+00 -1.62574615e+01 -1.27492228e+01 -1.36126022e+01\n",
      " -3.78031799e+02 -9.77883911e+00 -1.62413513e+02  3.93792458e+01\n",
      " -4.95024490e+01 -6.25760555e-01 -3.73189735e+00 -7.72480106e+00\n",
      "  5.14817095e+00 -9.43658173e-01  8.90023899e+00  4.81888485e+00\n",
      "  2.91285347e-02  8.03989258e+01  1.78389187e+01  3.11695137e+01\n",
      "  1.88223610e+01  1.95934772e+01  9.88203430e+00  9.81474876e+00\n",
      "  9.35540676e+00  6.19378090e+00  6.64263439e+00  8.38670349e+00\n",
      "  7.03471041e+00  6.25368261e+00 -6.95373413e+02 -6.44163055e+01\n",
      " -1.98375519e+02 -2.97238398e-03 -9.14128265e+01 -1.87220078e+01\n",
      " -2.72821121e+01 -3.06996994e+01 -1.04680920e+01 -1.74845657e+01\n",
      " -1.79145947e+01 -1.83506489e+01 -1.67871647e+01 -2.93432312e+02\n",
      "  2.18189926e+01  0.00000000e+00  8.81842575e+01  1.15289097e+01\n",
      "  2.74193649e+01  1.70098763e+01  1.13695221e+01  2.45030174e+01\n",
      "  2.21738167e+01  2.91056252e+01  2.93264084e+01  2.02210732e+01\n",
      "  3.57199669e-01  3.07124913e-01  3.35543245e-01  3.79172206e-01\n",
      "  3.74025345e-01  3.68796736e-01  5.04714608e-01  5.14655232e-01\n",
      "  4.76456851e-01  5.64261794e-01  4.03102040e-01  3.45245868e-01\n",
      "  2.93734103e-01  2.65951812e-01  2.80152529e-01  2.76178867e-01\n",
      "  2.77236491e-01  2.68116444e-01  3.31215024e-01  3.24809521e-01\n",
      "  3.44703048e-01  3.66285741e-01  2.83933997e-01  2.77644515e-01\n",
      "  1.67002832e-03  3.50483810e-03  3.55488108e-03  3.75877111e-03\n",
      "  6.80759642e-03  1.12461336e-02  1.24585917e-02  4.05499246e-03\n",
      "  1.84830569e-03  1.53691496e-03  1.69276062e-03  1.32982619e-03\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  1.62796840e+01  1.06705893e+01  1.42380687e+01  1.45769883e+01\n",
      "  1.70175472e+01  1.90484405e+01  6.91645007e+01  5.76671939e+00\n",
      "  3.39324799e+00  3.19839626e+00  3.02200336e+00  3.37177983e+00\n",
      "  3.76862514e+00  9.06251354e+00  5.12264553e-01  1.33189977e+00\n",
      "  6.94615049e+00  7.49248545e+00  9.89702858e+00  1.29031502e+01\n",
      "  1.64578900e+01  4.13874441e+01  2.38273993e+01  2.72583739e+01\n",
      "  2.54605702e+01  2.47335460e+01  3.22489909e+01  8.43767014e+01\n",
      "  3.01676464e-02 -4.96030862e-02  8.57828109e-03 -1.20755398e-02\n",
      " -2.17345903e-02  2.96799421e-02  7.32131957e-02  8.62107497e-02\n",
      "  8.40756654e-02  1.11186105e-01  4.14829638e-02  4.96818389e-02\n",
      " -1.32700045e-01 -3.35049950e-01 -2.59177864e-01 -3.18522770e-01\n",
      " -1.80899387e-01 -8.71330742e-02  3.30933152e-01  1.39883232e-01\n",
      "  2.68008487e-01  2.97368016e-01  8.07791594e-02  2.04888635e-01\n",
      "  4.20911135e+03  5.54060369e+02  3.51706073e+03  7.67759452e+03\n",
      "  1.70350726e+03  3.90775028e+02  9.74245254e+02  4.31770577e+03\n",
      "  5.86841725e+03  8.78995727e+02  4.96875000e+03  1.30000000e+04\n",
      "  2.51867224e-01  4.45658672e-02  1.33789062e-01  4.06738281e-01\n",
      "  3.53122093e-02  3.67540419e-02  1.29457030e-05  1.58307001e-01]\n"
     ]
    }
   ],
   "source": [
    "ogg_file_path = './birdclef-2024/train_audio/asbfly/XC49755.ogg'\n",
    "max_length = 22050 * 5  # For example, 5 seconds at a sample rate of 22050 Hz\n",
    "\n",
    "# Extract and aggregate features\n",
    "features = extract_audio_features(ogg_file_path, max_length)\n",
    "aggregated_features = aggregate_features(features)\n",
    "\n",
    "# Format features for model input\n",
    "formatted_features = format_features(aggregated_features)\n",
    "\n",
    "print(formatted_features.shape)\n",
    "print(formatted_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(file_paths, labels, max_length=22050*5):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for file_path, label in zip(file_paths, labels):\n",
    "        features = extract_audio_features(file_path, max_length)\n",
    "        aggregated_features = aggregate_features(features)\n",
    "        formatted_features = format_features(aggregated_features)\n",
    "        X.append(formatted_features)\n",
    "        y.append(label)\n",
    "        # print(len(X), len(X[-1]),y)\n",
    "        if len(X) % 100 == 0:\n",
    "            print(f\"Processed {len(X)} files\")\n",
    "    return np.array(X), np.array(y)\n",
    "def load_data_denoised(file_paths, labels, max_length=22050*5):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for file_path, label in zip(file_paths, labels):\n",
    "        features = extract_audio_features_denoised(file_path, max_length)\n",
    "        aggregated_features = aggregate_features(features)\n",
    "        formatted_features = format_features(aggregated_features)\n",
    "        X.append(formatted_features)\n",
    "        y.append(label)\n",
    "        # print(len(X), len(X[-1]),y)\n",
    "        if len(X) % 100 == 0:\n",
    "            print(f\"Processed {len(X)} files\")\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2.1\n"
     ]
    }
   ],
   "source": [
    "import jupyterlab\n",
    "print(jupyterlab.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_train is already loaded in your notebook\n",
    "file_paths = df_train['filename'].apply(lambda x : \"./birdclef-2024/train_audio/\" + x ).tolist()\n",
    "labels = df_train['primary_label'].astype('category').cat.codes.tolist()\n",
    "\n",
    "# Load data\n",
    "# X, y = load_data(file_paths, labels)\n",
    "X_noise, y_noise = np.load('X.npy'), np.load('y.npy')# load_data(file_paths, labels)\n",
    "X_denoised, y_denoised = load_data_denoised(file_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the x and y\n",
    "\n",
    "np.save('X_denoised.npy', X_denoised)\n",
    "np.save('y_denoised.npy', y_denoised)\n",
    "np.save('X_noise.npy', X_noise)\n",
    "np.save('y_noise.npy', y_noise)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24459 files.\n",
      "(24459, 684)\n",
      "(24459,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def extract_and_format_features(file_path, func=extract_audio_features, max_length=22050*5):\n",
    "    try:\n",
    "        features = func(file_path, max_length)\n",
    "        aggregated_features = aggregate_features(features)\n",
    "        formatted_features = format_features(aggregated_features)\n",
    "        return formatted_features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_batch(file_paths, labels, max_length, func=extract_audio_features):\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    for file_path, label in zip(file_paths, labels):\n",
    "        formatted_features = extract_and_format_features(file_path, max_length, func=func)\n",
    "        if formatted_features is not None:\n",
    "            X_batch.append(formatted_features)\n",
    "            y_batch.append(label)\n",
    "    return X_batch, y_batch\n",
    "\n",
    "def load_data(file_paths, labels, max_length=22050*5, batch_size=1000):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Split the data into batches\n",
    "    batches = [(file_paths[i:i + batch_size], labels[i:i + batch_size])\n",
    "               for i in range(0, len(file_paths), batch_size)]\n",
    "\n",
    "    # Process batches in parallel using ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_batch, batch_files, batch_labels, max_length) \n",
    "                   for batch_files, batch_labels in batches]\n",
    "        \n",
    "        for future in futures:\n",
    "            X_batch, y_batch = future.result()\n",
    "            X.extend(X_batch)\n",
    "            y.extend(y_batch)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# Assuming df_train is already loaded in your notebook\n",
    "file_paths = df_train['filename'].apply(lambda x: \"./birdclef-2024/train_audio/\" + x).tolist()\n",
    "labels = df_train['primary_label'].astype('category').cat.codes.tolist()\n",
    "\n",
    "# Load data\n",
    "X_denoised, y_denoised = np.load('X_denoised.npy'), np.load('y_denoised.npy') # load_data(file_paths, labels)\n",
    "X_noise, y_noise = np.load('X_noise.npy'), np.load('y_noise.npy') # load_data(file_paths, labels)\n",
    "print(f\"Loaded {X.shape[0]} files.\")\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24459"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24459, 684)\n",
      "(24459,)\n"
     ]
    }
   ],
   "source": [
    "# save x and y as csv\n",
    "\n",
    "# np.savetxt('X.csv', X, delimiter=',')\n",
    "# np.savetxt('y.csv', y, delimiter=',')\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.58963785e+01 -1.62785816e+01 -1.60039845e+01 -1.61279907e+01\n",
      " -1.68902264e+01 -1.66303806e+01 -1.90093765e+01 -1.82996597e+01\n",
      " -1.97097206e+01 -1.88165798e+01 -1.93298340e+01 -1.86974010e+01\n",
      " -1.96464577e+01 -1.79825916e+01 -1.93120747e+01 -1.85301857e+01\n",
      " -1.84365807e+01 -1.69842072e+01 -1.81452885e+01 -1.72752266e+01\n",
      " -1.80543804e+01 -1.79443626e+01 -1.84281940e+01 -1.83274288e+01\n",
      " -1.93307648e+01 -1.77445602e+01 -1.90370407e+01 -1.87021999e+01\n",
      " -1.97975559e+01 -1.95816879e+01 -2.00255089e+01 -1.96887741e+01\n",
      " -2.06255875e+01 -2.05158596e+01 -2.18840008e+01 -2.18313236e+01\n",
      " -2.27161179e+01 -2.29569702e+01 -2.43032150e+01 -2.38553371e+01\n",
      " -2.37021084e+01 -2.30606480e+01 -2.36355991e+01 -2.39111691e+01\n",
      " -2.52166252e+01 -2.51034756e+01 -2.48021603e+01 -2.50561848e+01\n",
      " -2.54581928e+01 -2.63199348e+01 -2.61479130e+01 -2.68570290e+01\n",
      " -2.75378208e+01 -2.76385975e+01 -2.76007042e+01 -2.82462063e+01\n",
      " -2.88554058e+01 -2.94312649e+01 -2.99256134e+01 -2.95353432e+01\n",
      " -3.00814590e+01 -2.95706291e+01 -3.02831745e+01 -3.02847157e+01\n",
      " -3.04633465e+01 -3.09638348e+01 -3.09991474e+01 -3.06975136e+01\n",
      " -3.08780289e+01 -3.04459114e+01 -3.05076771e+01 -3.07302399e+01\n",
      " -3.10183868e+01 -3.07234344e+01 -3.10168362e+01 -3.11551495e+01\n",
      " -3.07549973e+01 -3.11800880e+01 -3.23080597e+01 -3.27356300e+01\n",
      " -3.25212135e+01 -3.20499535e+01 -3.08147736e+01 -3.06439247e+01\n",
      " -3.06258316e+01 -3.07438736e+01 -3.03000259e+01 -3.00276985e+01\n",
      " -2.93606796e+01 -2.96538124e+01 -2.95904541e+01 -2.95188942e+01\n",
      " -2.95135498e+01 -3.07016029e+01 -3.22070999e+01 -3.26674614e+01\n",
      " -3.18068352e+01 -3.14414768e+01 -3.33251419e+01 -3.38636055e+01\n",
      " -3.32444611e+01 -3.37453270e+01 -3.44179153e+01 -3.31108246e+01\n",
      " -3.17247791e+01 -3.11491680e+01 -3.18245564e+01 -3.22816391e+01\n",
      " -3.18280010e+01 -3.16259975e+01 -3.19214230e+01 -3.21652565e+01\n",
      " -3.09325714e+01 -3.01787491e+01 -2.97508163e+01 -3.08613014e+01\n",
      " -2.98247471e+01 -3.03215466e+01 -3.14026012e+01 -3.13200092e+01\n",
      " -3.05774784e+01 -3.04428711e+01 -2.91375237e+01 -2.85069237e+01\n",
      " -2.87903061e+01 -2.96167107e+01 -2.93624535e+01 -2.97864628e+01\n",
      "  8.64745998e+00  7.16344833e+00  7.36130905e+00  7.14328003e+00\n",
      "  7.09108257e+00  7.18040705e+00  7.33450747e+00  6.69792843e+00\n",
      "  6.96023035e+00  6.56854916e+00  6.33412552e+00  5.69745255e+00\n",
      "  6.07326889e+00  5.91410589e+00  6.53240204e+00  6.82798004e+00\n",
      "  6.30658674e+00  6.12102413e+00  6.57062387e+00  6.18342018e+00\n",
      "  6.21349239e+00  6.48826647e+00  6.95030737e+00  6.61598969e+00\n",
      "  6.96417999e+00  6.60658884e+00  6.46461725e+00  6.27549601e+00\n",
      "  6.62227201e+00  7.13657522e+00  6.67238045e+00  6.19962931e+00\n",
      "  5.98661661e+00  5.99430847e+00  6.19947481e+00  6.28139305e+00\n",
      "  6.44840527e+00  6.15236855e+00  6.43376350e+00  6.54566240e+00\n",
      "  6.30506516e+00  6.06274223e+00  5.88235903e+00  5.70145035e+00\n",
      "  5.41455746e+00  5.88379908e+00  6.44336987e+00  6.33592844e+00\n",
      "  5.82041311e+00  5.46617746e+00  5.56238365e+00  5.80523920e+00\n",
      "  5.95956564e+00  5.78231049e+00  5.30890036e+00  5.72684574e+00\n",
      "  5.89241457e+00  5.37442350e+00  5.34040642e+00  5.48718786e+00\n",
      "  5.97134876e+00  6.09700012e+00  5.23120546e+00  5.68398905e+00\n",
      "  5.66045666e+00  5.27050114e+00  5.70577097e+00  5.95201159e+00\n",
      "  5.94395828e+00  5.95346451e+00  6.39020491e+00  6.38441753e+00\n",
      "  6.88994169e+00  7.09665251e+00  6.62589741e+00  6.94951010e+00\n",
      "  6.75732613e+00  6.81736755e+00  6.84914637e+00  6.54324055e+00\n",
      "  6.21657228e+00  6.52047253e+00  6.44191599e+00  6.59947348e+00\n",
      "  6.42113876e+00  6.03311920e+00  6.29669428e+00  6.61074543e+00\n",
      "  6.29463911e+00  5.59307051e+00  6.15042830e+00  6.79041195e+00\n",
      "  6.56970739e+00  6.32874584e+00  6.06223106e+00  6.45676374e+00\n",
      "  6.55129242e+00  6.96113253e+00  6.15083313e+00  6.36808109e+00\n",
      "  6.58333349e+00  6.24675751e+00  6.21397114e+00  6.24397802e+00\n",
      "  6.17722559e+00  6.83606291e+00  7.50287867e+00  7.76126909e+00\n",
      "  8.03764534e+00  7.40579653e+00  7.19027805e+00  8.12052917e+00\n",
      "  9.66219139e+00  9.89211845e+00  9.84974480e+00  8.80079651e+00\n",
      "  9.57324409e+00  9.42989445e+00  9.03293610e+00  8.19472313e+00\n",
      "  8.30798817e+00  8.43874550e+00  9.17224884e+00  9.77703285e+00\n",
      "  9.54730415e+00  8.90021992e+00  9.31238365e+00  8.79791355e+00\n",
      " -7.88744125e+01 -7.92073669e+01 -7.87869720e+01 -7.86629944e+01\n",
      " -7.91639786e+01 -7.83864822e+01 -7.78594818e+01 -7.84457016e+01\n",
      " -7.79598465e+01 -7.42010422e+01 -7.04523163e+01 -6.80655823e+01\n",
      " -6.85948105e+01 -6.89979324e+01 -7.40623856e+01 -7.64001694e+01\n",
      " -6.98311462e+01 -6.66954880e+01 -7.17537079e+01 -6.83171692e+01\n",
      " -6.92702179e+01 -7.32707596e+01 -8.00000000e+01 -7.52815018e+01\n",
      " -7.92733078e+01 -7.21284943e+01 -7.16828537e+01 -6.94096222e+01\n",
      " -7.47823715e+01 -7.94920502e+01 -7.36520767e+01 -7.05104446e+01\n",
      " -7.18919830e+01 -7.05902557e+01 -7.36767426e+01 -7.71902924e+01\n",
      " -7.85011444e+01 -7.33734283e+01 -7.44840012e+01 -7.65772858e+01\n",
      " -7.43562164e+01 -6.72263718e+01 -6.47518997e+01 -6.46430130e+01\n",
      " -6.85729446e+01 -7.58512955e+01 -7.74299545e+01 -7.41308517e+01\n",
      " -7.07993927e+01 -6.86671677e+01 -7.09558487e+01 -7.44128342e+01\n",
      " -7.79833755e+01 -8.00000000e+01 -7.22012482e+01 -7.01364136e+01\n",
      " -7.62544861e+01 -7.24140625e+01 -7.34999542e+01 -7.88085632e+01\n",
      " -8.00000000e+01 -7.71312485e+01 -7.35789261e+01 -7.72038727e+01\n",
      " -7.59460068e+01 -7.01402740e+01 -7.41222458e+01 -7.78565369e+01\n",
      " -7.33742676e+01 -7.08317719e+01 -7.39989548e+01 -7.05880966e+01\n",
      " -7.89444427e+01 -7.98405991e+01 -7.40169144e+01 -7.65372620e+01\n",
      " -7.28705444e+01 -7.47853165e+01 -7.55964890e+01 -7.57794113e+01\n",
      " -7.22604065e+01 -7.39573364e+01 -7.34597931e+01 -7.73487244e+01\n",
      " -7.52344208e+01 -7.22617950e+01 -7.54706802e+01 -7.87640533e+01\n",
      " -7.25853577e+01 -7.17371979e+01 -7.72360382e+01 -7.80260925e+01\n",
      " -7.58923492e+01 -7.54154282e+01 -7.04084702e+01 -7.51223984e+01\n",
      " -7.08073883e+01 -7.41804428e+01 -7.16302185e+01 -7.34962921e+01\n",
      " -7.76512527e+01 -7.25509567e+01 -7.43515930e+01 -7.37640915e+01\n",
      " -7.08831482e+01 -7.41651382e+01 -7.33156281e+01 -7.53264694e+01\n",
      " -7.66769104e+01 -7.48695068e+01 -7.15952759e+01 -7.24876633e+01\n",
      " -7.14066086e+01 -7.23021927e+01 -7.68528671e+01 -7.02043381e+01\n",
      " -7.46079407e+01 -7.43356857e+01 -7.38232117e+01 -7.41959991e+01\n",
      " -7.41143417e+01 -7.34945908e+01 -7.30838165e+01 -7.40168915e+01\n",
      " -7.41514282e+01 -7.24547272e+01 -7.35751801e+01 -7.20711212e+01\n",
      "  0.00000000e+00 -6.70642185e+00 -7.24602795e+00 -7.76843929e+00\n",
      " -9.82867527e+00 -6.56833553e+00 -6.77038288e+00 -8.49011135e+00\n",
      " -1.03660707e+01 -7.39453983e+00 -9.69680691e+00 -1.15366735e+01\n",
      " -1.12853651e+01 -1.04197741e+01 -1.01131926e+01 -1.01788397e+01\n",
      " -8.39172459e+00 -7.81788731e+00 -1.01099615e+01 -8.42434978e+00\n",
      " -9.45026684e+00 -9.22789478e+00 -1.03537397e+01 -9.41030025e+00\n",
      " -1.03827791e+01 -8.20087337e+00 -8.37929630e+00 -8.61129093e+00\n",
      " -1.04193506e+01 -1.09501219e+01 -7.60232639e+00 -7.26029873e+00\n",
      " -1.15825243e+01 -1.23086042e+01 -1.07986555e+01 -9.39124775e+00\n",
      " -1.16285639e+01 -1.56171103e+01 -1.53768988e+01 -1.10409346e+01\n",
      " -1.09337378e+01 -1.09741259e+01 -8.90957928e+00 -9.61977291e+00\n",
      " -1.39129591e+01 -1.57216043e+01 -1.38012953e+01 -1.58345346e+01\n",
      " -1.60100670e+01 -1.78381653e+01 -1.67354050e+01 -1.72014580e+01\n",
      " -1.56490564e+01 -1.74433365e+01 -1.73797798e+01 -1.47488585e+01\n",
      " -1.62811394e+01 -1.99103508e+01 -1.96377106e+01 -1.70427742e+01\n",
      " -2.00390739e+01 -1.90269775e+01 -1.89940300e+01 -1.50100298e+01\n",
      " -1.57355032e+01 -2.06336136e+01 -1.72563019e+01 -1.63598404e+01\n",
      " -1.55593138e+01 -1.33582487e+01 -1.23295965e+01 -9.40960407e+00\n",
      " -1.09563398e+01 -1.13918409e+01 -1.25637941e+01 -9.34617901e+00\n",
      " -1.23500242e+01 -1.18307753e+01 -1.39252234e+01 -1.10903769e+01\n",
      " -8.98031902e+00 -1.06998034e+01 -1.02659063e+01 -1.20930777e+01\n",
      " -1.14846067e+01 -1.24501162e+01 -1.36478148e+01 -1.44304075e+01\n",
      " -1.39099550e+01 -1.73282051e+01 -1.49640341e+01 -1.15611944e+01\n",
      " -1.32251520e+01 -1.23642874e+01 -1.54775305e+01 -1.70697136e+01\n",
      " -1.53142138e+01 -1.34074583e+01 -1.55450754e+01 -1.62278366e+01\n",
      " -1.70706367e+01 -1.92417068e+01 -1.76086388e+01 -1.69747086e+01\n",
      " -1.59172659e+01 -1.67753639e+01 -9.47465229e+00 -8.09475231e+00\n",
      " -2.46144009e+00 -1.09984121e+01 -1.22836103e+01 -9.16280460e+00\n",
      " -3.90282154e+00 -3.07284927e+00 -4.49246311e+00 -1.02195578e+01\n",
      " -4.49201012e+00 -2.89605427e+00 -1.09639215e+01 -8.61118984e+00\n",
      " -8.64368534e+00 -1.23531961e+01 -8.59678173e+00 -9.95633602e+00\n",
      " -7.28274250e+00 -1.05474539e+01 -1.08519392e+01 -1.22551985e+01\n",
      " -4.29232544e+02  7.11802902e+01  1.09755116e+01  2.44486351e+01\n",
      " -1.77542171e+01  8.81186104e+00 -1.30285301e+01  8.04521942e+00\n",
      " -2.74320722e+00  1.36702263e+00 -2.52119851e+00  8.58198261e+00\n",
      " -6.30305195e+00  5.54706993e+01  1.70713959e+01  1.08007555e+01\n",
      "  9.85739803e+00  1.22010775e+01  1.04473476e+01  7.81685352e+00\n",
      "  8.00547981e+00  9.75406456e+00  8.74553680e+00  7.75885820e+00\n",
      "  8.16620541e+00  6.95966625e+00 -9.38135864e+02 -4.73120546e+00\n",
      " -2.23391285e+01 -8.05861664e+00 -4.83238640e+01 -1.15266733e+01\n",
      " -3.98961868e+01 -2.51951485e+01 -2.27428837e+01 -3.32791443e+01\n",
      " -2.34698696e+01 -7.74576950e+00 -2.86516609e+01 -3.06136841e+02\n",
      "  1.08477905e+02  3.34678802e+01  5.79081116e+01  9.07241917e+00\n",
      "  3.96806107e+01  5.55209970e+00  2.66234550e+01  2.52259674e+01\n",
      "  1.57141762e+01  1.82818279e+01  3.26189575e+01  9.54405022e+00\n",
      "  5.47173321e-01  5.05384922e-01  5.14317632e-01  5.40771067e-01\n",
      "  5.21623075e-01  5.60736835e-01  5.78839302e-01  5.68989515e-01\n",
      "  5.54292798e-01  5.92711329e-01  5.93373477e-01  5.99355459e-01\n",
      "  2.63690084e-01  2.49390304e-01  2.60953903e-01  2.69437671e-01\n",
      "  2.43797109e-01  2.14238524e-01  2.51847476e-01  2.48875186e-01\n",
      "  2.49994889e-01  2.73460686e-01  2.74991304e-01  2.66462505e-01\n",
      "  3.10289543e-02  2.43774205e-02  2.38180645e-02  3.25032882e-02\n",
      "  6.64836764e-02  9.64392200e-02  9.31174085e-02  5.94973005e-02\n",
      "  5.33092059e-02  4.88696694e-02  3.15375999e-02  3.13420817e-02\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  1.06268047e+01  8.83279133e+00  1.10944987e+01  1.46141853e+01\n",
      "  1.47990242e+01  1.97491527e+01  5.88799431e+01  4.18891500e+00\n",
      "  2.91077802e+00  2.68534981e+00  3.25983833e+00  2.50931478e+00\n",
      "  4.14553740e+00  1.06017807e+01  4.82231547e-01  2.58272172e+00\n",
      "  6.03219356e+00  7.44142978e+00  9.21958966e+00  1.22308310e+01\n",
      "  1.64203037e+01  3.33382102e+01  1.96001314e+01  1.88650294e+01\n",
      "  2.75555861e+01  2.22237025e+01  3.36100821e+01  7.37398713e+01\n",
      " -1.07564141e-02 -8.08599090e-03 -1.75095580e-02  5.19610402e-03\n",
      "  6.94840007e-03 -1.19459232e-03  2.51519188e-02  2.54680349e-02\n",
      "  5.30174908e-02  3.43008609e-02  1.84619357e-02  1.52257772e-02\n",
      " -8.39795339e-02 -7.56455103e-02 -1.30029291e-01 -8.44599009e-02\n",
      " -4.01493612e-02 -3.84319611e-02  5.52698054e-02  7.03565347e-02\n",
      "  1.55698881e-01  9.28038545e-02  6.72644710e-02  4.97426288e-02\n",
      "  4.81627142e+03  6.92639850e+02  3.32156903e+03  7.66062260e+03\n",
      "  4.03365624e+03  2.97669625e+02  3.18341688e+03  4.55592710e+03\n",
      "  9.39091435e+03  1.00139375e+03  7.35937500e+03  1.29531250e+04\n",
      "  2.31047454e-01  7.77359536e-02  1.07421875e-01  4.71191406e-01\n",
      "  2.82290578e-03  9.59766388e-04  1.05680574e-05  5.80285303e-03]\n"
     ]
    }
   ],
   "source": [
    "print(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24459, 684) (24459, 182)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rami\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 3:16 3s/step - accuracy: 0.0000e+00 - loss: 6.082 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0049 - loss: 6.0865    ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.0074 - loss: 6.05 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.0090 - loss: 6.01 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.0107 - loss: 5.98 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.0128 - loss: 5.96 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.0156 - loss: 5.93 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.0185 - loss: 5.9024\n",
      "Epoch 1: val_accuracy improved from -inf to 0.10220, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 4s 14ms/step - accuracy: 0.0202 - loss: 5.8838 - val_accuracy: 0.1022 - val_loss: 5.2576\n",
      "Epoch 2/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 46ms/step - accuracy: 0.0977 - loss: 5.318 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.1004 - loss: 5.262 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.0982 - loss: 5.25 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.0973 - loss: 5.23 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.0973 - loss: 5.21 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.0981 - loss: 5.18 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.0995 - loss: 5.16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.1010 - loss: 5.1398\n",
      "Epoch 2: val_accuracy improved from 0.10220 to 0.15202, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.1019 - loss: 5.1244 - val_accuracy: 0.1520 - val_loss: 4.5467\n",
      "Epoch 3/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 46ms/step - accuracy: 0.1758 - loss: 4.564 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.1439 - loss: 4.635 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.1418 - loss: 4.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.1419 - loss: 4.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.1422 - loss: 4.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.1426 - loss: 4.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.1431 - loss: 4.58 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.1439 - loss: 4.5698\n",
      "Epoch 3: val_accuracy improved from 0.15202 to 0.18140, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.1445 - loss: 4.5627 - val_accuracy: 0.1814 - val_loss: 4.1946\n",
      "Epoch 4/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.1445 - loss: 4.354 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.1649 - loss: 4.298 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.1679 - loss: 4.27 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.1699 - loss: 4.25 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.1705 - loss: 4.25 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.1712 - loss: 4.24 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.1717 - loss: 4.24 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.1722 - loss: 4.2387\n",
      "Epoch 4: val_accuracy improved from 0.18140 to 0.20312, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.1726 - loss: 4.2358 - val_accuracy: 0.2031 - val_loss: 3.9803\n",
      "Epoch 5/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.1602 - loss: 4.102 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.1843 - loss: 4.074 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.1860 - loss: 4.05 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.1881 - loss: 4.05 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.1884 - loss: 4.05 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.1885 - loss: 4.05 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.1889 - loss: 4.05 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.1893 - loss: 4.0502\n",
      "Epoch 5: val_accuracy improved from 0.20312 to 0.22662, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.1896 - loss: 4.0491 - val_accuracy: 0.2266 - val_loss: 3.8267\n",
      "Epoch 6/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 4s 77ms/step - accuracy: 0.2344 - loss: 3.958 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.2083 - loss: 3.921 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2068 - loss: 3.92 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2063 - loss: 3.92 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2058 - loss: 3.92 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2058 - loss: 3.92 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2061 - loss: 3.92 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2064 - loss: 3.9185\n",
      "Epoch 6: val_accuracy improved from 0.22662 to 0.25141, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.2066 - loss: 3.9162 - val_accuracy: 0.2514 - val_loss: 3.7041\n",
      "Epoch 7/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.2031 - loss: 3.833 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2108 - loss: 3.828 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2128 - loss: 3.82 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2148 - loss: 3.82 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2162 - loss: 3.82 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2174 - loss: 3.81 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2181 - loss: 3.81 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2187 - loss: 3.81 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2194 - loss: 3.8109\n",
      "Epoch 7: val_accuracy improved from 0.25141 to 0.25728, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.2199 - loss: 3.8088 - val_accuracy: 0.2573 - val_loss: 3.6301\n",
      "Epoch 8/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.2422 - loss: 3.710 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2334 - loss: 3.706 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2331 - loss: 3.69 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2334 - loss: 3.69 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2339 - loss: 3.69 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2339 - loss: 3.69 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2340 - loss: 3.69 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2344 - loss: 3.6968\n",
      "Epoch 8: val_accuracy improved from 0.25728 to 0.26648, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.2345 - loss: 3.6966 - val_accuracy: 0.2665 - val_loss: 3.5454\n",
      "Epoch 9/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.2305 - loss: 3.659 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2365 - loss: 3.629 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2389 - loss: 3.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2394 - loss: 3.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2393 - loss: 3.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2399 - loss: 3.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2404 - loss: 3.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2410 - loss: 3.6372\n",
      "Epoch 9: val_accuracy improved from 0.26648 to 0.28232, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.2415 - loss: 3.6349 - val_accuracy: 0.2823 - val_loss: 3.4861\n",
      "Epoch 10/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.2852 - loss: 3.555 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2574 - loss: 3.565 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2536 - loss: 3.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2527 - loss: 3.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2515 - loss: 3.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2511 - loss: 3.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2510 - loss: 3.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2511 - loss: 3.5657\n",
      "Epoch 10: val_accuracy improved from 0.28232 to 0.28436, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.2511 - loss: 3.5649 - val_accuracy: 0.2844 - val_loss: 3.4105\n",
      "Epoch 11/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 43ms/step - accuracy: 0.3008 - loss: 3.337 ━━━━━━━━━━━━━━━━━━━━ 1s 26ms/step - accuracy: 0.2969 - loss: 3.391 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.2767 - loss: 3.480 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.2712 - loss: 3.501 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.2679 - loss: 3.505 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.2654 - loss: 3.50 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.2642 - loss: 3.50 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.2640 - loss: 3.49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2639 - loss: 3.4963\n",
      "Epoch 11: val_accuracy improved from 0.28436 to 0.29637, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.2636 - loss: 3.4948 - val_accuracy: 0.2964 - val_loss: 3.3713\n",
      "Epoch 12/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 48ms/step - accuracy: 0.2695 - loss: 3.451 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2804 - loss: 3.389 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2805 - loss: 3.39 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2806 - loss: 3.40 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2807 - loss: 3.40 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2802 - loss: 3.40 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2797 - loss: 3.40 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2792 - loss: 3.4074\n",
      "Epoch 12: val_accuracy did not improve from 0.29637\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.2787 - loss: 3.4089 - val_accuracy: 0.2951 - val_loss: 3.3329\n",
      "Epoch 13/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.2383 - loss: 3.562 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2692 - loss: 3.430 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2712 - loss: 3.41 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2716 - loss: 3.40 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2717 - loss: 3.40 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2719 - loss: 3.40 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2721 - loss: 3.4010\n",
      "Epoch 13: val_accuracy improved from 0.29637 to 0.30123, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.2724 - loss: 3.3984 - val_accuracy: 0.3012 - val_loss: 3.2844\n",
      "Epoch 14/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 48ms/step - accuracy: 0.2695 - loss: 3.387 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2960 - loss: 3.300 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2936 - loss: 3.31 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2911 - loss: 3.32 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2889 - loss: 3.32 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2871 - loss: 3.33 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2866 - loss: 3.33 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2858 - loss: 3.3349\n",
      "Epoch 14: val_accuracy improved from 0.30123 to 0.30634, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.2850 - loss: 3.3369 - val_accuracy: 0.3063 - val_loss: 3.2564\n",
      "Epoch 15/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.3398 - loss: 3.162 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3012 - loss: 3.256 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2959 - loss: 3.26 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2924 - loss: 3.27 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2896 - loss: 3.28 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2877 - loss: 3.29 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2867 - loss: 3.2967\n",
      "Epoch 15: val_accuracy improved from 0.30634 to 0.30838, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.2862 - loss: 3.2995 - val_accuracy: 0.3084 - val_loss: 3.2300\n",
      "Epoch 16/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.2969 - loss: 3.377 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3009 - loss: 3.277 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3001 - loss: 3.25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2993 - loss: 3.25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2984 - loss: 3.24 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2982 - loss: 3.24 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2976 - loss: 3.24 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2970 - loss: 3.2493\n",
      "Epoch 16: val_accuracy improved from 0.30838 to 0.31553, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.2966 - loss: 3.2503 - val_accuracy: 0.3155 - val_loss: 3.2005\n",
      "Epoch 17/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.2773 - loss: 3.364 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2929 - loss: 3.241 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2910 - loss: 3.24 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2900 - loss: 3.24 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2898 - loss: 3.25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2902 - loss: 3.25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.2909 - loss: 3.2499\n",
      "Epoch 17: val_accuracy improved from 0.31553 to 0.31758, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.2913 - loss: 3.2489 - val_accuracy: 0.3176 - val_loss: 3.1836\n",
      "Epoch 18/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3008 - loss: 3.166 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3015 - loss: 3.221 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.2996 - loss: 3.22 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2980 - loss: 3.22 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2978 - loss: 3.21 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2978 - loss: 3.21 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2978 - loss: 3.21 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.2981 - loss: 3.2127\n",
      "Epoch 18: val_accuracy did not improve from 0.31758\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.2983 - loss: 3.2119 - val_accuracy: 0.3102 - val_loss: 3.1709\n",
      "Epoch 19/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.2812 - loss: 3.188 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3046 - loss: 3.174 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3042 - loss: 3.19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3048 - loss: 3.19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3056 - loss: 3.19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3059 - loss: 3.19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3060 - loss: 3.18 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3059 - loss: 3.1888\n",
      "Epoch 19: val_accuracy did not improve from 0.31758\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3059 - loss: 3.1886 - val_accuracy: 0.3145 - val_loss: 3.1443\n",
      "Epoch 20/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3203 - loss: 3.183 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3118 - loss: 3.159 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3095 - loss: 3.15 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3076 - loss: 3.14 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3063 - loss: 3.15 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3059 - loss: 3.15 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3058 - loss: 3.15 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3057 - loss: 3.1550\n",
      "Epoch 20: val_accuracy improved from 0.31758 to 0.32601, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3056 - loss: 3.1554 - val_accuracy: 0.3260 - val_loss: 3.1315\n",
      "Epoch 21/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 45ms/step - accuracy: 0.3242 - loss: 3.024 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3076 - loss: 3.184 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3103 - loss: 3.17 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3108 - loss: 3.16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3109 - loss: 3.16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3107 - loss: 3.15 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3102 - loss: 3.15 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3104 - loss: 3.15 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3104 - loss: 3.1501\n",
      "Epoch 21: val_accuracy did not improve from 0.32601\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3104 - loss: 3.1499 - val_accuracy: 0.3242 - val_loss: 3.1135\n",
      "Epoch 22/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.3281 - loss: 2.953 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3074 - loss: 3.105 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3078 - loss: 3.11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3094 - loss: 3.12 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3105 - loss: 3.12 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3111 - loss: 3.12 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3116 - loss: 3.12 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3120 - loss: 3.1243\n",
      "Epoch 22: val_accuracy improved from 0.32601 to 0.32729, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3121 - loss: 3.1243 - val_accuracy: 0.3273 - val_loss: 3.1043\n",
      "Epoch 23/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.3867 - loss: 2.913 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3191 - loss: 3.100 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3149 - loss: 3.11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3142 - loss: 3.11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3131 - loss: 3.11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3124 - loss: 3.11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3118 - loss: 3.11 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3115 - loss: 3.1171\n",
      "Epoch 23: val_accuracy improved from 0.32729 to 0.33061, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3113 - loss: 3.1174 - val_accuracy: 0.3306 - val_loss: 3.0747\n",
      "Epoch 24/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 4s 66ms/step - accuracy: 0.3086 - loss: 3.232 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3266 - loss: 3.086 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3284 - loss: 3.07 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3271 - loss: 3.07 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3253 - loss: 3.08 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3238 - loss: 3.08 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3231 - loss: 3.09 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3223 - loss: 3.0945\n",
      "Epoch 24: val_accuracy improved from 0.33061 to 0.33546, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3220 - loss: 3.0953 - val_accuracy: 0.3355 - val_loss: 3.0683\n",
      "Epoch 25/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 46ms/step - accuracy: 0.2891 - loss: 3.177 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3185 - loss: 3.093 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3153 - loss: 3.09 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3156 - loss: 3.09 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3158 - loss: 3.09 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3160 - loss: 3.08 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3166 - loss: 3.08 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3168 - loss: 3.0820\n",
      "Epoch 25: val_accuracy did not improve from 0.33546\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3169 - loss: 3.0812 - val_accuracy: 0.3316 - val_loss: 3.0618\n",
      "Epoch 26/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4023 - loss: 2.789 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3440 - loss: 2.957 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3324 - loss: 2.99 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3250 - loss: 3.02 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3232 - loss: 3.03 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3223 - loss: 3.03 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3221 - loss: 3.04 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3218 - loss: 3.0463\n",
      "Epoch 26: val_accuracy did not improve from 0.33546\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3216 - loss: 3.0491 - val_accuracy: 0.3344 - val_loss: 3.0595\n",
      "Epoch 27/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3008 - loss: 3.083 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3089 - loss: 3.064 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3104 - loss: 3.05 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3124 - loss: 3.05 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3143 - loss: 3.04 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3157 - loss: 3.04 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3165 - loss: 3.0503\n",
      "Epoch 27: val_accuracy did not improve from 0.33546\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3169 - loss: 3.0507 - val_accuracy: 0.3327 - val_loss: 3.0463\n",
      "Epoch 28/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.3203 - loss: 3.013 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3253 - loss: 2.988 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3217 - loss: 3.00 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3211 - loss: 3.01 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3211 - loss: 3.02 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3213 - loss: 3.02 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3220 - loss: 3.02 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3225 - loss: 3.0232\n",
      "Epoch 28: val_accuracy improved from 0.33546 to 0.33878, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3225 - loss: 3.0236 - val_accuracy: 0.3388 - val_loss: 3.0361\n",
      "Epoch 29/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.3125 - loss: 2.886 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3251 - loss: 3.011 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3238 - loss: 3.01 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3238 - loss: 3.01 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3240 - loss: 3.01 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3240 - loss: 3.01 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3243 - loss: 3.01 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3246 - loss: 3.0119\n",
      "Epoch 29: val_accuracy did not improve from 0.33878\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3246 - loss: 3.0116 - val_accuracy: 0.3304 - val_loss: 3.0318\n",
      "Epoch 30/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 44ms/step - accuracy: 0.3125 - loss: 2.961 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3298 - loss: 2.978 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3313 - loss: 2.98 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3325 - loss: 2.99 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3325 - loss: 2.99 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3327 - loss: 2.99 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3326 - loss: 2.9950\n",
      "Epoch 30: val_accuracy improved from 0.33878 to 0.33981, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3326 - loss: 2.9964 - val_accuracy: 0.3398 - val_loss: 3.0204\n",
      "Epoch 31/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 43ms/step - accuracy: 0.3281 - loss: 2.911 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3153 - loss: 3.014 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3216 - loss: 3.00 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3242 - loss: 3.00 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3259 - loss: 3.00 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3266 - loss: 3.00 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3272 - loss: 2.99 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3279 - loss: 2.9974\n",
      "Epoch 31: val_accuracy did not improve from 0.33981\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3284 - loss: 2.9965 - val_accuracy: 0.3357 - val_loss: 3.0175\n",
      "Epoch 32/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3555 - loss: 2.923 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3343 - loss: 2.972 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3317 - loss: 2.97 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3322 - loss: 2.97 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3326 - loss: 2.97 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3332 - loss: 2.97 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3334 - loss: 2.9756\n",
      "Epoch 32: val_accuracy did not improve from 0.33981\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3334 - loss: 2.9761 - val_accuracy: 0.3388 - val_loss: 3.0048\n",
      "Epoch 33/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 3s 65ms/step - accuracy: 0.3750 - loss: 2.854 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3551 - loss: 2.907 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3471 - loss: 2.93 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3439 - loss: 2.95 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3421 - loss: 2.95 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3406 - loss: 2.95 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3394 - loss: 2.95 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3386 - loss: 2.9588\n",
      "Epoch 33: val_accuracy did not improve from 0.33981\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.3385 - loss: 2.9590 - val_accuracy: 0.3380 - val_loss: 2.9818\n",
      "Epoch 34/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 43ms/step - accuracy: 0.3281 - loss: 3.007 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3232 - loss: 2.981 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3276 - loss: 2.97 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3279 - loss: 2.97 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3288 - loss: 2.97 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3303 - loss: 2.96 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3313 - loss: 2.96 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3321 - loss: 2.96 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3326 - loss: 2.9645\n",
      "Epoch 34: val_accuracy improved from 0.33981 to 0.34773, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3328 - loss: 2.9640 - val_accuracy: 0.3477 - val_loss: 2.9824\n",
      "Epoch 35/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 48ms/step - accuracy: 0.3164 - loss: 2.826 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3373 - loss: 2.903 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3359 - loss: 2.92 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3387 - loss: 2.92 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3394 - loss: 2.92 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3396 - loss: 2.92 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3397 - loss: 2.92 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3399 - loss: 2.92 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3399 - loss: 2.9325\n",
      "Epoch 35: val_accuracy improved from 0.34773 to 0.34926, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3398 - loss: 2.9337 - val_accuracy: 0.3493 - val_loss: 2.9900\n",
      "Epoch 36/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3555 - loss: 2.842 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3507 - loss: 2.920 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3462 - loss: 2.93 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3433 - loss: 2.94 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3418 - loss: 2.94 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3411 - loss: 2.94 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3408 - loss: 2.94 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3409 - loss: 2.9469\n",
      "Epoch 36: val_accuracy did not improve from 0.34926\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3409 - loss: 2.9470 - val_accuracy: 0.3482 - val_loss: 2.9855\n",
      "Epoch 37/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3594 - loss: 2.867 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3602 - loss: 2.882 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3544 - loss: 2.89 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3516 - loss: 2.89 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3499 - loss: 2.89 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3489 - loss: 2.89 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3480 - loss: 2.8947\n",
      "Epoch 37: val_accuracy improved from 0.34926 to 0.35003, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3471 - loss: 2.8978 - val_accuracy: 0.3500 - val_loss: 2.9686\n",
      "Epoch 38/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 4s 73ms/step - accuracy: 0.3828 - loss: 2.873 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3542 - loss: 2.919 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3494 - loss: 2.92 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3483 - loss: 2.91 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3482 - loss: 2.91 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3476 - loss: 2.91 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3472 - loss: 2.91 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3472 - loss: 2.9153\n",
      "Epoch 38: val_accuracy improved from 0.35003 to 0.35054, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3471 - loss: 2.9156 - val_accuracy: 0.3505 - val_loss: 2.9795\n",
      "Epoch 39/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.3438 - loss: 2.914 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3603 - loss: 2.870 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3548 - loss: 2.88 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3524 - loss: 2.89 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3517 - loss: 2.89 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3512 - loss: 2.90 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3505 - loss: 2.90 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3500 - loss: 2.9065\n",
      "Epoch 39: val_accuracy improved from 0.35054 to 0.35130, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3498 - loss: 2.9076 - val_accuracy: 0.3513 - val_loss: 2.9595\n",
      "Epoch 40/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 3s 53ms/step - accuracy: 0.3203 - loss: 3.016 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3430 - loss: 2.911 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3450 - loss: 2.89 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3447 - loss: 2.89 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3443 - loss: 2.89 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3443 - loss: 2.89 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3439 - loss: 2.89 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3437 - loss: 2.89 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3435 - loss: 2.9013\n",
      "Epoch 40: val_accuracy did not improve from 0.35130\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3436 - loss: 2.9019 - val_accuracy: 0.3452 - val_loss: 2.9556\n",
      "Epoch 41/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3477 - loss: 2.986 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3494 - loss: 2.918 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3473 - loss: 2.91 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3471 - loss: 2.91 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3472 - loss: 2.90 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3468 - loss: 2.90 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3462 - loss: 2.9102\n",
      "Epoch 41: val_accuracy did not improve from 0.35130\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3455 - loss: 2.9116 - val_accuracy: 0.3513 - val_loss: 2.9435\n",
      "Epoch 42/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4219 - loss: 2.490 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.3876 - loss: 2.651 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3785 - loss: 2.733 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3689 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3643 - loss: 2.81 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3614 - loss: 2.83 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3591 - loss: 2.84 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3573 - loss: 2.8511\n",
      "Epoch 42: val_accuracy did not improve from 0.35130\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3567 - loss: 2.8544 - val_accuracy: 0.3477 - val_loss: 2.9547\n",
      "Epoch 43/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.3398 - loss: 2.721 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3439 - loss: 2.870 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3453 - loss: 2.88 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3460 - loss: 2.88 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3478 - loss: 2.88 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3494 - loss: 2.88 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3497 - loss: 2.88 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3496 - loss: 2.8830\n",
      "Epoch 43: val_accuracy improved from 0.35130 to 0.35846, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3496 - loss: 2.8833 - val_accuracy: 0.3585 - val_loss: 2.9497\n",
      "Epoch 44/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.3477 - loss: 2.822 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3725 - loss: 2.817 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3632 - loss: 2.85 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3605 - loss: 2.86 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3602 - loss: 2.86 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3598 - loss: 2.85 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3589 - loss: 2.86 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3580 - loss: 2.8637\n",
      "Epoch 44: val_accuracy did not improve from 0.35846\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3573 - loss: 2.8665 - val_accuracy: 0.3544 - val_loss: 2.9426\n",
      "Epoch 45/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3555 - loss: 2.887 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3562 - loss: 2.842 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3551 - loss: 2.83 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3556 - loss: 2.84 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3552 - loss: 2.84 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3547 - loss: 2.85 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3543 - loss: 2.8535\n",
      "Epoch 45: val_accuracy did not improve from 0.35846\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3541 - loss: 2.8562 - val_accuracy: 0.3441 - val_loss: 2.9543\n",
      "Epoch 46/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 4s 74ms/step - accuracy: 0.3320 - loss: 2.941 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3456 - loss: 2.944 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3459 - loss: 2.91 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3466 - loss: 2.90 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3476 - loss: 2.89 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3482 - loss: 2.88 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3486 - loss: 2.88 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3488 - loss: 2.8832\n",
      "Epoch 46: val_accuracy did not improve from 0.35846\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.3488 - loss: 2.8831 - val_accuracy: 0.3536 - val_loss: 2.9552\n",
      "Epoch 47/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.3359 - loss: 3.013 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3700 - loss: 2.827 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3659 - loss: 2.84 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3649 - loss: 2.84 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3617 - loss: 2.85 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3596 - loss: 2.86 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3583 - loss: 2.86 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3577 - loss: 2.87 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3572 - loss: 2.8734\n",
      "Epoch 47: val_accuracy did not improve from 0.35846\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3571 - loss: 2.8736 - val_accuracy: 0.3523 - val_loss: 2.9471\n",
      "Epoch 48/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3945 - loss: 2.647 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3669 - loss: 2.776 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3641 - loss: 2.79 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3620 - loss: 2.81 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3616 - loss: 2.81 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3611 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3599 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3593 - loss: 2.8339\n",
      "Epoch 48: val_accuracy did not improve from 0.35846\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3588 - loss: 2.8379 - val_accuracy: 0.3541 - val_loss: 2.9296\n",
      "Epoch 49/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3828 - loss: 2.754 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3763 - loss: 2.805 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3768 - loss: 2.80 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3757 - loss: 2.80 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3746 - loss: 2.81 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3736 - loss: 2.81 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3724 - loss: 2.81 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3710 - loss: 2.8230\n",
      "Epoch 49: val_accuracy did not improve from 0.35846\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3707 - loss: 2.8240 - val_accuracy: 0.3569 - val_loss: 2.9225\n",
      "Epoch 50/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4258 - loss: 2.607 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3878 - loss: 2.708 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3796 - loss: 2.75 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3737 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3696 - loss: 2.79 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3667 - loss: 2.80 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3646 - loss: 2.81 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3638 - loss: 2.8181\n",
      "Epoch 50: val_accuracy did not improve from 0.35846\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3634 - loss: 2.8194 - val_accuracy: 0.3577 - val_loss: 2.9280\n",
      "Epoch 51/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3281 - loss: 2.844 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3702 - loss: 2.843 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3682 - loss: 2.84 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3667 - loss: 2.84 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3646 - loss: 2.84 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3638 - loss: 2.84 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3634 - loss: 2.84 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3627 - loss: 2.8438\n",
      "Epoch 51: val_accuracy did not improve from 0.35846\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3626 - loss: 2.8437 - val_accuracy: 0.3567 - val_loss: 2.9330\n",
      "Epoch 52/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.3633 - loss: 2.751 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3717 - loss: 2.791 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3693 - loss: 2.80 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3669 - loss: 2.81 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3655 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3650 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3647 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3642 - loss: 2.8270\n",
      "Epoch 52: val_accuracy improved from 0.35846 to 0.35948, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3642 - loss: 2.8272 - val_accuracy: 0.3595 - val_loss: 2.9211\n",
      "Epoch 53/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 46ms/step - accuracy: 0.3789 - loss: 2.846 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - accuracy: 0.3783 - loss: 2.840 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step - accuracy: 0.3682 - loss: 2.852 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.3625 - loss: 2.853 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.3609 - loss: 2.845 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3606 - loss: 2.842 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3604 - loss: 2.84 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3604 - loss: 2.83 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3602 - loss: 2.8384\n",
      "Epoch 53: val_accuracy improved from 0.35948 to 0.36101, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3601 - loss: 2.8385 - val_accuracy: 0.3610 - val_loss: 2.9123\n",
      "Epoch 54/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 44ms/step - accuracy: 0.3750 - loss: 2.749 ━━━━━━━━━━━━━━━━━━━━ 1s 19ms/step - accuracy: 0.3814 - loss: 2.707 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.3795 - loss: 2.723 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.3811 - loss: 2.735 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3796 - loss: 2.754 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3782 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3768 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3755 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3743 - loss: 2.7921\n",
      "Epoch 54: val_accuracy did not improve from 0.36101\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3740 - loss: 2.7934 - val_accuracy: 0.3590 - val_loss: 2.9058\n",
      "Epoch 55/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.3828 - loss: 2.734 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3932 - loss: 2.693 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3901 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3857 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3832 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3814 - loss: 2.75 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3800 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3785 - loss: 2.7685\n",
      "Epoch 55: val_accuracy did not improve from 0.36101\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3777 - loss: 2.7718 - val_accuracy: 0.3579 - val_loss: 2.9183\n",
      "Epoch 56/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.3477 - loss: 2.776 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3357 - loss: 2.848 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3464 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3502 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3512 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3525 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3534 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3543 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3549 - loss: 2.8275\n",
      "Epoch 56: val_accuracy improved from 0.36101 to 0.36203, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3552 - loss: 2.8271 - val_accuracy: 0.3620 - val_loss: 2.9073\n",
      "Epoch 57/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.3516 - loss: 2.917 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3535 - loss: 2.825 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3562 - loss: 2.81 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3582 - loss: 2.81 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3591 - loss: 2.81 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3595 - loss: 2.81 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3595 - loss: 2.81 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3594 - loss: 2.8126\n",
      "Epoch 57: val_accuracy did not improve from 0.36203\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3595 - loss: 2.8130 - val_accuracy: 0.3531 - val_loss: 2.9285\n",
      "Epoch 58/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.3750 - loss: 2.760 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3606 - loss: 2.839 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3625 - loss: 2.83 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3648 - loss: 2.83 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3656 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3656 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3652 - loss: 2.82 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3651 - loss: 2.8261\n",
      "Epoch 58: val_accuracy did not improve from 0.36203\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3650 - loss: 2.8262 - val_accuracy: 0.3572 - val_loss: 2.9227\n",
      "Epoch 59/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3984 - loss: 2.702 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3755 - loss: 2.757 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3720 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3708 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3704 - loss: 2.79 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3702 - loss: 2.79 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3701 - loss: 2.79 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3700 - loss: 2.7948\n",
      "Epoch 59: val_accuracy did not improve from 0.36203\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3699 - loss: 2.7952 - val_accuracy: 0.3567 - val_loss: 2.9065\n",
      "Epoch 60/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4258 - loss: 2.649 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3830 - loss: 2.739 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3754 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3738 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3721 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3712 - loss: 2.79 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3705 - loss: 2.7934\n",
      "Epoch 60: val_accuracy did not improve from 0.36203\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3701 - loss: 2.7959 - val_accuracy: 0.3590 - val_loss: 2.9217\n",
      "Epoch 61/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 4s 71ms/step - accuracy: 0.4180 - loss: 2.614 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3845 - loss: 2.735 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3771 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3725 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3704 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3688 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3679 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3675 - loss: 2.7910\n",
      "Epoch 61: val_accuracy did not improve from 0.36203\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.3674 - loss: 2.7916 - val_accuracy: 0.3556 - val_loss: 2.9218\n",
      "Epoch 62/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3516 - loss: 2.845 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.3848 - loss: 2.770 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.3830 - loss: 2.773 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3781 - loss: 2.782 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3766 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3760 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3754 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3747 - loss: 2.7886\n",
      "Epoch 62: val_accuracy did not improve from 0.36203\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3743 - loss: 2.7896 - val_accuracy: 0.3523 - val_loss: 2.9315\n",
      "Epoch 63/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3945 - loss: 2.611 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3841 - loss: 2.723 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3819 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3796 - loss: 2.75 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3776 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3764 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3754 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3746 - loss: 2.7781\n",
      "Epoch 63: val_accuracy did not improve from 0.36203\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3743 - loss: 2.7792 - val_accuracy: 0.3613 - val_loss: 2.9068\n",
      "Epoch 64/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3516 - loss: 2.851 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3668 - loss: 2.778 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3671 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3678 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3673 - loss: 2.79 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3672 - loss: 2.79 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3668 - loss: 2.80 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3665 - loss: 2.8030\n",
      "Epoch 64: val_accuracy did not improve from 0.36203\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3661 - loss: 2.8047 - val_accuracy: 0.3567 - val_loss: 2.9328\n",
      "Epoch 65/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.3555 - loss: 2.866 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3693 - loss: 2.831 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3691 - loss: 2.81 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3708 - loss: 2.80 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3716 - loss: 2.80 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3717 - loss: 2.80 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3718 - loss: 2.80 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3719 - loss: 2.7983\n",
      "Epoch 65: val_accuracy did not improve from 0.36203\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3717 - loss: 2.7979 - val_accuracy: 0.3613 - val_loss: 2.9119\n",
      "Epoch 66/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.3594 - loss: 2.937 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3731 - loss: 2.782 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3733 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3735 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3739 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3734 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3731 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3726 - loss: 2.7746\n",
      "Epoch 66: val_accuracy did not improve from 0.36203\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3720 - loss: 2.7762 - val_accuracy: 0.3559 - val_loss: 2.9103\n",
      "Epoch 67/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4023 - loss: 2.637 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3791 - loss: 2.751 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3764 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3747 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3734 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3730 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3728 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3726 - loss: 2.7780\n",
      "Epoch 67: val_accuracy did not improve from 0.36203\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3726 - loss: 2.7781 - val_accuracy: 0.3574 - val_loss: 2.9304\n",
      "Epoch 68/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 43ms/step - accuracy: 0.4102 - loss: 2.669 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3766 - loss: 2.766 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3720 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3706 - loss: 2.79 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3710 - loss: 2.79 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3715 - loss: 2.79 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3719 - loss: 2.7943\n",
      "Epoch 68: val_accuracy did not improve from 0.36203\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3725 - loss: 2.7938 - val_accuracy: 0.3562 - val_loss: 2.9094\n",
      "Epoch 69/800\n",
      "53/62 ━━━━━━━━━━━━━━━━━━━━ 3s 60ms/step - accuracy: 0.4219 - loss: 2.591 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3896 - loss: 2.716 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3821 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3775 - loss: 2.75 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3756 - loss: 2.75 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3743 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3735 - loss: 2.7654\n",
      "Epoch 69: val_accuracy improved from 0.36203 to 0.36510, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3730 - loss: 2.7665 - val_accuracy: 0.3651 - val_loss: 2.9040\n",
      "Epoch 70/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 48ms/step - accuracy: 0.3555 - loss: 2.793 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.3615 - loss: 2.819 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3678 - loss: 2.794 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3705 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3731 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3743 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3754 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3760 - loss: 2.7686\n",
      "Epoch 70: val_accuracy did not improve from 0.36510\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3763 - loss: 2.7685 - val_accuracy: 0.3636 - val_loss: 2.8948\n",
      "Epoch 71/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3633 - loss: 2.708 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.3656 - loss: 2.713 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3659 - loss: 2.728 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3669 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3677 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3686 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3692 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3696 - loss: 2.7475\n",
      "Epoch 71: val_accuracy improved from 0.36510 to 0.36638, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3698 - loss: 2.7480 - val_accuracy: 0.3664 - val_loss: 2.8994\n",
      "Epoch 72/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.3945 - loss: 2.749 ━━━━━━━━━━━━━━━━━━━━ 1s 20ms/step - accuracy: 0.3801 - loss: 2.787 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - accuracy: 0.3782 - loss: 2.790 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.3777 - loss: 2.788 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3783 - loss: 2.784 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3788 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3786 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3783 - loss: 2.77 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3780 - loss: 2.7746\n",
      "Epoch 72: val_accuracy improved from 0.36638 to 0.36765, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3780 - loss: 2.7746 - val_accuracy: 0.3677 - val_loss: 2.8997\n",
      "Epoch 73/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 46ms/step - accuracy: 0.3945 - loss: 2.653 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3768 - loss: 2.735 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.3770 - loss: 2.738 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.3756 - loss: 2.748 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3747 - loss: 2.753 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3739 - loss: 2.75 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3736 - loss: 2.75 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3738 - loss: 2.75 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3741 - loss: 2.7599\n",
      "Epoch 73: val_accuracy did not improve from 0.36765\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3742 - loss: 2.7603 - val_accuracy: 0.3620 - val_loss: 2.9259\n",
      "Epoch 74/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 9s 161ms/step - accuracy: 0.4062 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3732 - loss: 2.7499 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3726 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3715 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3715 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3721 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3726 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3729 - loss: 2.7685\n",
      "Epoch 74: val_accuracy improved from 0.36765 to 0.36893, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3729 - loss: 2.7684 - val_accuracy: 0.3689 - val_loss: 2.9024\n",
      "Epoch 75/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 44ms/step - accuracy: 0.4180 - loss: 2.571 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3968 - loss: 2.705 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3911 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3878 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3851 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3835 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3827 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3823 - loss: 2.7495\n",
      "Epoch 75: val_accuracy did not improve from 0.36893\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3822 - loss: 2.7506 - val_accuracy: 0.3636 - val_loss: 2.9175\n",
      "Epoch 76/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 35ms/step - accuracy: 0.4297 - loss: 2.728 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4036 - loss: 2.733 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4003 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3988 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3968 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3942 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3920 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3905 - loss: 2.7354\n",
      "Epoch 76: val_accuracy did not improve from 0.36893\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3904 - loss: 2.7357 - val_accuracy: 0.3648 - val_loss: 2.9065\n",
      "Epoch 77/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.3828 - loss: 2.630 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3767 - loss: 2.685 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3799 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3808 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3809 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3810 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3808 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3806 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3806 - loss: 2.7487\n",
      "Epoch 77: val_accuracy did not improve from 0.36893\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3806 - loss: 2.7492 - val_accuracy: 0.3674 - val_loss: 2.9146\n",
      "Epoch 78/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3359 - loss: 2.960 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3711 - loss: 2.775 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3770 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3794 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3804 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3811 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3809 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3809 - loss: 2.7360\n",
      "Epoch 78: val_accuracy improved from 0.36893 to 0.37711, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3809 - loss: 2.7362 - val_accuracy: 0.3771 - val_loss: 2.8962\n",
      "Epoch 79/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 44ms/step - accuracy: 0.3750 - loss: 2.663 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3899 - loss: 2.674 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3876 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3859 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3851 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3848 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3845 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3841 - loss: 2.7239\n",
      "Epoch 79: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3839 - loss: 2.7249 - val_accuracy: 0.3582 - val_loss: 2.8941\n",
      "Epoch 80/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 3s 54ms/step - accuracy: 0.3867 - loss: 2.799 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3750 - loss: 2.761 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3768 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3798 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3815 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3820 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3825 - loss: 2.7280\n",
      "Epoch 80: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3824 - loss: 2.7288 - val_accuracy: 0.3587 - val_loss: 2.9233\n",
      "Epoch 81/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3594 - loss: 2.840 ━━━━━━━━━━━━━━━━━━━━ 4s 75ms/step - accuracy: 0.3574 - loss: 2.863 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - accuracy: 0.3710 - loss: 2.793 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.3757 - loss: 2.766 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.3767 - loss: 2.760 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3774 - loss: 2.755 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3776 - loss: 2.75 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3779 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3782 - loss: 2.7463\n",
      "Epoch 81: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3783 - loss: 2.7457 - val_accuracy: 0.3715 - val_loss: 2.8990\n",
      "Epoch 82/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 3s 56ms/step - accuracy: 0.3750 - loss: 2.720 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3757 - loss: 2.683 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3769 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3775 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3769 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3764 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3761 - loss: 2.7177\n",
      "Epoch 82: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3760 - loss: 2.7209 - val_accuracy: 0.3651 - val_loss: 2.9011\n",
      "Epoch 83/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 3s 52ms/step - accuracy: 0.3984 - loss: 2.763 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3842 - loss: 2.758 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3891 - loss: 2.73 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3899 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3895 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3885 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3877 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3869 - loss: 2.7234\n",
      "Epoch 83: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3868 - loss: 2.7237 - val_accuracy: 0.3643 - val_loss: 2.9149\n",
      "Epoch 84/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.3984 - loss: 2.639 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.3800 - loss: 2.687 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3898 - loss: 2.692 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3918 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3925 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3921 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3914 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3907 - loss: 2.7107\n",
      "Epoch 84: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3904 - loss: 2.7119 - val_accuracy: 0.3702 - val_loss: 2.8918\n",
      "Epoch 85/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.3867 - loss: 2.641 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.3837 - loss: 2.676 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3816 - loss: 2.707 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3828 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3838 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3836 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3828 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3824 - loss: 2.7135\n",
      "Epoch 85: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3823 - loss: 2.7162 - val_accuracy: 0.3702 - val_loss: 2.9053\n",
      "Epoch 86/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3555 - loss: 2.771 ━━━━━━━━━━━━━━━━━━━━ 1s 18ms/step - accuracy: 0.3800 - loss: 2.730 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.3870 - loss: 2.712 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3881 - loss: 2.706 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3868 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3852 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3846 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3839 - loss: 2.7286\n",
      "Epoch 86: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3833 - loss: 2.7325 - val_accuracy: 0.3628 - val_loss: 2.9094\n",
      "Epoch 87/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4141 - loss: 2.619 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.4060 - loss: 2.599 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.3949 - loss: 2.651 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3919 - loss: 2.679 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3899 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3895 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3887 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3879 - loss: 2.7093\n",
      "Epoch 87: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3870 - loss: 2.7139 - val_accuracy: 0.3720 - val_loss: 2.9064\n",
      "Epoch 88/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4297 - loss: 2.593 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.3953 - loss: 2.711 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3908 - loss: 2.707 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3904 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3903 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3899 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3890 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3881 - loss: 2.7037\n",
      "Epoch 88: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3877 - loss: 2.7055 - val_accuracy: 0.3738 - val_loss: 2.8853\n",
      "Epoch 89/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4180 - loss: 2.476 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3947 - loss: 2.601 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3896 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3886 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3878 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3865 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3859 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3856 - loss: 2.6904\n",
      "Epoch 89: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3855 - loss: 2.6925 - val_accuracy: 0.3746 - val_loss: 2.8950\n",
      "Epoch 90/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4141 - loss: 2.588 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3964 - loss: 2.657 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3947 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3919 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3894 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3877 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3865 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3858 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3853 - loss: 2.7180\n",
      "Epoch 90: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3852 - loss: 2.7188 - val_accuracy: 0.3628 - val_loss: 2.9215\n",
      "Epoch 91/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 4s 72ms/step - accuracy: 0.4414 - loss: 2.418 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4063 - loss: 2.636 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3959 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3935 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3919 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3904 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3894 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3889 - loss: 2.7153\n",
      "Epoch 91: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3886 - loss: 2.7160 - val_accuracy: 0.3746 - val_loss: 2.8965\n",
      "Epoch 92/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.3828 - loss: 2.587 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3785 - loss: 2.652 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3838 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3853 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3857 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3858 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3859 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3858 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3858 - loss: 2.6989\n",
      "Epoch 92: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3858 - loss: 2.6992 - val_accuracy: 0.3674 - val_loss: 2.9095\n",
      "Epoch 93/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.3672 - loss: 2.788 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3657 - loss: 2.796 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3705 - loss: 2.78 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3731 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3740 - loss: 2.76 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3754 - loss: 2.75 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3760 - loss: 2.75 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3769 - loss: 2.7465\n",
      "Epoch 93: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3776 - loss: 2.7431 - val_accuracy: 0.3720 - val_loss: 2.8799\n",
      "Epoch 94/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 43ms/step - accuracy: 0.4219 - loss: 2.750 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4005 - loss: 2.736 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3997 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3992 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3970 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3956 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3940 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3932 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3929 - loss: 2.7097\n",
      "Epoch 94: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3928 - loss: 2.7095 - val_accuracy: 0.3697 - val_loss: 2.9023\n",
      "Epoch 95/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 44ms/step - accuracy: 0.3906 - loss: 2.698 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.4138 - loss: 2.625 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4038 - loss: 2.651 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3997 - loss: 2.662 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3962 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3938 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3917 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3903 - loss: 2.6966\n",
      "Epoch 95: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3894 - loss: 2.6992 - val_accuracy: 0.3687 - val_loss: 2.8993\n",
      "Epoch 96/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 4s 68ms/step - accuracy: 0.3867 - loss: 2.631 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3782 - loss: 2.709 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3772 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3779 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3787 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3797 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3809 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3816 - loss: 2.7155\n",
      "Epoch 96: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3819 - loss: 2.7152 - val_accuracy: 0.3728 - val_loss: 2.9066\n",
      "Epoch 97/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4102 - loss: 2.686 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3974 - loss: 2.711 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3951 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3934 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3923 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3915 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3910 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3905 - loss: 2.7046\n",
      "Epoch 97: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3903 - loss: 2.7044 - val_accuracy: 0.3710 - val_loss: 2.9078\n",
      "Epoch 98/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.4180 - loss: 2.586 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3892 - loss: 2.700 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3869 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3874 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3877 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3881 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3882 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3882 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3880 - loss: 2.7210\n",
      "Epoch 98: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3880 - loss: 2.7210 - val_accuracy: 0.3679 - val_loss: 2.9063\n",
      "Epoch 99/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 3s 51ms/step - accuracy: 0.4180 - loss: 2.599 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4028 - loss: 2.636 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4006 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3993 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3978 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3965 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3952 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3941 - loss: 2.6824\n",
      "Epoch 99: val_accuracy did not improve from 0.37711\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3936 - loss: 2.6841 - val_accuracy: 0.3707 - val_loss: 2.9085\n",
      "Epoch 100/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4570 - loss: 2.647 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4198 - loss: 2.651 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4103 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4071 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4056 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4039 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4022 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4010 - loss: 2.6839\n",
      "Epoch 100: val_accuracy improved from 0.37711 to 0.37813, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3997 - loss: 2.6874 - val_accuracy: 0.3781 - val_loss: 2.8950\n",
      "Epoch 101/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.4336 - loss: 2.432 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4027 - loss: 2.568 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3962 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3928 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3916 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3914 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3912 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3910 - loss: 2.6788\n",
      "Epoch 101: val_accuracy did not improve from 0.37813\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3908 - loss: 2.6815 - val_accuracy: 0.3636 - val_loss: 2.9213\n",
      "Epoch 102/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3867 - loss: 2.781 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.3862 - loss: 2.738 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3882 - loss: 2.725 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3881 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3875 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3871 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3871 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3873 - loss: 2.7143\n",
      "Epoch 102: val_accuracy did not improve from 0.37813\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3874 - loss: 2.7122 - val_accuracy: 0.3666 - val_loss: 2.9037\n",
      "Epoch 103/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.4219 - loss: 2.478 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - accuracy: 0.4084 - loss: 2.603 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4002 - loss: 2.646 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3946 - loss: 2.669 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3906 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3887 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3882 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3879 - loss: 2.6956\n",
      "Epoch 103: val_accuracy did not improve from 0.37813\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3876 - loss: 2.6985 - val_accuracy: 0.3679 - val_loss: 2.9124\n",
      "Epoch 104/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 5s 98ms/step - accuracy: 0.3789 - loss: 2.831 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3990 - loss: 2.694 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4002 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3982 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3966 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3950 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3940 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3936 - loss: 2.6863\n",
      "Epoch 104: val_accuracy did not improve from 0.37813\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3935 - loss: 2.6866 - val_accuracy: 0.3712 - val_loss: 2.9158\n",
      "Epoch 105/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4062 - loss: 2.571 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3990 - loss: 2.644 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3954 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3940 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3937 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3941 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3941 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3939 - loss: 2.6760\n",
      "Epoch 105: val_accuracy did not improve from 0.37813\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3938 - loss: 2.6770 - val_accuracy: 0.3735 - val_loss: 2.9133\n",
      "Epoch 106/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3945 - loss: 2.636 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3917 - loss: 2.685 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3919 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3924 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3930 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3929 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3928 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3927 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3926 - loss: 2.6928\n",
      "Epoch 106: val_accuracy improved from 0.37813 to 0.37915, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3924 - loss: 2.6930 - val_accuracy: 0.3792 - val_loss: 2.9092\n",
      "Epoch 107/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.3711 - loss: 2.875 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3807 - loss: 2.734 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3835 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3848 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3856 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3861 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3866 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3867 - loss: 2.7116\n",
      "Epoch 107: val_accuracy did not improve from 0.37915\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3867 - loss: 2.7116 - val_accuracy: 0.3730 - val_loss: 2.9249\n",
      "Epoch 108/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.3945 - loss: 2.644 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4018 - loss: 2.647 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4021 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4026 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4032 - loss: 2.644 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4028 - loss: 2.643 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4014 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4001 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3988 - loss: 2.6559\n",
      "Epoch 108: val_accuracy did not improve from 0.37915\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3984 - loss: 2.6576 - val_accuracy: 0.3753 - val_loss: 2.9062\n",
      "Epoch 109/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 3s 59ms/step - accuracy: 0.4297 - loss: 2.573 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3969 - loss: 2.672 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3977 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3990 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3996 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3991 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3983 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3975 - loss: 2.6696\n",
      "Epoch 109: val_accuracy did not improve from 0.37915\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3973 - loss: 2.6704 - val_accuracy: 0.3735 - val_loss: 2.9164\n",
      "Epoch 110/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 43ms/step - accuracy: 0.3828 - loss: 2.825 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3983 - loss: 2.699 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3943 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3919 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3907 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3905 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3906 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3903 - loss: 2.70 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3901 - loss: 2.7074\n",
      "Epoch 110: val_accuracy did not improve from 0.37915\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3901 - loss: 2.7074 - val_accuracy: 0.3735 - val_loss: 2.8995\n",
      "Epoch 111/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4375 - loss: 2.464 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4212 - loss: 2.543 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4158 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4115 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4096 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4070 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4052 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4037 - loss: 2.6329\n",
      "Epoch 111: val_accuracy did not improve from 0.37915\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4027 - loss: 2.6373 - val_accuracy: 0.3723 - val_loss: 2.8980\n",
      "Epoch 112/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4180 - loss: 2.591 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4052 - loss: 2.629 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3987 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3953 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3930 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3927 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3927 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3927 - loss: 2.6826\n",
      "Epoch 112: val_accuracy did not improve from 0.37915\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3927 - loss: 2.6833 - val_accuracy: 0.3738 - val_loss: 2.9143\n",
      "Epoch 113/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4141 - loss: 2.710 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4058 - loss: 2.620 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4005 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3981 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3970 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3954 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3940 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3932 - loss: 2.6796\n",
      "Epoch 113: val_accuracy did not improve from 0.37915\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3929 - loss: 2.6809 - val_accuracy: 0.3689 - val_loss: 2.9313\n",
      "Epoch 114/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 46ms/step - accuracy: 0.3984 - loss: 2.649 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.3849 - loss: 2.788 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3855 - loss: 2.771 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3889 - loss: 2.74 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3915 - loss: 2.72 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3923 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3933 - loss: 2.71 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3939 - loss: 2.7071\n",
      "Epoch 114: val_accuracy did not improve from 0.37915\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3942 - loss: 2.7037 - val_accuracy: 0.3776 - val_loss: 2.9091\n",
      "Epoch 115/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.3867 - loss: 2.759 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3967 - loss: 2.656 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4000 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4011 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4015 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4020 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4021 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4019 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4015 - loss: 2.6514\n",
      "Epoch 115: val_accuracy did not improve from 0.37915\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4015 - loss: 2.6519 - val_accuracy: 0.3761 - val_loss: 2.8987\n",
      "Epoch 116/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4297 - loss: 2.434 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4134 - loss: 2.554 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4071 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4037 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4022 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4004 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3989 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3979 - loss: 2.6617\n",
      "Epoch 116: val_accuracy did not improve from 0.37915\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3976 - loss: 2.6636 - val_accuracy: 0.3756 - val_loss: 2.9112\n",
      "Epoch 117/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.5117 - loss: 2.403 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4310 - loss: 2.574 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4227 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4183 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4155 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4132 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4111 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4091 - loss: 2.6305\n",
      "Epoch 117: val_accuracy improved from 0.37915 to 0.37941, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4085 - loss: 2.6323 - val_accuracy: 0.3794 - val_loss: 2.9112\n",
      "Epoch 118/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 3s 56ms/step - accuracy: 0.3945 - loss: 2.561 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4009 - loss: 2.606 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3953 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3911 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3900 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3897 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3896 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3896 - loss: 2.6773\n",
      "Epoch 118: val_accuracy did not improve from 0.37941\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.3897 - loss: 2.6777 - val_accuracy: 0.3769 - val_loss: 2.9018\n",
      "Epoch 119/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 3s 55ms/step - accuracy: 0.4883 - loss: 2.366 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4181 - loss: 2.566 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4060 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4023 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4007 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3995 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3989 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3986 - loss: 2.6470\n",
      "Epoch 119: val_accuracy improved from 0.37941 to 0.38017, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3984 - loss: 2.6497 - val_accuracy: 0.3802 - val_loss: 2.9061\n",
      "Epoch 120/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.3711 - loss: 2.690 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3894 - loss: 2.693 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3974 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3983 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3997 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4007 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4012 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4013 - loss: 2.6600\n",
      "Epoch 120: val_accuracy did not improve from 0.38017\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4011 - loss: 2.6606 - val_accuracy: 0.3682 - val_loss: 2.9103\n",
      "Epoch 121/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 45ms/step - accuracy: 0.4023 - loss: 2.575 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3972 - loss: 2.619 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3993 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3982 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3974 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3968 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3969 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3964 - loss: 2.6527\n",
      "Epoch 121: val_accuracy did not improve from 0.38017\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3960 - loss: 2.6556 - val_accuracy: 0.3743 - val_loss: 2.9186\n",
      "Epoch 122/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 45ms/step - accuracy: 0.4180 - loss: 2.540 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4083 - loss: 2.610 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4028 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4013 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4008 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4000 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3996 - loss: 2.6708\n",
      "Epoch 122: val_accuracy did not improve from 0.38017\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3992 - loss: 2.6702 - val_accuracy: 0.3779 - val_loss: 2.8958\n",
      "Epoch 123/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4375 - loss: 2.589 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4044 - loss: 2.665 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3973 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3950 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3935 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3928 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3925 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3925 - loss: 2.6870\n",
      "Epoch 123: val_accuracy did not improve from 0.38017\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3925 - loss: 2.6868 - val_accuracy: 0.3758 - val_loss: 2.9020\n",
      "Epoch 124/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 3s 50ms/step - accuracy: 0.3867 - loss: 2.750 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4010 - loss: 2.698 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3971 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3956 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3947 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3949 - loss: 2.69 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3948 - loss: 2.68 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3947 - loss: 2.6885\n",
      "Epoch 124: val_accuracy did not improve from 0.38017\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3947 - loss: 2.6880 - val_accuracy: 0.3723 - val_loss: 2.9058\n",
      "Epoch 125/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4180 - loss: 2.656 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3974 - loss: 2.656 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3947 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3935 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3928 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3929 - loss: 2.67 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3934 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3935 - loss: 2.6687\n",
      "Epoch 125: val_accuracy did not improve from 0.38017\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3936 - loss: 2.6683 - val_accuracy: 0.3702 - val_loss: 2.9126\n",
      "Epoch 126/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4023 - loss: 2.640 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3998 - loss: 2.679 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4006 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3989 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3979 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3973 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3967 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3963 - loss: 2.6668\n",
      "Epoch 126: val_accuracy did not improve from 0.38017\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3961 - loss: 2.6671 - val_accuracy: 0.3802 - val_loss: 2.9049\n",
      "Epoch 127/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4023 - loss: 2.634 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4170 - loss: 2.573 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4120 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4113 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4095 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4087 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4075 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4061 - loss: 2.6157\n",
      "Epoch 127: val_accuracy did not improve from 0.38017\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4050 - loss: 2.6207 - val_accuracy: 0.3763 - val_loss: 2.9012\n",
      "Epoch 128/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 3s 54ms/step - accuracy: 0.4219 - loss: 2.546 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4204 - loss: 2.565 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4165 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4119 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4097 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4074 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4057 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4049 - loss: 2.6475\n",
      "Epoch 128: val_accuracy did not improve from 0.38017\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4044 - loss: 2.6497 - val_accuracy: 0.3751 - val_loss: 2.9087\n",
      "Epoch 129/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 45ms/step - accuracy: 0.4180 - loss: 2.497 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4163 - loss: 2.604 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4142 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4121 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4098 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4085 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4072 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4061 - loss: 2.6437\n",
      "Epoch 129: val_accuracy improved from 0.38017 to 0.38043, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4053 - loss: 2.6455 - val_accuracy: 0.3804 - val_loss: 2.8983\n",
      "Epoch 130/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4219 - loss: 2.573 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4068 - loss: 2.651 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4081 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4059 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4041 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4024 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4014 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4006 - loss: 2.6700\n",
      "Epoch 130: val_accuracy did not improve from 0.38043\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4001 - loss: 2.6704 - val_accuracy: 0.3751 - val_loss: 2.9107\n",
      "Epoch 131/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 5s 89ms/step - accuracy: 0.4609 - loss: 2.538 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4193 - loss: 2.581 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4038 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4002 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3986 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3977 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3976 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3977 - loss: 2.6337\n",
      "Epoch 131: val_accuracy did not improve from 0.38043\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3978 - loss: 2.6355 - val_accuracy: 0.3753 - val_loss: 2.9099\n",
      "Epoch 132/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4023 - loss: 2.626 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4043 - loss: 2.641 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4009 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3985 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3983 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3983 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3977 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3976 - loss: 2.6695\n",
      "Epoch 132: val_accuracy did not improve from 0.38043\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3976 - loss: 2.6696 - val_accuracy: 0.3751 - val_loss: 2.9142\n",
      "Epoch 133/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.4297 - loss: 2.535 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4029 - loss: 2.578 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4040 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4029 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4019 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4013 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4006 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4003 - loss: 2.6238\n",
      "Epoch 133: val_accuracy did not improve from 0.38043\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4004 - loss: 2.6281 - val_accuracy: 0.3774 - val_loss: 2.9070\n",
      "Epoch 134/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4180 - loss: 2.568 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4051 - loss: 2.617 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3995 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3977 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3972 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3971 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3972 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3974 - loss: 2.6639\n",
      "Epoch 134: val_accuracy did not improve from 0.38043\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3974 - loss: 2.6641 - val_accuracy: 0.3792 - val_loss: 2.9087\n",
      "Epoch 135/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3906 - loss: 2.672 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4073 - loss: 2.628 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4027 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4008 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3983 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3965 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3956 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3955 - loss: 2.6551\n",
      "Epoch 135: val_accuracy did not improve from 0.38043\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3956 - loss: 2.6559 - val_accuracy: 0.3720 - val_loss: 2.9272\n",
      "Epoch 136/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3984 - loss: 2.746 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.4160 - loss: 2.626 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4152 - loss: 2.628 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4115 - loss: 2.639 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4097 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4083 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4071 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4060 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4056 - loss: 2.6513\n",
      "Epoch 136: val_accuracy did not improve from 0.38043\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4052 - loss: 2.6523 - val_accuracy: 0.3728 - val_loss: 2.9129\n",
      "Epoch 137/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4180 - loss: 2.733 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4199 - loss: 2.669 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4172 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4145 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4127 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4118 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4114 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4109 - loss: 2.6535\n",
      "Epoch 137: val_accuracy did not improve from 0.38043\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4104 - loss: 2.6530 - val_accuracy: 0.3763 - val_loss: 2.8960\n",
      "Epoch 138/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4336 - loss: 2.504 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4051 - loss: 2.642 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4033 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4022 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4022 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4020 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4018 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4017 - loss: 2.6559\n",
      "Epoch 138: val_accuracy did not improve from 0.38043\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4017 - loss: 2.6558 - val_accuracy: 0.3740 - val_loss: 2.8926\n",
      "Epoch 139/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.4609 - loss: 2.395 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4323 - loss: 2.516 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4196 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4156 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4131 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4119 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4109 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4104 - loss: 2.6073\n",
      "Epoch 139: val_accuracy did not improve from 0.38043\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4100 - loss: 2.6091 - val_accuracy: 0.3804 - val_loss: 2.9007\n",
      "Epoch 140/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 3s 52ms/step - accuracy: 0.4219 - loss: 2.511 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4301 - loss: 2.493 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4216 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4169 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4133 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4112 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4099 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4090 - loss: 2.6041\n",
      "Epoch 140: val_accuracy did not improve from 0.38043\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4088 - loss: 2.6051 - val_accuracy: 0.3761 - val_loss: 2.9185\n",
      "Epoch 141/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3945 - loss: 2.634 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4054 - loss: 2.607 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4028 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4005 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3988 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3986 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3985 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3985 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3983 - loss: 2.6579\n",
      "Epoch 141: val_accuracy did not improve from 0.38043\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3983 - loss: 2.6590 - val_accuracy: 0.3776 - val_loss: 2.9188\n",
      "Epoch 142/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4570 - loss: 2.460 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4288 - loss: 2.561 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4206 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4141 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4116 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4090 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4072 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4056 - loss: 2.6368\n",
      "Epoch 142: val_accuracy improved from 0.38043 to 0.38196, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4049 - loss: 2.6400 - val_accuracy: 0.3820 - val_loss: 2.9029\n",
      "Epoch 143/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.4453 - loss: 2.580 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4240 - loss: 2.558 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4209 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4176 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4153 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4136 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4123 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4113 - loss: 2.6203\n",
      "Epoch 143: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4108 - loss: 2.6228 - val_accuracy: 0.3763 - val_loss: 2.9215\n",
      "Epoch 144/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4414 - loss: 2.475 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4156 - loss: 2.570 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4112 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4115 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4110 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4105 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4104 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4103 - loss: 2.6153\n",
      "Epoch 144: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4101 - loss: 2.6169 - val_accuracy: 0.3743 - val_loss: 2.9228\n",
      "Epoch 145/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 48ms/step - accuracy: 0.4062 - loss: 2.640 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - accuracy: 0.4068 - loss: 2.605 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.4072 - loss: 2.615 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4069 - loss: 2.624 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4071 - loss: 2.630 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4073 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4068 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4062 - loss: 2.6351\n",
      "Epoch 145: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4058 - loss: 2.6360 - val_accuracy: 0.3743 - val_loss: 2.9134\n",
      "Epoch 146/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4180 - loss: 2.637 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4224 - loss: 2.601 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4140 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4115 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4111 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4104 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4097 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4087 - loss: 2.6379\n",
      "Epoch 146: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4082 - loss: 2.6391 - val_accuracy: 0.3817 - val_loss: 2.9069\n",
      "Epoch 147/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4297 - loss: 2.547 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4130 - loss: 2.580 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4106 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4094 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4083 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4072 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4064 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4054 - loss: 2.6252\n",
      "Epoch 147: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4047 - loss: 2.6270 - val_accuracy: 0.3743 - val_loss: 2.9237\n",
      "Epoch 148/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.3789 - loss: 2.541 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3999 - loss: 2.583 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4041 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4040 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4039 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4041 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4041 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4041 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4040 - loss: 2.6222\n",
      "Epoch 148: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4039 - loss: 2.6240 - val_accuracy: 0.3761 - val_loss: 2.9088\n",
      "Epoch 149/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 4s 74ms/step - accuracy: 0.4062 - loss: 2.645 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3948 - loss: 2.625 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3964 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3971 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3976 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3978 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3983 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3988 - loss: 2.6458\n",
      "Epoch 149: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3993 - loss: 2.6465 - val_accuracy: 0.3786 - val_loss: 2.9070\n",
      "Epoch 150/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4102 - loss: 2.660 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4054 - loss: 2.633 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4051 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4051 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4042 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4044 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4047 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4047 - loss: 2.6425\n",
      "Epoch 150: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4048 - loss: 2.6425 - val_accuracy: 0.3735 - val_loss: 2.8944\n",
      "Epoch 151/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.3828 - loss: 2.710 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4007 - loss: 2.624 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3974 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3969 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3971 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3979 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3985 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3990 - loss: 2.6413\n",
      "Epoch 151: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3994 - loss: 2.6417 - val_accuracy: 0.3774 - val_loss: 2.9064\n",
      "Epoch 152/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4062 - loss: 2.695 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step - accuracy: 0.4121 - loss: 2.647 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.4124 - loss: 2.621 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4095 - loss: 2.619 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4081 - loss: 2.619 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4067 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4055 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4046 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4040 - loss: 2.6285\n",
      "Epoch 152: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4039 - loss: 2.6287 - val_accuracy: 0.3769 - val_loss: 2.9056\n",
      "Epoch 153/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.3906 - loss: 2.563 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4047 - loss: 2.584 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4050 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4052 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4061 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4060 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4058 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4055 - loss: 2.6276\n",
      "Epoch 153: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4055 - loss: 2.6285 - val_accuracy: 0.3817 - val_loss: 2.9020\n",
      "Epoch 154/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 3s 54ms/step - accuracy: 0.4375 - loss: 2.536 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4154 - loss: 2.593 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4110 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4065 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4039 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4034 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4026 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4022 - loss: 2.6447\n",
      "Epoch 154: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.4020 - loss: 2.6456 - val_accuracy: 0.3781 - val_loss: 2.9118\n",
      "Epoch 155/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 3s 56ms/step - accuracy: 0.4258 - loss: 2.553 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4108 - loss: 2.563 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4070 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4068 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4069 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4074 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4079 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4079 - loss: 2.6246\n",
      "Epoch 155: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4080 - loss: 2.6255 - val_accuracy: 0.3789 - val_loss: 2.9255\n",
      "Epoch 156/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4258 - loss: 2.475 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4353 - loss: 2.511 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4302 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4274 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4244 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4214 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4201 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4184 - loss: 2.5861\n",
      "Epoch 156: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4172 - loss: 2.5916 - val_accuracy: 0.3743 - val_loss: 2.9026\n",
      "Epoch 157/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4297 - loss: 2.496 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4116 - loss: 2.588 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4076 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4050 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4043 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4040 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4037 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4036 - loss: 2.6413\n",
      "Epoch 157: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4035 - loss: 2.6422 - val_accuracy: 0.3753 - val_loss: 2.9194\n",
      "Epoch 158/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4180 - loss: 2.655 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4067 - loss: 2.660 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4031 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4027 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4024 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4015 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4017 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4020 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4024 - loss: 2.6389\n",
      "Epoch 158: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4025 - loss: 2.6387 - val_accuracy: 0.3717 - val_loss: 2.9015\n",
      "Epoch 159/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4375 - loss: 2.473 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4255 - loss: 2.539 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4167 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4134 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4092 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4073 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4061 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4056 - loss: 2.6137\n",
      "Epoch 159: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4055 - loss: 2.6151 - val_accuracy: 0.3784 - val_loss: 2.9057\n",
      "Epoch 160/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3906 - loss: 2.774 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4116 - loss: 2.631 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4139 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4114 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4090 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4075 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4063 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4055 - loss: 2.6314\n",
      "Epoch 160: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4053 - loss: 2.6320 - val_accuracy: 0.3771 - val_loss: 2.9131\n",
      "Epoch 161/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4219 - loss: 2.578 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4224 - loss: 2.548 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4232 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4209 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4196 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4177 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4165 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4152 - loss: 2.5999\n",
      "Epoch 161: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4148 - loss: 2.6018 - val_accuracy: 0.3748 - val_loss: 2.9159\n",
      "Epoch 162/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.3750 - loss: 2.684 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4150 - loss: 2.582 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4137 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4133 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4135 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4127 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4122 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4119 - loss: 2.6004\n",
      "Epoch 162: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4117 - loss: 2.6020 - val_accuracy: 0.3725 - val_loss: 2.9111\n",
      "Epoch 163/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4531 - loss: 2.440 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.4234 - loss: 2.589 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4160 - loss: 2.613 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4121 - loss: 2.621 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4102 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4086 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4075 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4071 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4069 - loss: 2.6350\n",
      "Epoch 163: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4069 - loss: 2.6351 - val_accuracy: 0.3817 - val_loss: 2.8909\n",
      "Epoch 164/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4258 - loss: 2.604 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4030 - loss: 2.553 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3985 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3987 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3998 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4003 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4007 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4012 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4018 - loss: 2.6247\n",
      "Epoch 164: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4018 - loss: 2.6251 - val_accuracy: 0.3682 - val_loss: 2.9287\n",
      "Epoch 165/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 7s 122ms/step - accuracy: 0.4258 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4232 - loss: 2.6018 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4206 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4164 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4140 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4130 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4119 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4112 - loss: 2.6161\n",
      "Epoch 165: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4106 - loss: 2.6173 - val_accuracy: 0.3743 - val_loss: 2.9183\n",
      "Epoch 166/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4453 - loss: 2.479 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4221 - loss: 2.596 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4155 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4116 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4100 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4095 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4090 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4086 - loss: 2.6284\n",
      "Epoch 166: val_accuracy did not improve from 0.38196\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4086 - loss: 2.6286 - val_accuracy: 0.3820 - val_loss: 2.9070\n",
      "Epoch 167/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3750 - loss: 2.732 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4064 - loss: 2.609 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4088 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4100 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4092 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4085 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4082 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4086 - loss: 2.6152\n",
      "Epoch 167: val_accuracy improved from 0.38196 to 0.38401, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4087 - loss: 2.6152 - val_accuracy: 0.3840 - val_loss: 2.9015\n",
      "Epoch 168/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 3s 52ms/step - accuracy: 0.4766 - loss: 2.385 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4327 - loss: 2.549 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4248 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4214 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4198 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4187 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4181 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4174 - loss: 2.6008\n",
      "Epoch 168: val_accuracy did not improve from 0.38401\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4169 - loss: 2.6033 - val_accuracy: 0.3774 - val_loss: 2.9370\n",
      "Epoch 169/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4102 - loss: 2.469 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4053 - loss: 2.568 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4071 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4061 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4046 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4028 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4022 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4018 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4017 - loss: 2.6241\n",
      "Epoch 169: val_accuracy did not improve from 0.38401\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4017 - loss: 2.6247 - val_accuracy: 0.3815 - val_loss: 2.9383\n",
      "Epoch 170/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 44ms/step - accuracy: 0.3945 - loss: 2.557 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.3804 - loss: 2.625 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3865 - loss: 2.637 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3925 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3944 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3954 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3961 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3968 - loss: 2.6318\n",
      "Epoch 170: val_accuracy did not improve from 0.38401\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.3974 - loss: 2.6322 - val_accuracy: 0.3779 - val_loss: 2.9392\n",
      "Epoch 171/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4375 - loss: 2.380 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4123 - loss: 2.578 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4063 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4050 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4054 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4057 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4060 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4062 - loss: 2.6217\n",
      "Epoch 171: val_accuracy did not improve from 0.38401\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4063 - loss: 2.6233 - val_accuracy: 0.3761 - val_loss: 2.9028\n",
      "Epoch 172/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4531 - loss: 2.454 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4194 - loss: 2.586 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4100 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4075 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4040 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4024 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4022 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4029 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4034 - loss: 2.6489\n",
      "Epoch 172: val_accuracy did not improve from 0.38401\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4035 - loss: 2.6479 - val_accuracy: 0.3746 - val_loss: 2.9269\n",
      "Epoch 173/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.5117 - loss: 2.400 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4410 - loss: 2.565 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4308 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4250 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4234 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4213 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4197 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4183 - loss: 2.5977\n",
      "Epoch 173: val_accuracy did not improve from 0.38401\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4170 - loss: 2.5999 - val_accuracy: 0.3746 - val_loss: 2.9210\n",
      "Epoch 174/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 4s 81ms/step - accuracy: 0.3594 - loss: 2.683 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.3808 - loss: 2.672 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3846 - loss: 2.673 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3928 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3963 - loss: 2.64 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3978 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3986 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3994 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3999 - loss: 2.6313\n",
      "Epoch 174: val_accuracy did not improve from 0.38401\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4002 - loss: 2.6305 - val_accuracy: 0.3633 - val_loss: 2.9406\n",
      "Epoch 175/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4336 - loss: 2.560 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4140 - loss: 2.583 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4132 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4116 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4116 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4115 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4119 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4119 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4118 - loss: 2.6029\n",
      "Epoch 175: val_accuracy improved from 0.38401 to 0.38579, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 10ms/step - accuracy: 0.4118 - loss: 2.6033 - val_accuracy: 0.3858 - val_loss: 2.9232\n",
      "Epoch 176/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 3s 55ms/step - accuracy: 0.4180 - loss: 2.641 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4194 - loss: 2.593 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4172 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4151 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4141 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4139 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4138 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4136 - loss: 2.5972\n",
      "Epoch 176: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4130 - loss: 2.6001 - val_accuracy: 0.3774 - val_loss: 2.9179\n",
      "Epoch 177/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4219 - loss: 2.613 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4105 - loss: 2.571 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.4096 - loss: 2.576 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.4062 - loss: 2.595 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4082 - loss: 2.592 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4095 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4107 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4112 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4111 - loss: 2.5900\n",
      "Epoch 177: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4111 - loss: 2.5907 - val_accuracy: 0.3776 - val_loss: 2.9190\n",
      "Epoch 178/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4492 - loss: 2.451 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4247 - loss: 2.549 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4215 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4197 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4187 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4172 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4162 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4152 - loss: 2.5872\n",
      "Epoch 178: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4147 - loss: 2.5891 - val_accuracy: 0.3835 - val_loss: 2.9246\n",
      "Epoch 179/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 44ms/step - accuracy: 0.4297 - loss: 2.464 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4153 - loss: 2.589 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4109 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4108 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4107 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4101 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4096 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4097 - loss: 2.6064\n",
      "Epoch 179: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4098 - loss: 2.6065 - val_accuracy: 0.3825 - val_loss: 2.9252\n",
      "Epoch 180/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 3s 55ms/step - accuracy: 0.3984 - loss: 2.467 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3935 - loss: 2.578 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.3999 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4046 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4072 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4081 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4087 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4092 - loss: 2.6050\n",
      "Epoch 180: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4093 - loss: 2.6056 - val_accuracy: 0.3758 - val_loss: 2.9055\n",
      "Epoch 181/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.4336 - loss: 2.491 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4136 - loss: 2.576 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4095 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4071 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4065 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4052 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4046 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4044 - loss: 2.6255\n",
      "Epoch 181: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4041 - loss: 2.6270 - val_accuracy: 0.3723 - val_loss: 2.9161\n",
      "Epoch 182/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 4s 68ms/step - accuracy: 0.4102 - loss: 2.585 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4274 - loss: 2.554 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4260 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4203 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4165 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4151 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4145 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4144 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4141 - loss: 2.5894\n",
      "Epoch 182: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4140 - loss: 2.5899 - val_accuracy: 0.3763 - val_loss: 2.9181\n",
      "Epoch 183/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4141 - loss: 2.607 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4156 - loss: 2.585 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4154 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4133 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4121 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4120 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4118 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4118 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4118 - loss: 2.6135\n",
      "Epoch 183: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4118 - loss: 2.6136 - val_accuracy: 0.3827 - val_loss: 2.9067\n",
      "Epoch 184/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3867 - loss: 2.643 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4111 - loss: 2.576 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4121 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4110 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4110 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4105 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4098 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4094 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4091 - loss: 2.6061\n",
      "Epoch 184: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4089 - loss: 2.6089 - val_accuracy: 0.3835 - val_loss: 2.8973\n",
      "Epoch 185/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4570 - loss: 2.503 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4279 - loss: 2.502 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4195 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4168 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4157 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4144 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4134 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4125 - loss: 2.5768\n",
      "Epoch 185: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4121 - loss: 2.5798 - val_accuracy: 0.3835 - val_loss: 2.9087\n",
      "Epoch 186/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 3s 64ms/step - accuracy: 0.3867 - loss: 2.836 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4078 - loss: 2.643 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4129 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4137 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4135 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4131 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4128 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4127 - loss: 2.6044\n",
      "Epoch 186: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4129 - loss: 2.6036 - val_accuracy: 0.3825 - val_loss: 2.9132\n",
      "Epoch 187/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4062 - loss: 2.581 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3991 - loss: 2.570 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3992 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4008 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4011 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4014 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4020 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4025 - loss: 2.6205\n",
      "Epoch 187: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4025 - loss: 2.6212 - val_accuracy: 0.3766 - val_loss: 2.9224\n",
      "Epoch 188/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 6s 109ms/step - accuracy: 0.4023 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4175 - loss: 2.5742 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4160 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4164 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4158 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4146 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4141 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4136 - loss: 2.5941\n",
      "Epoch 188: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4131 - loss: 2.5945 - val_accuracy: 0.3766 - val_loss: 2.9423\n",
      "Epoch 189/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4375 - loss: 2.511 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4189 - loss: 2.571 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4158 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4140 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4125 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4107 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4095 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4089 - loss: 2.6145\n",
      "Epoch 189: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4088 - loss: 2.6147 - val_accuracy: 0.3723 - val_loss: 2.9198\n",
      "Epoch 190/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4570 - loss: 2.456 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4128 - loss: 2.554 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4126 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4129 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4130 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4126 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4119 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4117 - loss: 2.5870\n",
      "Epoch 190: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4116 - loss: 2.5896 - val_accuracy: 0.3735 - val_loss: 2.9140\n",
      "Epoch 191/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4453 - loss: 2.629 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4247 - loss: 2.589 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4196 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4164 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4155 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4150 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4146 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4140 - loss: 2.6021\n",
      "Epoch 191: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4137 - loss: 2.6044 - val_accuracy: 0.3753 - val_loss: 2.9442\n",
      "Epoch 192/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 46ms/step - accuracy: 0.4062 - loss: 2.582 ━━━━━━━━━━━━━━━━━━━━ 3s 55ms/step - accuracy: 0.3984 - loss: 2.607 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step - accuracy: 0.3969 - loss: 2.636 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.3938 - loss: 2.658 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.3924 - loss: 2.663 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3923 - loss: 2.664 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3932 - loss: 2.66 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.3941 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3955 - loss: 2.65 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.3968 - loss: 2.6506\n",
      "Epoch 192: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.3970 - loss: 2.6500 - val_accuracy: 0.3827 - val_loss: 2.9032\n",
      "Epoch 193/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4414 - loss: 2.484 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4191 - loss: 2.595 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4200 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4184 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4176 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4169 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4160 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4155 - loss: 2.5914\n",
      "Epoch 193: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4151 - loss: 2.5937 - val_accuracy: 0.3779 - val_loss: 2.9325\n",
      "Epoch 194/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.3867 - loss: 2.589 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.3966 - loss: 2.632 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4004 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4012 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4017 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4027 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4034 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4042 - loss: 2.6246\n",
      "Epoch 194: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4050 - loss: 2.6207 - val_accuracy: 0.3779 - val_loss: 2.9190\n",
      "Epoch 195/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 4s 77ms/step - accuracy: 0.4570 - loss: 2.564 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4473 - loss: 2.581 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4335 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4283 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4240 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4221 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4209 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4204 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4197 - loss: 2.6079\n",
      "Epoch 195: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4196 - loss: 2.6078 - val_accuracy: 0.3792 - val_loss: 2.9217\n",
      "Epoch 196/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4023 - loss: 2.609 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4095 - loss: 2.593 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4097 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4105 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4115 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4123 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4126 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4128 - loss: 2.5851\n",
      "Epoch 196: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4129 - loss: 2.5862 - val_accuracy: 0.3758 - val_loss: 2.9236\n",
      "Epoch 197/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.5039 - loss: 2.307 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4618 - loss: 2.437 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4461 - loss: 2.483 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4383 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4330 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4305 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4270 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4244 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4227 - loss: 2.5622\n",
      "Epoch 197: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4222 - loss: 2.5643 - val_accuracy: 0.3804 - val_loss: 2.9207\n",
      "Epoch 198/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 3s 54ms/step - accuracy: 0.4766 - loss: 2.519 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4437 - loss: 2.515 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4308 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4262 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4232 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4213 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4204 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4194 - loss: 2.5772\n",
      "Epoch 198: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4187 - loss: 2.5792 - val_accuracy: 0.3763 - val_loss: 2.9268\n",
      "Epoch 199/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4258 - loss: 2.426 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4249 - loss: 2.506 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4220 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4184 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4172 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4162 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4152 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4143 - loss: 2.5761\n",
      "Epoch 199: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4138 - loss: 2.5781 - val_accuracy: 0.3822 - val_loss: 2.9030\n",
      "Epoch 200/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3984 - loss: 2.715 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4080 - loss: 2.632 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4116 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4126 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4128 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4130 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4129 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4127 - loss: 2.5968\n",
      "Epoch 200: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4127 - loss: 2.5966 - val_accuracy: 0.3774 - val_loss: 2.9258\n",
      "Epoch 201/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4180 - loss: 2.658 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4121 - loss: 2.628 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4155 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4142 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4136 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4132 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4129 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4126 - loss: 2.6058\n",
      "Epoch 201: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4122 - loss: 2.6053 - val_accuracy: 0.3804 - val_loss: 2.9131\n",
      "Epoch 202/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4297 - loss: 2.587 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4324 - loss: 2.545 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4263 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.4256 - loss: 2.564 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4213 - loss: 2.572 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4183 - loss: 2.578 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4162 - loss: 2.579 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4153 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4147 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4142 - loss: 2.5850\n",
      "Epoch 202: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4140 - loss: 2.5855 - val_accuracy: 0.3794 - val_loss: 2.9105\n",
      "Epoch 203/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4297 - loss: 2.487 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4190 - loss: 2.510 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4205 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4197 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4191 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4178 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4167 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4159 - loss: 2.5772\n",
      "Epoch 203: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4155 - loss: 2.5800 - val_accuracy: 0.3776 - val_loss: 2.9085\n",
      "Epoch 204/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 43ms/step - accuracy: 0.4453 - loss: 2.540 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4273 - loss: 2.564 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4277 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4265 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4254 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4241 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4230 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4218 - loss: 2.5890\n",
      "Epoch 204: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4209 - loss: 2.5910 - val_accuracy: 0.3702 - val_loss: 2.9287\n",
      "Epoch 205/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4375 - loss: 2.572 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4215 - loss: 2.547 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4172 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4158 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4145 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4138 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4129 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4124 - loss: 2.5865\n",
      "Epoch 205: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4123 - loss: 2.5877 - val_accuracy: 0.3822 - val_loss: 2.9264\n",
      "Epoch 206/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 4s 71ms/step - accuracy: 0.4336 - loss: 2.602 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4140 - loss: 2.651 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4142 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4127 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4114 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4103 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4101 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4103 - loss: 2.6243\n",
      "Epoch 206: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4106 - loss: 2.6222 - val_accuracy: 0.3820 - val_loss: 2.9162\n",
      "Epoch 207/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4531 - loss: 2.432 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4347 - loss: 2.534 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4310 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4277 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4227 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4183 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4161 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4142 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4131 - loss: 2.5939\n",
      "Epoch 207: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4129 - loss: 2.5945 - val_accuracy: 0.3733 - val_loss: 2.9418\n",
      "Epoch 208/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4023 - loss: 2.673 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4130 - loss: 2.640 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4125 - loss: 2.63 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4128 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4136 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4136 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4132 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4130 - loss: 2.6111\n",
      "Epoch 208: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4130 - loss: 2.6104 - val_accuracy: 0.3799 - val_loss: 2.9216\n",
      "Epoch 209/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 3s 58ms/step - accuracy: 0.4297 - loss: 2.515 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4143 - loss: 2.563 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4134 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4139 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4136 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4128 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4127 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4127 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4126 - loss: 2.5891\n",
      "Epoch 209: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4123 - loss: 2.5898 - val_accuracy: 0.3820 - val_loss: 2.9396\n",
      "Epoch 210/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3984 - loss: 2.585 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4169 - loss: 2.564 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4157 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4155 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4156 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4157 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4159 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4160 - loss: 2.5792\n",
      "Epoch 210: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4163 - loss: 2.5802 - val_accuracy: 0.3725 - val_loss: 2.9194\n",
      "Epoch 211/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4336 - loss: 2.478 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4190 - loss: 2.533 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4186 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4180 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4167 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4163 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4161 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4159 - loss: 2.5835\n",
      "Epoch 211: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4155 - loss: 2.5860 - val_accuracy: 0.3774 - val_loss: 2.9422\n",
      "Epoch 212/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.4453 - loss: 2.505 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4215 - loss: 2.563 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4168 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4143 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4123 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4110 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4106 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4106 - loss: 2.6056\n",
      "Epoch 212: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4106 - loss: 2.6056 - val_accuracy: 0.3779 - val_loss: 2.9325\n",
      "Epoch 213/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 3s 53ms/step - accuracy: 0.4180 - loss: 2.532 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4208 - loss: 2.556 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4202 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4179 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4168 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4157 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4155 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4155 - loss: 2.5842\n",
      "Epoch 213: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4155 - loss: 2.5844 - val_accuracy: 0.3789 - val_loss: 2.9269\n",
      "Epoch 214/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4258 - loss: 2.641 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4074 - loss: 2.630 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4096 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4109 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4118 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4128 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4134 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4141 - loss: 2.5933\n",
      "Epoch 214: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4144 - loss: 2.5919 - val_accuracy: 0.3786 - val_loss: 2.9313\n",
      "Epoch 215/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4141 - loss: 2.600 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4115 - loss: 2.573 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4138 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4140 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4144 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4148 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4153 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4160 - loss: 2.5725\n",
      "Epoch 215: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4162 - loss: 2.5730 - val_accuracy: 0.3776 - val_loss: 2.9257\n",
      "Epoch 216/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 4s 71ms/step - accuracy: 0.4180 - loss: 2.502 ━━━━━━━━━━━━━━━━━━━━ 3s 54ms/step - accuracy: 0.4199 - loss: 2.542 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.4219 - loss: 2.582 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4205 - loss: 2.592 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4191 - loss: 2.597 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4177 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4164 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4156 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4148 - loss: 2.6040\n",
      "Epoch 216: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4144 - loss: 2.6042 - val_accuracy: 0.3774 - val_loss: 2.9149\n",
      "Epoch 217/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 43ms/step - accuracy: 0.3828 - loss: 2.710 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4022 - loss: 2.668 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4115 - loss: 2.62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4121 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4124 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4133 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4137 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4140 - loss: 2.5965\n",
      "Epoch 217: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4140 - loss: 2.5963 - val_accuracy: 0.3753 - val_loss: 2.9225\n",
      "Epoch 218/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4219 - loss: 2.444 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4243 - loss: 2.513 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4221 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4214 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4213 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4209 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4205 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4200 - loss: 2.5588\n",
      "Epoch 218: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4197 - loss: 2.5606 - val_accuracy: 0.3794 - val_loss: 2.9071\n",
      "Epoch 219/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 3s 51ms/step - accuracy: 0.4648 - loss: 2.256 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4320 - loss: 2.490 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4242 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4215 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4195 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4184 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4178 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4176 - loss: 2.5930\n",
      "Epoch 219: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4174 - loss: 2.5938 - val_accuracy: 0.3761 - val_loss: 2.9380\n",
      "Epoch 220/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4102 - loss: 2.591 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4115 - loss: 2.571 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4146 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4149 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4138 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4132 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4128 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4125 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4121 - loss: 2.5852\n",
      "Epoch 220: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4120 - loss: 2.5857 - val_accuracy: 0.3679 - val_loss: 2.9335\n",
      "Epoch 221/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.3594 - loss: 2.678 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4058 - loss: 2.567 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4117 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4157 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4171 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4174 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4176 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4178 - loss: 2.5759\n",
      "Epoch 221: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4179 - loss: 2.5763 - val_accuracy: 0.3771 - val_loss: 2.9356\n",
      "Epoch 222/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 35ms/step - accuracy: 0.4492 - loss: 2.347 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4271 - loss: 2.551 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4207 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4183 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4170 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4165 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4163 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4162 - loss: 2.6042\n",
      "Epoch 222: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4162 - loss: 2.6043 - val_accuracy: 0.3763 - val_loss: 2.9265\n",
      "Epoch 223/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4219 - loss: 2.560 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4213 - loss: 2.568 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4195 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4187 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4180 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4180 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4179 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4176 - loss: 2.5753\n",
      "Epoch 223: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4174 - loss: 2.5761 - val_accuracy: 0.3858 - val_loss: 2.9216\n",
      "Epoch 224/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 3s 57ms/step - accuracy: 0.3906 - loss: 2.639 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4020 - loss: 2.590 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4070 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4104 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4118 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4120 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4120 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4119 - loss: 2.5844\n",
      "Epoch 224: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4117 - loss: 2.5858 - val_accuracy: 0.3792 - val_loss: 2.9326\n",
      "Epoch 225/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.3750 - loss: 2.542 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4071 - loss: 2.573 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4074 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4113 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4134 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4144 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4144 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4143 - loss: 2.5753\n",
      "Epoch 225: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4141 - loss: 2.5757 - val_accuracy: 0.3789 - val_loss: 2.9496\n",
      "Epoch 226/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4141 - loss: 2.799 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4310 - loss: 2.599 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4216 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4172 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4153 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4139 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4135 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4135 - loss: 2.6100\n",
      "Epoch 226: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4135 - loss: 2.6097 - val_accuracy: 0.3766 - val_loss: 2.9456\n",
      "Epoch 227/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.4336 - loss: 2.467 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4331 - loss: 2.560 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4283 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4253 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4241 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4232 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4225 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4217 - loss: 2.5856\n",
      "Epoch 227: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4212 - loss: 2.5868 - val_accuracy: 0.3797 - val_loss: 2.9308\n",
      "Epoch 228/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3945 - loss: 2.616 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4244 - loss: 2.554 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4261 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4272 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4270 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4265 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4255 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4248 - loss: 2.5651\n",
      "Epoch 228: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4243 - loss: 2.5668 - val_accuracy: 0.3743 - val_loss: 2.9248\n",
      "Epoch 229/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4922 - loss: 2.325 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.4460 - loss: 2.462 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4299 - loss: 2.511 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4236 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4223 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4207 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4196 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4186 - loss: 2.5614\n",
      "Epoch 229: val_accuracy did not improve from 0.38579\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4182 - loss: 2.5664 - val_accuracy: 0.3792 - val_loss: 2.9449\n",
      "Epoch 230/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 48ms/step - accuracy: 0.3672 - loss: 2.576 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4026 - loss: 2.610 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4090 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4114 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4119 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4131 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4140 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4144 - loss: 2.5942\n",
      "Epoch 230: val_accuracy improved from 0.38579 to 0.38707, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4144 - loss: 2.5941 - val_accuracy: 0.3871 - val_loss: 2.9409\n",
      "Epoch 231/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 3s 58ms/step - accuracy: 0.4102 - loss: 2.477 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4153 - loss: 2.535 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4160 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4145 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4145 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4145 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4143 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4143 - loss: 2.5594\n",
      "Epoch 231: val_accuracy did not improve from 0.38707\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4142 - loss: 2.5625 - val_accuracy: 0.3776 - val_loss: 2.9305\n",
      "Epoch 232/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 43ms/step - accuracy: 0.3945 - loss: 2.629 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4053 - loss: 2.583 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.4072 - loss: 2.585 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4083 - loss: 2.586 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4094 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4105 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4115 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4121 - loss: 2.5854\n",
      "Epoch 232: val_accuracy did not improve from 0.38707\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4124 - loss: 2.5859 - val_accuracy: 0.3692 - val_loss: 2.9574\n",
      "Epoch 233/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.4336 - loss: 2.459 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4224 - loss: 2.510 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4226 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4225 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4231 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4230 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4226 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4223 - loss: 2.5496\n",
      "Epoch 233: val_accuracy did not improve from 0.38707\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4222 - loss: 2.5506 - val_accuracy: 0.3815 - val_loss: 2.9309\n",
      "Epoch 234/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 35ms/step - accuracy: 0.3945 - loss: 2.689 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4053 - loss: 2.590 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4060 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4096 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4105 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4114 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4126 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4137 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4143 - loss: 2.5841\n",
      "Epoch 234: val_accuracy did not improve from 0.38707\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4144 - loss: 2.5844 - val_accuracy: 0.3794 - val_loss: 2.9292\n",
      "Epoch 235/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 3s 60ms/step - accuracy: 0.4375 - loss: 2.699 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4299 - loss: 2.621 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4277 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4256 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4250 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4245 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4238 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4233 - loss: 2.5900\n",
      "Epoch 235: val_accuracy did not improve from 0.38707\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4230 - loss: 2.5892 - val_accuracy: 0.3807 - val_loss: 2.9222\n",
      "Epoch 236/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4141 - loss: 2.588 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4165 - loss: 2.600 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4174 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4167 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4166 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4166 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4165 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4164 - loss: 2.6073\n",
      "Epoch 236: val_accuracy did not improve from 0.38707\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4165 - loss: 2.6071 - val_accuracy: 0.3779 - val_loss: 2.9179\n",
      "Epoch 237/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4766 - loss: 2.263 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4354 - loss: 2.477 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4295 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4304 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4303 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4300 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4294 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4289 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4282 - loss: 2.5353\n",
      "Epoch 237: val_accuracy did not improve from 0.38707\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4281 - loss: 2.5362 - val_accuracy: 0.3843 - val_loss: 2.9223\n",
      "Epoch 238/800\n",
      "53/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4922 - loss: 2.294 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4456 - loss: 2.492 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4363 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4309 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4285 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4274 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4267 - loss: 2.5542\n",
      "Epoch 238: val_accuracy improved from 0.38707 to 0.39142, saving model to best_model.weights.h5\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4259 - loss: 2.5578 - val_accuracy: 0.3914 - val_loss: 2.9000\n",
      "Epoch 239/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 3s 55ms/step - accuracy: 0.4805 - loss: 2.511 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4435 - loss: 2.558 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4310 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4273 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4254 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4242 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4234 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4231 - loss: 2.5726\n",
      "Epoch 239: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4229 - loss: 2.5730 - val_accuracy: 0.3779 - val_loss: 2.9197\n",
      "Epoch 240/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.3828 - loss: 2.667 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4105 - loss: 2.601 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4123 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4157 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4165 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4173 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4178 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4177 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4176 - loss: 2.5808\n",
      "Epoch 240: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4176 - loss: 2.5817 - val_accuracy: 0.3835 - val_loss: 2.9427\n",
      "Epoch 241/800\n",
      "53/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.3867 - loss: 2.677 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4194 - loss: 2.553 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4155 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4143 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4138 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4135 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4132 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4130 - loss: 2.6076\n",
      "Epoch 241: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4127 - loss: 2.6095 - val_accuracy: 0.3771 - val_loss: 2.9381\n",
      "Epoch 242/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4453 - loss: 2.517 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4383 - loss: 2.513 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4354 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4322 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4306 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4289 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4276 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4263 - loss: 2.5603\n",
      "Epoch 242: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4255 - loss: 2.5630 - val_accuracy: 0.3853 - val_loss: 2.9065\n",
      "Epoch 243/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4727 - loss: 2.476 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.4336 - loss: 2.529 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4222 - loss: 2.550 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4191 - loss: 2.557 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4195 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4185 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4188 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4191 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4194 - loss: 2.5609\n",
      "Epoch 243: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4194 - loss: 2.5611 - val_accuracy: 0.3766 - val_loss: 2.9227\n",
      "Epoch 244/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3672 - loss: 2.682 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4171 - loss: 2.551 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4207 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4200 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4196 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4194 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4189 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4190 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4191 - loss: 2.5567\n",
      "Epoch 244: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4192 - loss: 2.5568 - val_accuracy: 0.3802 - val_loss: 2.9206\n",
      "Epoch 245/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3867 - loss: 2.641 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.4094 - loss: 2.595 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.4125 - loss: 2.592 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4129 - loss: 2.597 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4141 - loss: 2.595 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4154 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4163 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4170 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4171 - loss: 2.5905\n",
      "Epoch 245: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4174 - loss: 2.5898 - val_accuracy: 0.3802 - val_loss: 2.9311\n",
      "Epoch 246/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4648 - loss: 2.499 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4644 - loss: 2.441 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4499 - loss: 2.48 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4424 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4375 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4349 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4323 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4302 - loss: 2.5383\n",
      "Epoch 246: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4293 - loss: 2.5418 - val_accuracy: 0.3707 - val_loss: 2.9350\n",
      "Epoch 247/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 6s 115ms/step - accuracy: 0.5039 - loss: 2.27 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4747 - loss: 2.3784 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4588 - loss: 2.44 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4487 - loss: 2.47 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4439 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4419 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4398 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4381 - loss: 2.5157\n",
      "Epoch 247: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4361 - loss: 2.5230 - val_accuracy: 0.3733 - val_loss: 2.9409\n",
      "Epoch 248/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4414 - loss: 2.449 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4165 - loss: 2.538 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4167 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4182 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4190 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4195 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4195 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4195 - loss: 2.5571\n",
      "Epoch 248: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4194 - loss: 2.5585 - val_accuracy: 0.3794 - val_loss: 2.9426\n",
      "Epoch 249/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4609 - loss: 2.418 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4456 - loss: 2.466 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4389 - loss: 2.48 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4351 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4326 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4299 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4278 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4264 - loss: 2.5271\n",
      "Epoch 249: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4252 - loss: 2.5338 - val_accuracy: 0.3861 - val_loss: 2.9557\n",
      "Epoch 250/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4258 - loss: 2.470 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4206 - loss: 2.470 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4180 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4157 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4159 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4162 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4162 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4164 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4164 - loss: 2.5520\n",
      "Epoch 250: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4164 - loss: 2.5530 - val_accuracy: 0.3868 - val_loss: 2.9328\n",
      "Epoch 251/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 46ms/step - accuracy: 0.3984 - loss: 2.508 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4184 - loss: 2.503 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4188 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4171 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4162 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4162 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4161 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4160 - loss: 2.5436\n",
      "Epoch 251: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4159 - loss: 2.5453 - val_accuracy: 0.3871 - val_loss: 2.9270\n",
      "Epoch 252/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.3711 - loss: 2.650 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4106 - loss: 2.515 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4147 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4178 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4192 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4185 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4178 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4173 - loss: 2.5552\n",
      "Epoch 252: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4168 - loss: 2.5599 - val_accuracy: 0.3804 - val_loss: 2.9435\n",
      "Epoch 253/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 35ms/step - accuracy: 0.4453 - loss: 2.470 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4348 - loss: 2.545 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4322 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4295 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4272 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4250 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4234 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4225 - loss: 2.5761\n",
      "Epoch 253: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4223 - loss: 2.5762 - val_accuracy: 0.3815 - val_loss: 2.9417\n",
      "Epoch 254/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 3s 57ms/step - accuracy: 0.3984 - loss: 2.435 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4087 - loss: 2.535 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4095 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4104 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4112 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4117 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4122 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4128 - loss: 2.5942\n",
      "Epoch 254: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4129 - loss: 2.5946 - val_accuracy: 0.3797 - val_loss: 2.9359\n",
      "Epoch 255/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 4s 76ms/step - accuracy: 0.4648 - loss: 2.352 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.4469 - loss: 2.470 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4342 - loss: 2.536 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4262 - loss: 2.572 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4224 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4212 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4205 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4201 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4196 - loss: 2.5878\n",
      "Epoch 255: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4196 - loss: 2.5880 - val_accuracy: 0.3753 - val_loss: 2.9422\n",
      "Epoch 256/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 43ms/step - accuracy: 0.4688 - loss: 2.510 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4328 - loss: 2.533 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4196 - loss: 2.550 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4164 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4149 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4139 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4134 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4129 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4128 - loss: 2.5725\n",
      "Epoch 256: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4128 - loss: 2.5725 - val_accuracy: 0.3822 - val_loss: 2.9363\n",
      "Epoch 257/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4844 - loss: 2.494 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4424 - loss: 2.543 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.4406 - loss: 2.547 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.4379 - loss: 2.554 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4344 - loss: 2.558 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4325 - loss: 2.559 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4313 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4302 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4287 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4274 - loss: 2.5652\n",
      "Epoch 257: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4270 - loss: 2.5660 - val_accuracy: 0.3825 - val_loss: 2.9219\n",
      "Epoch 258/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4688 - loss: 2.425 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4354 - loss: 2.483 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4237 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4198 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4187 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4183 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4180 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4180 - loss: 2.5624\n",
      "Epoch 258: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4177 - loss: 2.5647 - val_accuracy: 0.3822 - val_loss: 2.9438\n",
      "Epoch 259/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4805 - loss: 2.531 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4458 - loss: 2.514 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4408 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4357 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4317 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4287 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4268 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4256 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4250 - loss: 2.5778\n",
      "Epoch 259: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4247 - loss: 2.5782 - val_accuracy: 0.3820 - val_loss: 2.9258\n",
      "Epoch 260/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.4844 - loss: 2.422 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4282 - loss: 2.589 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4239 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4234 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4237 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4237 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4231 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4227 - loss: 2.5700\n",
      "Epoch 260: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4223 - loss: 2.5705 - val_accuracy: 0.3891 - val_loss: 2.9358\n",
      "Epoch 261/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4258 - loss: 2.530 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4336 - loss: 2.508 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4304 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4286 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4276 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4270 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4265 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4259 - loss: 2.5595\n",
      "Epoch 261: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4257 - loss: 2.5609 - val_accuracy: 0.3812 - val_loss: 2.9429\n",
      "Epoch 262/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 3s 62ms/step - accuracy: 0.4766 - loss: 2.455 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4508 - loss: 2.480 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4403 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4337 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4324 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4312 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4308 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4302 - loss: 2.5459\n",
      "Epoch 262: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4297 - loss: 2.5481 - val_accuracy: 0.3827 - val_loss: 2.9423\n",
      "Epoch 263/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 3s 52ms/step - accuracy: 0.4375 - loss: 2.505 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4381 - loss: 2.529 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4293 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4258 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4239 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4225 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4216 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4208 - loss: 2.5816\n",
      "Epoch 263: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4206 - loss: 2.5821 - val_accuracy: 0.3804 - val_loss: 2.9424\n",
      "Epoch 264/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 4s 70ms/step - accuracy: 0.4258 - loss: 2.550 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4168 - loss: 2.604 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4194 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4203 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4201 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4199 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4200 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4201 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4202 - loss: 2.5798\n",
      "Epoch 264: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4202 - loss: 2.5797 - val_accuracy: 0.3861 - val_loss: 2.9374\n",
      "Epoch 265/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4336 - loss: 2.515 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4147 - loss: 2.547 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4140 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4155 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4184 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4202 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4208 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4213 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4219 - loss: 2.5411\n",
      "Epoch 265: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4220 - loss: 2.5414 - val_accuracy: 0.3850 - val_loss: 2.9283\n",
      "Epoch 266/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4453 - loss: 2.571 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4303 - loss: 2.576 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4254 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4255 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4254 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4250 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4247 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4241 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4234 - loss: 2.5615\n",
      "Epoch 266: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4233 - loss: 2.5616 - val_accuracy: 0.3776 - val_loss: 2.9272\n",
      "Epoch 267/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 3s 56ms/step - accuracy: 0.4648 - loss: 2.515 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4301 - loss: 2.579 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4262 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4261 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4266 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4263 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4258 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4255 - loss: 2.5977\n",
      "Epoch 267: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4251 - loss: 2.5976 - val_accuracy: 0.3820 - val_loss: 2.9334\n",
      "Epoch 268/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4766 - loss: 2.300 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4563 - loss: 2.419 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4474 - loss: 2.46 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4411 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4368 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4331 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4300 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4279 - loss: 2.5599\n",
      "Epoch 268: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4267 - loss: 2.5653 - val_accuracy: 0.3881 - val_loss: 2.9213\n",
      "Epoch 269/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4492 - loss: 2.348 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4110 - loss: 2.507 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4126 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4136 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4135 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4136 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4136 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4136 - loss: 2.5532\n",
      "Epoch 269: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4137 - loss: 2.5546 - val_accuracy: 0.3838 - val_loss: 2.9103\n",
      "Epoch 270/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4570 - loss: 2.496 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4356 - loss: 2.531 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4350 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4342 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4323 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4303 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4293 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4283 - loss: 2.5505\n",
      "Epoch 270: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4276 - loss: 2.5529 - val_accuracy: 0.3756 - val_loss: 2.9268\n",
      "Epoch 271/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 45ms/step - accuracy: 0.4297 - loss: 2.482 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4320 - loss: 2.531 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4280 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4260 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4237 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4219 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4213 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4212 - loss: 2.5640\n",
      "Epoch 271: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4209 - loss: 2.5647 - val_accuracy: 0.3858 - val_loss: 2.9298\n",
      "Epoch 272/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4023 - loss: 2.634 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4169 - loss: 2.577 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.4240 - loss: 2.563 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4259 - loss: 2.557 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4254 - loss: 2.554 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4244 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4236 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4235 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4232 - loss: 2.5539\n",
      "Epoch 272: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4232 - loss: 2.5531 - val_accuracy: 0.3792 - val_loss: 2.9364\n",
      "Epoch 273/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 43ms/step - accuracy: 0.4297 - loss: 2.407 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4331 - loss: 2.536 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4305 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4282 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4276 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4275 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4276 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4272 - loss: 2.5663\n",
      "Epoch 273: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4267 - loss: 2.5669 - val_accuracy: 0.3838 - val_loss: 2.9246\n",
      "Epoch 274/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4453 - loss: 2.382 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4363 - loss: 2.509 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4305 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4268 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4248 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4242 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4237 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4232 - loss: 2.5648\n",
      "Epoch 274: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4226 - loss: 2.5668 - val_accuracy: 0.3894 - val_loss: 2.9309\n",
      "Epoch 275/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 35ms/step - accuracy: 0.4219 - loss: 2.545 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4342 - loss: 2.540 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4313 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4314 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4306 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4293 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4284 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4276 - loss: 2.5676\n",
      "Epoch 275: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4273 - loss: 2.5686 - val_accuracy: 0.3820 - val_loss: 2.9274\n",
      "Epoch 276/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3828 - loss: 2.678 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4146 - loss: 2.573 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4171 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4178 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4180 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4182 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4182 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4181 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4179 - loss: 2.5824\n",
      "Epoch 276: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4179 - loss: 2.5822 - val_accuracy: 0.3799 - val_loss: 2.9307\n",
      "Epoch 277/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 3s 55ms/step - accuracy: 0.3945 - loss: 2.572 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4152 - loss: 2.550 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4142 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4146 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4155 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4158 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4167 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4175 - loss: 2.5692\n",
      "Epoch 277: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4179 - loss: 2.5685 - val_accuracy: 0.3817 - val_loss: 2.9385\n",
      "Epoch 278/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4180 - loss: 2.538 ━━━━━━━━━━━━━━━━━━━━ 1s 19ms/step - accuracy: 0.4283 - loss: 2.516 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4271 - loss: 2.541 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4236 - loss: 2.554 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4212 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4205 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4199 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4197 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4194 - loss: 2.5689\n",
      "Epoch 278: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4192 - loss: 2.5698 - val_accuracy: 0.3863 - val_loss: 2.9293\n",
      "Epoch 279/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4375 - loss: 2.553 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4178 - loss: 2.602 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4200 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4206 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4206 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4204 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4198 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4190 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4186 - loss: 2.5947\n",
      "Epoch 279: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4186 - loss: 2.5941 - val_accuracy: 0.3912 - val_loss: 2.9294\n",
      "Epoch 280/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4062 - loss: 2.487 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4233 - loss: 2.496 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4225 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4220 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4214 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4213 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4213 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4211 - loss: 2.5489\n",
      "Epoch 280: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4210 - loss: 2.5496 - val_accuracy: 0.3835 - val_loss: 2.9352\n",
      "Epoch 281/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 3s 60ms/step - accuracy: 0.4297 - loss: 2.414 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4329 - loss: 2.461 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4272 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4266 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4252 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4244 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4238 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4231 - loss: 2.5462\n",
      "Epoch 281: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4228 - loss: 2.5480 - val_accuracy: 0.3797 - val_loss: 2.9612\n",
      "Epoch 282/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4766 - loss: 2.313 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4393 - loss: 2.502 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4376 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4368 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4348 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4333 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4322 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4319 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4316 - loss: 2.5475\n",
      "Epoch 282: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4316 - loss: 2.5477 - val_accuracy: 0.3789 - val_loss: 2.9213\n",
      "Epoch 283/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 35ms/step - accuracy: 0.3867 - loss: 2.639 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4276 - loss: 2.503 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4290 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4299 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4296 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4296 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4293 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4290 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4288 - loss: 2.5238\n",
      "Epoch 283: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4288 - loss: 2.5241 - val_accuracy: 0.3832 - val_loss: 2.9477\n",
      "Epoch 284/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4062 - loss: 2.503 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4247 - loss: 2.526 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4210 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4211 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4205 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4203 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4203 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4205 - loss: 2.5527\n",
      "Epoch 284: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4207 - loss: 2.5541 - val_accuracy: 0.3756 - val_loss: 2.9684\n",
      "Epoch 285/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4453 - loss: 2.381 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4368 - loss: 2.482 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4365 - loss: 2.48 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4367 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4351 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4327 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4309 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4294 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4285 - loss: 2.5325\n",
      "Epoch 285: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4282 - loss: 2.5336 - val_accuracy: 0.3817 - val_loss: 2.9278\n",
      "Epoch 286/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.3711 - loss: 2.738 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4117 - loss: 2.626 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4134 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4135 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4128 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4127 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4135 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4144 - loss: 2.6061\n",
      "Epoch 286: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4149 - loss: 2.6039 - val_accuracy: 0.3761 - val_loss: 2.9432\n",
      "Epoch 287/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4453 - loss: 2.559 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4274 - loss: 2.544 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4217 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4191 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4181 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4174 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4171 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4170 - loss: 2.5782\n",
      "Epoch 287: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4170 - loss: 2.5787 - val_accuracy: 0.3815 - val_loss: 2.9230\n",
      "Epoch 288/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4414 - loss: 2.408 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4458 - loss: 2.468 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4381 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4356 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4345 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4341 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4337 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4332 - loss: 2.5363\n",
      "Epoch 288: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4326 - loss: 2.5391 - val_accuracy: 0.3871 - val_loss: 2.9453\n",
      "Epoch 289/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4219 - loss: 2.443 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4041 - loss: 2.565 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4042 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4064 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4078 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4093 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4105 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4115 - loss: 2.5846\n",
      "Epoch 289: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4123 - loss: 2.5829 - val_accuracy: 0.3761 - val_loss: 2.9574\n",
      "Epoch 290/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 44ms/step - accuracy: 0.4453 - loss: 2.484 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4367 - loss: 2.569 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4328 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4305 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4295 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4295 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4292 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4291 - loss: 2.5621\n",
      "Epoch 290: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4291 - loss: 2.5617 - val_accuracy: 0.3825 - val_loss: 2.9511\n",
      "Epoch 291/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.3398 - loss: 2.929 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4097 - loss: 2.608 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4134 - loss: 2.59 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4144 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4152 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4150 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4151 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4151 - loss: 2.5886\n",
      "Epoch 291: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4152 - loss: 2.5887 - val_accuracy: 0.3845 - val_loss: 2.9588\n",
      "Epoch 292/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 45ms/step - accuracy: 0.4141 - loss: 2.527 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4311 - loss: 2.525 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4303 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4283 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4266 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4253 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4245 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4239 - loss: 2.5754\n",
      "Epoch 292: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4236 - loss: 2.5761 - val_accuracy: 0.3822 - val_loss: 2.9364\n",
      "Epoch 293/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4727 - loss: 2.374 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4284 - loss: 2.505 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4223 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4216 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4223 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4217 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4216 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4216 - loss: 2.5459\n",
      "Epoch 293: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4217 - loss: 2.5481 - val_accuracy: 0.3781 - val_loss: 2.9467\n",
      "Epoch 294/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3711 - loss: 2.572 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4093 - loss: 2.541 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4164 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4178 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4189 - loss: 2.537 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4193 - loss: 2.540 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4189 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4187 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4186 - loss: 2.5481\n",
      "Epoch 294: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4185 - loss: 2.5501 - val_accuracy: 0.3807 - val_loss: 2.9345\n",
      "Epoch 295/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.3867 - loss: 2.714 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4102 - loss: 2.573 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4131 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4137 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4138 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4146 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4153 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4161 - loss: 2.5687\n",
      "Epoch 295: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4164 - loss: 2.5691 - val_accuracy: 0.3807 - val_loss: 2.9518\n",
      "Epoch 296/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4453 - loss: 2.375 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4467 - loss: 2.456 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.4453 - loss: 2.463 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4410 - loss: 2.481 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4366 - loss: 2.497 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4339 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4320 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4304 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4294 - loss: 2.5297\n",
      "Epoch 296: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4288 - loss: 2.5324 - val_accuracy: 0.3725 - val_loss: 2.9651\n",
      "Epoch 297/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 35ms/step - accuracy: 0.4336 - loss: 2.447 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4179 - loss: 2.552 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4151 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4143 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4140 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4143 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4142 - loss: 2.58 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4145 - loss: 2.5804\n",
      "Epoch 297: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4148 - loss: 2.5795 - val_accuracy: 0.3858 - val_loss: 2.9522\n",
      "Epoch 298/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 3s 55ms/step - accuracy: 0.3711 - loss: 2.536 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4171 - loss: 2.512 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4248 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4262 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4268 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4269 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4269 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4264 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4257 - loss: 2.5391\n",
      "Epoch 298: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4256 - loss: 2.5399 - val_accuracy: 0.3807 - val_loss: 2.9335\n",
      "Epoch 299/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4570 - loss: 2.380 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4471 - loss: 2.437 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4402 - loss: 2.46 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4371 - loss: 2.48 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4347 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4321 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4307 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4298 - loss: 2.5310\n",
      "Epoch 299: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4292 - loss: 2.5357 - val_accuracy: 0.3868 - val_loss: 2.9330\n",
      "Epoch 300/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4141 - loss: 2.640 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4258 - loss: 2.536 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4241 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4232 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4229 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4230 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4236 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4237 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4237 - loss: 2.5571\n",
      "Epoch 300: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4237 - loss: 2.5575 - val_accuracy: 0.3878 - val_loss: 2.9332\n",
      "Epoch 301/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4688 - loss: 2.455 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4329 - loss: 2.555 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4314 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4302 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4287 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4273 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4265 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4263 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4261 - loss: 2.5679\n",
      "Epoch 301: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4260 - loss: 2.5678 - val_accuracy: 0.3858 - val_loss: 2.9297\n",
      "Epoch 302/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4766 - loss: 2.436 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4351 - loss: 2.547 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4314 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4288 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4273 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4266 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4259 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4253 - loss: 2.5664\n",
      "Epoch 302: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4249 - loss: 2.5670 - val_accuracy: 0.3815 - val_loss: 2.9457\n",
      "Epoch 303/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 35ms/step - accuracy: 0.3945 - loss: 2.569 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4271 - loss: 2.505 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4251 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4254 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4257 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4259 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4252 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4246 - loss: 2.5389\n",
      "Epoch 303: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4244 - loss: 2.5406 - val_accuracy: 0.3832 - val_loss: 2.9470\n",
      "Epoch 304/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4414 - loss: 2.427 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4329 - loss: 2.481 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4284 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4282 - loss: 2.499 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4270 - loss: 2.509 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4261 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4258 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4258 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4255 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4251 - loss: 2.5299\n",
      "Epoch 304: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4251 - loss: 2.5307 - val_accuracy: 0.3858 - val_loss: 2.9359\n",
      "Epoch 305/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 3s 54ms/step - accuracy: 0.4727 - loss: 2.389 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4615 - loss: 2.421 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4486 - loss: 2.46 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4417 - loss: 2.48 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4388 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4365 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4346 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4332 - loss: 2.5167\n",
      "Epoch 305: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4322 - loss: 2.5210 - val_accuracy: 0.3807 - val_loss: 2.9491\n",
      "Epoch 306/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4453 - loss: 2.496 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4309 - loss: 2.550 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4299 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4291 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4289 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4289 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4288 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4286 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4284 - loss: 2.5442\n",
      "Epoch 306: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4284 - loss: 2.5442 - val_accuracy: 0.3858 - val_loss: 2.9334\n",
      "Epoch 307/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4531 - loss: 2.477 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4588 - loss: 2.435 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - accuracy: 0.4573 - loss: 2.441 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.4534 - loss: 2.457 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4491 - loss: 2.474 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4456 - loss: 2.485 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4435 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4412 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4394 - loss: 2.5029\n",
      "Epoch 307: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4390 - loss: 2.5038 - val_accuracy: 0.3774 - val_loss: 2.9433\n",
      "Epoch 308/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.4180 - loss: 2.495 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4334 - loss: 2.518 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4341 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4344 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4340 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4337 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4329 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4317 - loss: 2.5396\n",
      "Epoch 308: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4312 - loss: 2.5408 - val_accuracy: 0.3797 - val_loss: 2.9554\n",
      "Epoch 309/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 12s 206ms/step - accuracy: 0.4453 - loss: 2.483 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4418 - loss: 2.4697  ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4334 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4287 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4265 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4256 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4245 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4237 - loss: 2.5523\n",
      "Epoch 309: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4236 - loss: 2.5538 - val_accuracy: 0.3812 - val_loss: 2.9362\n",
      "Epoch 310/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4375 - loss: 2.500 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4312 - loss: 2.534 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4267 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4262 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4260 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4260 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4263 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4262 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4262 - loss: 2.5569\n",
      "Epoch 310: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4263 - loss: 2.5568 - val_accuracy: 0.3832 - val_loss: 2.9345\n",
      "Epoch 311/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4414 - loss: 2.497 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4468 - loss: 2.531 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4417 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4362 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4336 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4318 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4310 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4301 - loss: 2.5589\n",
      "Epoch 311: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4294 - loss: 2.5588 - val_accuracy: 0.3871 - val_loss: 2.9378\n",
      "Epoch 312/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.3945 - loss: 2.550 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4128 - loss: 2.538 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4127 - loss: 2.550 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4129 - loss: 2.558 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4145 - loss: 2.556 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4161 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4173 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4186 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4196 - loss: 2.5525\n",
      "Epoch 312: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4201 - loss: 2.5520 - val_accuracy: 0.3807 - val_loss: 2.9431\n",
      "Epoch 313/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.5117 - loss: 2.368 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4610 - loss: 2.497 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4520 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4462 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4418 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4397 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4379 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4356 - loss: 2.5338\n",
      "Epoch 313: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4345 - loss: 2.5358 - val_accuracy: 0.3794 - val_loss: 2.9597\n",
      "Epoch 314/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 43ms/step - accuracy: 0.4219 - loss: 2.568 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4316 - loss: 2.537 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4274 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4259 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4249 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4245 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4245 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4244 - loss: 2.5581\n",
      "Epoch 314: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4243 - loss: 2.5581 - val_accuracy: 0.3789 - val_loss: 2.9581\n",
      "Epoch 315/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 35ms/step - accuracy: 0.4141 - loss: 2.556 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4088 - loss: 2.580 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4130 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4172 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4194 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4205 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4210 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4212 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4213 - loss: 2.5721\n",
      "Epoch 315: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4213 - loss: 2.5723 - val_accuracy: 0.3883 - val_loss: 2.9354\n",
      "Epoch 316/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4102 - loss: 2.452 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4187 - loss: 2.541 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4196 - loss: 2.547 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4209 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4215 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4222 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4225 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4227 - loss: 2.5550\n",
      "Epoch 316: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4228 - loss: 2.5548 - val_accuracy: 0.3817 - val_loss: 2.9434\n",
      "Epoch 317/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 4s 68ms/step - accuracy: 0.4453 - loss: 2.497 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4486 - loss: 2.499 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4449 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4385 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4343 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4309 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4292 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4277 - loss: 2.5502\n",
      "Epoch 317: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4270 - loss: 2.5531 - val_accuracy: 0.3817 - val_loss: 2.9307\n",
      "Epoch 318/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4414 - loss: 2.488 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4175 - loss: 2.569 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4204 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4213 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4226 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4243 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4255 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4259 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4259 - loss: 2.5529\n",
      "Epoch 318: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4260 - loss: 2.5533 - val_accuracy: 0.3784 - val_loss: 2.9413\n",
      "Epoch 319/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.3711 - loss: 2.736 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4365 - loss: 2.498 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4354 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4328 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4300 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4280 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4269 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4262 - loss: 2.5544\n",
      "Epoch 319: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4258 - loss: 2.5557 - val_accuracy: 0.3792 - val_loss: 2.9491\n",
      "Epoch 320/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 3s 56ms/step - accuracy: 0.4492 - loss: 2.398 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4563 - loss: 2.450 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4511 - loss: 2.48 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4453 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4427 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4407 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4392 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4379 - loss: 2.5208\n",
      "Epoch 320: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4373 - loss: 2.5225 - val_accuracy: 0.3894 - val_loss: 2.9259\n",
      "Epoch 321/800\n",
      "60/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.4375 - loss: 2.455 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.4408 - loss: 2.497 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4358 - loss: 2.522 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4334 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4327 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4323 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4317 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4307 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4299 - loss: 2.5371\n",
      "Epoch 321: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4297 - loss: 2.5377 - val_accuracy: 0.3866 - val_loss: 2.9297\n",
      "Epoch 322/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 43ms/step - accuracy: 0.4258 - loss: 2.445 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - accuracy: 0.4339 - loss: 2.483 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - accuracy: 0.4336 - loss: 2.492 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.4304 - loss: 2.512 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4279 - loss: 2.529 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4270 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4257 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4247 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4240 - loss: 2.5552\n",
      "Epoch 322: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4238 - loss: 2.5581 - val_accuracy: 0.3758 - val_loss: 2.9629\n",
      "Epoch 323/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 3s 61ms/step - accuracy: 0.4531 - loss: 2.515 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4456 - loss: 2.461 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4393 - loss: 2.48 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4346 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4317 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4311 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4309 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4306 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4302 - loss: 2.5282\n",
      "Epoch 323: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4302 - loss: 2.5285 - val_accuracy: 0.3891 - val_loss: 2.9532\n",
      "Epoch 324/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4180 - loss: 2.593 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step - accuracy: 0.4209 - loss: 2.572 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step - accuracy: 0.4196 - loss: 2.563 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.4196 - loss: 2.561 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4181 - loss: 2.567 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4173 - loss: 2.569 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4176 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4182 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4192 - loss: 2.5654\n",
      "Epoch 324: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4197 - loss: 2.5649 - val_accuracy: 0.3743 - val_loss: 2.9549\n",
      "Epoch 325/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4375 - loss: 2.580 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4355 - loss: 2.541 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4333 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4309 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4295 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4290 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4285 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4282 - loss: 2.5678\n",
      "Epoch 325: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4279 - loss: 2.5683 - val_accuracy: 0.3894 - val_loss: 2.9637\n",
      "Epoch 326/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 44ms/step - accuracy: 0.4688 - loss: 2.424 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4342 - loss: 2.496 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4309 - loss: 2.509 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.4302 - loss: 2.511 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4298 - loss: 2.515 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4291 - loss: 2.520 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4283 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4273 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4269 - loss: 2.5322\n",
      "Epoch 326: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4264 - loss: 2.5349 - val_accuracy: 0.3822 - val_loss: 2.9600\n",
      "Epoch 327/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 47ms/step - accuracy: 0.4531 - loss: 2.441 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4398 - loss: 2.560 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4374 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4368 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4350 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4332 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4311 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4299 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4293 - loss: 2.5555\n",
      "Epoch 327: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4292 - loss: 2.5556 - val_accuracy: 0.3809 - val_loss: 2.9515\n",
      "Epoch 328/800\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.4258 - loss: 2.579 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4352 - loss: 2.513 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4288 - loss: 2.529 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4267 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4251 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4242 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4238 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4236 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4234 - loss: 2.5552\n",
      "Epoch 328: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4234 - loss: 2.5554 - val_accuracy: 0.3761 - val_loss: 2.9567\n",
      "Epoch 329/800\n",
      "54/62 ━━━━━━━━━━━━━━━━━━━━ 4s 75ms/step - accuracy: 0.4766 - loss: 2.389 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4524 - loss: 2.460 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4511 - loss: 2.47 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4464 - loss: 2.48 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4426 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4410 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4401 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4392 - loss: 2.5121\n",
      "Epoch 329: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4384 - loss: 2.5154 - val_accuracy: 0.3873 - val_loss: 2.9335\n",
      "Epoch 330/800\n",
      "58/62 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - accuracy: 0.4141 - loss: 2.479 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.4399 - loss: 2.482 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4404 - loss: 2.489 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4379 - loss: 2.499 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4363 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4346 - loss: 2.51 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4340 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4334 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4324 - loss: 2.5299\n",
      "Epoch 330: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4320 - loss: 2.5318 - val_accuracy: 0.3784 - val_loss: 2.9548\n",
      "Epoch 331/800\n",
      "59/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3867 - loss: 2.627 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - accuracy: 0.4041 - loss: 2.600 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step - accuracy: 0.4095 - loss: 2.581 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.4159 - loss: 2.546 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4193 - loss: 2.533 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4220 - loss: 2.526 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4237 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4241 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4248 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4250 - loss: 2.5279\n",
      "Epoch 331: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 10ms/step - accuracy: 0.4251 - loss: 2.5287 - val_accuracy: 0.3786 - val_loss: 2.9376\n",
      "Epoch 332/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - accuracy: 0.4414 - loss: 2.647 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4329 - loss: 2.496 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4332 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4334 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4332 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4329 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4323 - loss: 2.50 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4314 - loss: 2.5107\n",
      "Epoch 332: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4309 - loss: 2.5122 - val_accuracy: 0.3848 - val_loss: 2.9362\n",
      "Epoch 333/800\n",
      "56/62 ━━━━━━━━━━━━━━━━━━━━ 2s 41ms/step - accuracy: 0.4336 - loss: 2.481 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4353 - loss: 2.473 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4319 - loss: 2.493 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4279 - loss: 2.516 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4269 - loss: 2.525 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4266 - loss: 2.533 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4263 - loss: 2.53 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4255 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4251 - loss: 2.5506\n",
      "Epoch 333: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4250 - loss: 2.5542 - val_accuracy: 0.3809 - val_loss: 2.9340\n",
      "Epoch 334/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 3s 52ms/step - accuracy: 0.4258 - loss: 2.569 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4296 - loss: 2.535 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4293 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4250 - loss: 2.56 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4224 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4219 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4217 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4217 - loss: 2.57 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4220 - loss: 2.5761\n",
      "Epoch 334: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4220 - loss: 2.5757 - val_accuracy: 0.3776 - val_loss: 2.9675\n",
      "Epoch 335/800\n",
      "57/62 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - accuracy: 0.4453 - loss: 2.572 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4218 - loss: 2.583 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4162 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4147 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4144 - loss: 2.61 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4146 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4151 - loss: 2.60 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4157 - loss: 2.6011\n",
      "Epoch 335: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4160 - loss: 2.5995 - val_accuracy: 0.3763 - val_loss: 2.9585\n",
      "Epoch 336/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 3s 52ms/step - accuracy: 0.4531 - loss: 2.417 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4581 - loss: 2.435 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4529 - loss: 2.44 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4478 - loss: 2.46 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.4440 - loss: 2.48 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4424 - loss: 2.48 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4402 - loss: 2.49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4380 - loss: 2.5045\n",
      "Epoch 336: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.4365 - loss: 2.5099 - val_accuracy: 0.3807 - val_loss: 2.9428\n",
      "Epoch 337/800\n",
      "55/62 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - accuracy: 0.3906 - loss: 2.568 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4448 - loss: 2.498 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4456 - loss: 2.493 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4400 - loss: 2.509 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.4387 - loss: 2.513 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4371 - loss: 2.518 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4357 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4346 - loss: 2.52 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4338 - loss: 2.5307\n",
      "Epoch 337: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4333 - loss: 2.5326 - val_accuracy: 0.3876 - val_loss: 2.9378\n",
      "Epoch 338/800\n",
      "61/62 ━━━━━━━━━━━━━━━━━━━━ 2s 36ms/step - accuracy: 0.3906 - loss: 2.668 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - accuracy: 0.4208 - loss: 2.586 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4204 - loss: 2.571 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4228 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4240 - loss: 2.55 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4246 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4248 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4247 - loss: 2.54 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.4250 - loss: 2.5490\n",
      "Epoch 338: val_accuracy did not improve from 0.39142\n",
      "62/62 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - accuracy: 0.4251 - loss: 2.5491 - val_accuracy: 0.3843 - val_loss: 2.9532\n",
      "153/153 ━━━━━━━━━━━━━━━━━━━━ 4s 31ms/step - accuracy: 0.4375 - loss: 2.667 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.4088 - loss: 2.825 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.3951 - loss: 2.85 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.3919 - loss: 2.87 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.3909 - loss: 2.8853\n",
      "Test Accuracy: 0.3859\n",
      "153/153 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.4375 - loss: 2.667 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.4104 - loss: 2.821 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.3954 - loss: 2.85 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.3920 - loss: 2.87 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.3913 - loss: 2.88 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.3909 - loss: 2.8853\n",
      "Test Accuracy with best weights: 0.3859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X.shape, y.shape)\n",
    "# Define the neural network model\n",
    "\n",
    "def create_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(256, input_shape=(input_shape,), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dense(y_train.shape[1], activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_shape = X_train.shape[1]\n",
    "model = create_model(input_shape)\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=100, restore_best_weights=True)\n",
    "\n",
    "# Define model checkpointing\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model.weights.h5',  # Path where the model weights will be saved\n",
    "    monitor='val_accuracy',  # Metric to monitor\n",
    "    save_best_only=True,  # Only save the model if the metric improves\n",
    "    mode='max',  # Minimize the monitored metric\n",
    "    save_weights_only=True,  # Only save the model's weights\n",
    "    verbose=1  # Print a message when saving the model\n",
    ")\n",
    "\n",
    "# Train the model with early stopping and model checkpointing\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=800,\n",
    "    batch_size=256,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Load the best weights\n",
    "model.load_weights('best_model.weights.h5')\n",
    "\n",
    "# Re-evaluate the model with the best weights\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy with best weights: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "# save the model as an image\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
